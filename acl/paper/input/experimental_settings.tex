\section{Experimental Settings}
\label{sec:experimental_settings}
  \subsection{Datasets}
  \label{subsec:datasets}
    \begin{table*}
      \centering
      \begin{tabular}{l|cccc|ccc}
        \toprule
        \multirow{2}{*}{\textbf{Corpus}} & \multicolumn{4}{c|}{\textbf{Documents}} & \multicolumn{3}{c}{\textbf{Keyphrases}}\\
        \cline{2-8}
        & Type & Language & Number & Tokens average & Average & Missing & Diversity\\
        \hline
        Inist & & & & & & &\\
        \hfill{}train & Abstracts & French & 515 & $~~$161 & $~~$8.6 & 60.6\% & 28.1\%\\
        \hfill{}test & Abstracts & French & 200 & $~~$147 & $~~$8.9 & 62.8\% & 40.4\%\\
        \hline
        DUC & News & English & 308 & $~~$901 & $~~$8.1 & $~~$3.5\% & 74.3\%\\
        \hline
        SemEval & & & & & & &\\
        \hfill{}train & Papers & English & 144 & 5135 & 15.4 & 13.5\% & 90.3\%\\
        \hfill{}test & Papers & English & 100 & 5178 & 14.7 & 22.1\% & 88.9\%\\
        \bottomrule
      \end{tabular}
      \caption{Dataset statistics. Missing represents the percentage of keyphrases
               that cannot be retrieved within the documents. Diversity shows the
               percentage of unique keyphrases.
               \label{tab:corpus_statistics}}
    \end{table*}
    We conduct our experiments on three datasets: Inist, DUC and SemEval, which differ
    in terms of language, nature and size. The following is the description of
    each dataset.

    \paragraph{Inist} is a collection of 715 bibliographic records that we
    collected from the French digital library Inist\footnote{The dataset will be soon available.}. The bibliographic records
    contain abstracts of scientific papers in the Linguistic area. The
    keyphrases are annotated by a single Inist indexer per record. Indexers were
    given the instruction to assign keyphrases from a terminology and extract new
    concepts if they are important in the context of the paper. We divided the
    corpus into two disjoint sets: a training set containing 515 bibliographic
    records and a test set containing 200 bibliographic records.

    \paragraph{DUC~\textnormal{\cite{wan2008expandrank}}} is a collection of 308
    news articles categorized within 30 topics. The documents have been manually
    annotated by students with 0.70 of inter-agreement (Kappa). DUC has no
    training set. For each document, we use the other
    documents of its topic to build the controlled graph.

    \paragraph{SemEval~\textnormal{\cite{kim2010semeval}}} contains 244 English
    scientific papers collected from the ACM Digital Libraries (conference and
    workshop papers). The papers represent four areas of Computer Science:
    Distributed Systems; Information Search and Retrieval; Distributed
    Artificial Intelligence -- Multiagent Systems; Social and Behavioral
    Sciences -- Economics. SemEval is divided into two disjoint sets: a training
    set containing 144 documents and a test set containing 100 documents. The
    associated keyphrases are provided by both authors and readers. As we do for
    DUC, we use keyphrases annotated on training documents of the same Computer Science
    area to build the controlled graph.

    \paragraph{}
    Table~\ref{tab:corpus_statistics} shows factual information about each
    dataset. Each dataset covers a different type of document. Inist contains the
    shortest documents where most reference keyphrases are missing within the text,
    DUC contains larger documents where most keyphrases can be found within the text
    and SemEval is composed of the largest documents with substantially less missing
    keyphrases than Inist and more than DUC. Moreover, we note differences
    between the diversity of the keyphrases. Diversity is computed as the ratio of
    unique keyphrases over the total number of annotated keyphrases and indicates
    whether or not the annotation is homogeneous, i.e., every documents have been annotated
    in the exact same manner. The keyphrase annotation on Inist is more homogeneous than
    the annotation on DUC and SemEval. The latter has a heterogeneous annotation that may
    handicap the keyphrase assignment.
    
  \begin{table*}
    \centering
    %\resizebox{\linewidth}{!}{
      \begin{tabular}{l|ccc@{}|ccc@{}|ccc@{~}}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Inist}} & \multicolumn{3}{c|}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}}\\
        \cline{2-10}
        & P & R & F$^{~~~~}$ & P & R & F$^{~~~~}$ & P & R & F$^{~~}$\\
        \hline
        TopicRank & 11.3 & 13.1 & 12.0$^{~~~~}$ & 17.8 & 22.7 & 19.7$^{~~~~}$ & 14.6 & 10.1 & 11.8$^{\ddagger}$\\
        \textit{Assignment} & 18.0 & 21.1 & 19.1$^{\dagger~~}$ & 24.3 & 31.2 & 27.0$^{\dagger~~}$ & ~~8.5 & ~~6.3 & ~~7.2$^{~~}$\\
        \hline
        TopicCoRank$_\textnormal{\textit{extr}}$ & 14.6 & 16.8 & 15.4$^{\dagger~~}$ & 25.5 & 32.4 & 28.1$^{\dagger~~}$ & 15.2 & 10.6 & 12.4$^{\ddagger}$\\
        TopicCoRank$_\textnormal{\textit{assign}}$ & \textbf{24.9} & \textbf{28.9} & \textbf{26.4$^{\dagger\ddagger}$} & 25.9 & 33.3 & 28.8$^{\dagger~~}$ & 11.6 & ~~8.3 & ~~9.5$^{~~}$\\
        \hline
        TopicCoRank & 19.0 & 22.0 & 20.1$^{\dagger~~}$ & \textbf{28.4} & \textbf{36.6} & \textbf{31.5$^{\dagger\ddagger}$} & \textbf{16.4} & \textbf{11.6} & \textbf{13.4$^{\ddagger}$}\\
        \bottomrule
      \end{tabular}
    %}
    \caption{Comparison of TopicCoRank with the baselines. Precision (P), Recall
             (R) and F-score (F) are reported in percentage. $\dagger$ and $\ddagger$
             indicate a significant F-score improvement over, respectively, TopicRank and
             \textit{Assignment}.
             \label{tab:comparison_results}}
  \end{table*}

  \subsection{Preprocessing}
  \label{subsec:preprocessing}
    We apply the following preprocessing steps to every document of the
    datasets: sentence segmentation, word tokenization and Part-of-Speech (POS)
    tagging. We perform sentence segmentation with the PunktSentenceTokenizer
    provided by the Python Natural Language ToolKit~\cite[NLTK]{bird2009nltk}.
    We tokenize the sentences into words using the NLTK TreebankWordTokenizer for
    English and the Bonsai word tokenizer\footnote{The Bonsai word tokenizer is
    a tool provided with the Bonsai PCFG-LA parser:
    \url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}.} for
    French. Finally, we use the Stanford POS
    tagger~\cite{toutanova2003stanfordpostagger} for English POS tagging and
    MElt~\cite{denis2009melt} for French POS tagging.

  \subsection{Baselines}
  \label{subsec:baselines}
    To show the effectiveness of our approach, we
    compare TopicCoRank with TopicRank and with a simple keyphrase assignment baseline,
    \textit{Assignment}, that considers training reference keyphrases as a
    controlled vocabulary and orders these by their number of occurrences within
    the document. Due to substential differences between TopicCoRank and KEA++, as
    well as the lack of readily available thesaurus, we do not compare
    TopicCoRank to KEA++.

    We also show the performances of TopicCoRank when it only extracts keyphrases
    (TopicCoRank$_\textnormal{\textit{extr}}$) and when it only assigns keyphrases
    (TopicCoRank$_\textnormal{\textit{assign}}$).
  \subsection{Evaluation measures}
  \label{subsec:evaluation_measures}
    To evaluate the performance of the keyphrase extraction methods, we use the
    common measures of precision (P), recall (R) and f-score (F):
    \begin{align}
        \text{P} = \frac{\text{\textit{TP}}}{\text{\textit{TP}}+\text{\textit{FP}}},\ \text{R} = \frac{\text{\textit{TP}}}{\text{\textit{TP}} + \text{\textit{FN}}},\ \text{F} = \frac{2 \text{PR}}{\text{P} + \text{R}}
    \end{align}
    where $\textnormal{\textit{TP}}$ is the total number of correctly extracted keyphrases (true positives),
    $\textnormal{\textit{FP}}$ is the total number of erroneously extracted keyphrases (false positives) and
    $\textnormal{\textit{FN}}$ is the total number of positive keyphrases that have not been extracted (false negatives).
    We cut-off the extracted keyphrases at the 10 best ranking ones and perform the
    comparisons with stemmed keyphrases.
    
    Consistent with \newcite{hassan2010conundrums}, we also report the performance of
    TopicCoRank and the baselines in terms of precision-recall curves. To generate the
    curves, we vary the evaluation cut-off from 1 to the total number of extracted and/or
    assigned keyphrases.

