\section{Related Work}
\label{sec:related_work}
  \subsection{Keyphrase extraction}
  \label{subsec:ake}
    Keyphrase extraction is the common approach to tackle the automatic
    keyphrase annotation task. Previous work proposed many approaches,
    including binary classification, statistical ranking and graph-based
    ranking of keyphrase candidates. As our approach uses graph-based
    ranking, we focus on the latter. For a detailed overview of
    keyphrase extraction methods, one may refer 
    to~\cite{hasan2014state_of_the_art}.

    % The unsupervised methods proposed so far use an important amount of
    % techniques, from frequency-based ranking to graph-based ranking.
    % TF-IDF~\cite{jones1972tfidf} is the most popular frequency-based ranking
    % scheme for keyphrase extraction. TF-IDF compares the number of occurrences
    % of a candidate in the document (Term Frequency, TF) with its specificity
    % (Inverse Document Frequency, IDF) to determine its importance within the
    % document.
    
    Since the seminal work of \newcite{mihalcea2004textrank}, graph-based
    ranking approaches to keyphrase extraction are becoming increasingly popular.
    The %basic plutot original dans le sens "originelle"
    original idea behind these approaches is to build a graph from 
    the input document and rank its nodes according to their importance 
    using centrality measures.
    
    In TextRank~\cite{mihalcea2004textrank}, the input document is represented as
    a co-occurrence graph in which nodes are words.
    Two words are connected by an edge if they co-occur in a fixed-sized window.
    A random walk algorithm is used to iteratively rank the words, and keyphrases
    are generated by concatenating the most important words.
    %
    %First introduced by TextRank~\cite{mihalcea2004textrank}, graph-based methods
    %construct a word co-occurrence graph, use a random walk algorithm to rank
    %the words by importance and finally find keyphrases based on the important
    %words.
    %The random walk algorithm simulates the ``voting concept'': a word
    %$V_i$ is important if it is connected with many words $V_j$ by edges $E_{ij}$
    %and if those words are important:
    
    The random walk algorithm simulates the ``voting concept'': a node is 
    important if it is connected with many other nodes, and if those nodes are 
    important.
    Formally, let $G=(V,E)$ be an undirected graph with a set of vertices $V$ and a 
    set of edges $E$, and $deg(V_i)$ the degree of $V_i$.
    The score of a vertex $V_i$ is computed iteratively until convergence 
    using the following equation~:
    %
    \begin{align}
      %S(V_i) = (1 - \lambda) + \lambda \sum_{E_{ij}}{\frac{S(V_j)}{|\{k \in V, \exists E_{jk}\}|}}
      S(V_i) = (1 - \lambda) + \lambda \sum_{E_{ij}}{\frac{S(V_j)}{deg(V_j)}}
    \end{align}
    where $\lambda$ is a damping factor defined to $0.85$ by
    \newcite{brin1998pagerank}.
    %TextRank has attracted attention and many
    %alternative graph-based rankings have been proposed.
    
    \newcite{wan2008expandrank} first added edge weights to the random walk
    and further improved the graph with co-occurrence information borrowed from
    similar documents. To extract keyphrases from a document, they first look for
    five similar documents, then use them to add new edges between words within
    the graph and reinforce the weight of existing edges.
    %established connections.
    
    \newcite{liu2010topicalpagerank} biased multiple graphs with topic
    probabilities. For each preliminary retrieved topic, a graph is biased with
    the conditional probabilities of the words given the topic. This method
    performs as many rankings as the number of topics and gives higher importance
    scores to high-ranking words for as much topics as possible. By doing so,
    \newcite{liu2010topicalpagerank} 
    %better cover the topics of the document with the keyphrase they extract.
    increase the topic coverage provided by the extracted keyphrases.
    
    
    \newcite{zhang2013wordtopicmultirank} %pursued 
    explored further the idea of
    \newcite{liu2010topicalpagerank}. 
    %Instead of multiple graphs biased by single topics, they used topics as graph nodes and performed co-ranking between words and topics. 
    Rather than using  multiple graphs biased by single topics, they added topics to graph nodes and performed co-ranking between words and topics.
    Topics %are judged important when they are connected to important words
    are regarded as important if they are connected to important words, and 
    words if they are connected to important topics and to
    other important words.
    
    \newcite{bougouin2013topicrank} %proposed 
    introduced TopicRank, a method that clusters keyphrase
    candidates in topics, ranks these topics, and %extracts 
    selects for each important
    topic one representative keyphrase. % which is viewed as the label topic. 
    %Dissimilar to previous graph-based methods,
    As our work extends that of \newcite{bougouin2013topicrank}, we describe their method 
    in detail in section~\ref{subsec:topicrank}.
    %Our work relates on TopicRank. We detail TopicRank more deeply in
    %section~\ref{subsec:topicrank}.
    % Unlike most graph-based methods, TopicRank doesn't rank words.
    % Ranking keyphrase candidates instead of words seems more legitimate for keyphrase
    % extraction. Moreover, clustering topically related candidates and extracting only
    % one candidate per cluster results in a set of non-redundant keyphrases that cover
    % %more 
    % the main topics of the document.

    % The supervised methods that tackle AKE mostly recast the problem as a
    % binary classification problem. Various classifiers are employed and many
    % discriminative features are proposed. KEA~\cite{witten1999kea} set the
    % foundation of supervised AKE. Using a Naive Bayes classifier and two
    % features, \newcite{witten1999kea} showed the efficiency of the first
    % offset position and the TF-IDF weight of a keyphrase candidate as features
    % for AKE. These features are used in combination with structural,
    % linguistic and other features in more complex systems such as
    % HUMB~\cite{lopez2010humb}, the top-ranking system of the keyphrase
    % extraction task at SemEval 2010~\cite{kim2010semeval}.

    % This work is based on the graph-based ranking method,
    % TopicRank~\cite{bougouin2013topicrank}. TopicRank topically clusters the
    % keyphrase candidates of the document, ranks the topics and extract the
    % most representative keyphrase for each of the $N$ most important topics.
    % Among other graph-based ranking methods, TopicRank is the only method that
    % proposes to cluster similar keyphrase candidates and to rank the clusters
    % instead of the words. By doing so, TopicRank avoids to extract redundant
    % keyphrases and further more gather the recommendation information of every
    % topically similar keyphrase candidate.

  \subsection{Keyphrase assignment}
  \label{subsec:aka}
    Automatic keyphrase assignment aims to provide coherent keyphrases for every
    document of a %same 
    domain. Its typical %usage 
    use is the indexing of documents for
    digital libraries. Keyphrases do not necessarily occur within the documents.
    %to allow a category mapping of the documents.
    They belong to  specialized domains or to the general language. Their role is to assign a predefined category to a document.
    Keyphrase assignment has %merely 
    seldom been employed for automatic keyphrase annotation.

    \newcite{medelyan2008smalltrainingset} proposed KEA++, a %simplified 
    shallow keyphrase assignment method.
    %refered to as automatic keyphrase indexing method by: je comprends pas ce que tu veux dire ....
    %\newcite{medelyan2008smalltrainingset} Pourquoi 2 fois la meme citation dans la meme phrase ? 
    %KEA++ selects candidate keyphrases using a domain-specific thesaurus 
     %and uses thesaurus information as classification feature of a binary classification algorithm. 
    KEA++ uses a domain-specific thesaurus to assign keyphrases to a document. Keyphrases are n-grams that could be matched to the thesaurus entries.
    %First, KEA++ selects n-grams within the documents and replaces ``ill-formed'' ones (e.g. with a plural flexion) with
    %their corresponding thesaurus entry. 
    N-grams %with no equivalent thesaurus entry 
    that do not match a thesaurus entry are either removed or substituted by a synonym that matches a thesaurus entry. 
    %The keyphrase assignment simplification appears at this step. 
    %Only occuring candidates and synonyms can be extracted. Tu l'as deja dit on a compris. Par contre on se demande comment les synumymes sont trouv√©s.
    Then, KEA++ %uses a thesaurus relation-based feature: the number of thesaurus links that connect a candidate to other candidates.
    exploits the semantic relationships between the thesaurus entries to connect keyphrase candidates.
    By leveraging a thesaurus, KEA++ achieves twice the performance of a similar method without a thesaurus.
    However, thesauri are not readily available for most domains and could be too general to perform accurate indexing. 
    %requires human effort. Also, there is not available thesaurus for every domain.
    The application scenarii of KEA++ are thus restricted.

    %\textcolor{red}{Finalement je mettrai peut-etre une partie sur le coranking ici... je sais pas}
    
    %The study of keyphrase extraction and keyphrase assigment state-of-the-art methods shows their individual merits. 
    %Combining both methods seems promising.% subject to the condition of a consistent modelling: our basic methodology will lay on clustering 
    %and graph-based ranking methods. 
    
    Keyphrase extraction and keyphrase assignment methods have their own strengths and weaknesses.
    Our work is, to our knowledge, the first attempt to perform both tasks in a joint model.
    
    
    

%    Another notable method is the one of \newcite{liu2011vocabularygap}. They
%    proposed to recast the AKA problem as a Statistical Machine Translation
%    (SMT) problem, where document and keyphrases represent the same object in
%    two (expressively inequal) languages. By doing so, they provide keyphrases
%    that either occur or not within the document. Their experiments have
%    been conducted using title and abstract phrases instead of terminological
%    entries. This setting differs from classic AKA setting, but the assummption
%    of \newcite{liu2011vocabularygap} makes their method applicable in more
%    cases than other AKA methods.
