\section{Related Work}
\label{sec:related_work}
  This section describes previous work on automatic keyphrase annotation (both
  AKE and AKA), as well as previous employments of graph co-ranking and their
  benefits to other NLP tasks.

  \subsection{Automatic Keyphrase Extraction (AKE)}
  \label{subsec:ake}
    AKE is the most used approach to tackle the automatic keyphrase annotation.
    Keyphrase candidates are selected within the document, e.g. based on
    linguistic patterns, then ranked by importance or classified as
    ``keyphrase'' or ``non-keyphrase''. The ranking is
    mostly performed by unsupervised methods while the classification is done by
    supervised methods, i.e. methods that learn from training data. Unsupervised
    methods extract the $N$ most important candidates as keyphrases; Supervised
    methods extract the $N$ candidates labeled ``keyphrases'' with the highest
    likelyhood as keyphrases.

    \subsubsection{Unsupervised AKE}
    \label{subsubsec:unsupervised_ake}
      The unsupervised methods proposed so far use an important amount of
      techniques, from frequency-based ranking to graph-based ranking.
      TF-IDF~\cite{jones1972tfidf} is the most popular frequency-based ranking
      scheme for keyphrase extraction. TF-IDF compares the number of occurrences
      of a candidate in the document (Term Frequency, TF) with its specificity
      (Inverse Document Frequency, IDF) to determine its importance within the
      document. Graph-based ranking methods are most recent, yet widely
      used~\cite{mihalcea2004textrank,wan2008expandrank,tsatsaronis2010semanticrank,liu2010topicalpagerank,bougouin2013topicrank}.
      First introduced by TextRank~\cite{mihalcea2004textrank} these methods
      construct a word co-occurrence graph, use a random walk algorithm to rank
      the words by importance and finally find keyphrases based on the important
      words. The random walk algorithm simulates the ``voting concept'',
      assuming that two words recommend each other when they co-occur in a given
      window of words. Alternative rankings have been proposed.
      \newcite{wan2008expandrank} and \newcite{tsatsaronis2010semanticrank}
      included edge weights to the random walk, using either co-occurrence
      numbers or semantic measures. \newcite{liu2010topicalpagerank} used
      multiple ranking biased with topic probabilities and
      \newcite{bougouin2013topicrank} ranked topics instead of words.

      This work is based on the unsupervised graph-based ranking method,
      TopicRank~\cite{bougouin2013topicrank}. TopicRank topically clusters the
      keyphrase candidates of the document, ranks the topics and extract the
      most representative keyphrase for each of the $N$ most important topics.
      Among other graph-based ranking methods, TopicRank is the only method that
      proposes to cluster similar keyphrase candidates and to rank the clusters
      instead of the words. By doing so, TopicRank avoids to extract redundant
      keyphrases and further more gather the recommendation information of all
      the topically similar keyphrase candidates.

    \subsubsection{Supervised AKE}
    \label{subsubsec:supervised_ake}
      The supervised methods that tackle AKE mostly recast the problem as a
      binary classification problem. Various classifiers are employed and many
      discriminative features are proposed. KEA~\cite{witten1999kea} set the
      foundation of supervised AKE. Using a Naive Bayes classifier and two
      features, \newcite{witten1999kea} showed the efficiency of the first
      offset position and the TF-IDF weight of a keyphrase candidate as features
      for AKE. These features are used in combination with structural,
      linguistic and other features in more complex systems such as
      HUMB~\cite{lopez2010humb}, the top-ranking system at the task 5 of the
      SemEval 2010 evaluation compaign~\cite{kim2010semeval}.

      In the current state of AKE, supervised methods perform better than
      unsupervised ones. However, their need for annotated data makes them
      suitable in fewer cases than unsupervised methods. Also, they become
      specific to the type of the training data. Our method uses such training
      data and suffers from specificity to them. However, the AKE part is
      unsupervised so our method can still work in the case of data-scarse
      domains.

  \subsection{Automatic Keyphrase Assignment (AKA)}
  \label{subsec:aka}
    AKA aims to provide coherent keyphrases for every document of the same
    domain. Its typical usage is the indexing of documents for digital
    libraries. Keyphrases do not necessarily occur within the documents and can
    be general terms to allow a category mapping of the documents.

    \newcite{medelyan2008smalltrainingset} are, in our knowledge, the first ones
    to propose an AKA method, namely KEA++. However, KEA++ is a simplified AKA,
    refered to as automatic keyphrase indexing by
    \newcite{medelyan2008smalltrainingset}. KEA++ is an improved version of KEA.
    KEA++ selects candidate keyphrases using a domain-specific thesaurus and add
    thesaurus information as classification features. The candidate selection
    first selects n-grams, then replaces the ``ill-formed" ones (e.g. with a
    plural flexion) with their corresponding thesaurus entry. N-grams with no
    equivalent thesaurus entry are either removed or replaced by a synonymous
    entry. The simplification appears at this step, as only occuring candidates
    and synonyms can be extracted, no general terms. The classification step is
    improved with a thesaurus relation-based feature: the number of thesaurus
    links that connect a candidate to the other candidates. These extensions to
    KEA induces a performance twice higher than the former.

    Another notable method is the one of \newcite{liu2011vocabularygap}. They
    proposed to recast the AKA problem as a Statistical Machine Translation
    (SMT) problem, where document and keyphrases represent the same object in
    two (expressively inequal) languages. By doing so, they provide keyphrases
    that either occur or not within the document. However, the experiments have
    been conducted using title and abstract phrases instead of thesaurus entries
    or reference keyphrases. Therefore, there is no clue about the performance
    of this method as as a proper AKA method.

    Our work presents a similarity with KEA++. Similar to KEA++, we leverage
    relations between candidates and controlled textual units. However, KEA++
    relies on qualitatively and linguistically better data than our method. The
    later can be an advantage, but the application scenarii of KEA++ are much
    more restricted than the application scenarii of our method.

  \subsection{Graph co-ranking for NLP}
  \label{subsec:graph_co_ranking_for_nlp}
    Graph co-ranking is a powerful technique that simultanously ranks different
    entities connected together. The ranking of specific entities is reinforced
    by the ranking of the other, somehow related, entities. In the past few
    years, this technique has been successfully employed to solve different NLP
    tasks~\cite{wan2011corankingsummarization,liu2014corankingopinionmining}.

    \newcite{wan2011corankingsummarization} used graph co-ranking to solve
    extractive cross-language summarization. To summarize a document written in
    a source language into a target language, the common approach is to either
    translate extracted salient sentences from the source language to the target
    language or translate the document to the target language and extract
    salient sentences from the translation. However,
    \newcite{wan2011corankingsummarization} stated that, due to translation
    errors, salient sentences in the source language may not be salient in the
    target language. \newcite{wan2011corankingsummarization} solved this problem
    by co-ranking source language sentences and target language sentences. He
    achieved the best performance on the standard evaluation dataset
    DUC-2001~\cite{over2001duc}

    \newcite{liu2014corankingopinionmining} leveraged graph co-ranking to
    extract opinion targets and opinion words from online reviews. Previous work
    mainly relied on the intuitions that opinion words usually co-occur with
    opinion targets in sentences, and that there is a strong modification
    relationship between them. However, \newcite{liu2014corankingopinionmining}
    argued that only considering opinion relations is insufficient and deplore
    the ignorance of word preferred collocations. Their co-ranking algorithm
    addresses these limitations and achieves higher results than previous work.

