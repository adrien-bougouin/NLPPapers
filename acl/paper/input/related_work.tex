\section{Related Work}
\label{sec:related_work}
  This section describes previous work on automatic keyphrase annotation (both
  AKE and AKA), as well as previous employments of graph co-ranking and their
  benefit to other NLP tasks.

  \subsection{Automatic Keyphrase Extraction (AKE)}
  \label{subsec:ake}
    AKE is the most used approach to tackle the automatic keyphrase annotation.
    Keyphrase candidates are selected within the document, e.g. based on
    linguistic patterns, then ranked by importance or classified as
    ``keyphrase'' or ``non-keyphrase''. The ranking is mostly performed by
    unsupervised methods while the classification is done by supervised methods,
    i.e. methods that learn from training data. Unsupervised methods extract the
    $N$ most important candidates; Supervised methods extract the $N$ candidates
    labeled ``keyphrases'' with the highest likelyhood.

    \subsubsection{Unsupervised AKE}
    \label{subsubsec:unsupervised_ake}
      The unsupervised methods proposed so far use an important amount of
      techniques, from frequency-based ranking to graph-based ranking.
      TF-IDF~\cite{jones1972tfidf} is the most popular frequency-based ranking
      scheme for keyphrase extraction. TF-IDF compares the number of occurrences
      of a candidate in the document (Term Frequency, TF) with its specificity
      (Inverse Document Frequency, IDF) to determine its importance within the
      document. Graph-based ranking methods are most recent, yet widely
      used~\cite{mihalcea2004textrank,wan2008expandrank,tsatsaronis2010semanticrank,liu2010topicalpagerank,bougouin2013topicrank}.
      First introduced by TextRank~\cite{mihalcea2004textrank} these methods
      construct a word co-occurrence graph, use a random walk algorithm to rank
      the words by importance and finally find keyphrases based on the important
      words. The random walk algorithm simulates the ``voting concept'',
      assuming that two words recommend each other when they co-occur in a given
      window of words. Alternative rankings have been proposed.
      \newcite{wan2008expandrank} and \newcite{tsatsaronis2010semanticrank}
      added edge weights to the random walk, using either co-occurrence numbers
      or semantic measures. \newcite{liu2010topicalpagerank} used multiple
      ranking biased with topic probabilities and
      \newcite{bougouin2013topicrank} ranked topics instead of words.

      This work is based on the graph-based ranking method,
      TopicRank~\cite{bougouin2013topicrank}. TopicRank topically clusters the
      keyphrase candidates of the document, ranks the topics and extract the
      most representative keyphrase for each of the $N$ most important topics.
      Among other graph-based ranking methods, TopicRank is the only method that
      proposes to cluster similar keyphrase candidates and to rank the clusters
      instead of the words. By doing so, TopicRank avoids to extract redundant
      keyphrases and further more gather the recommendation information of every
      topically similar keyphrase candidate.

    \subsubsection{Supervised AKE}
    \label{subsubsec:supervised_ake}
      The supervised methods that tackle AKE mostly recast the problem as a
      binary classification problem. Various classifiers are employed and many
      discriminative features are proposed. KEA~\cite{witten1999kea} set the
      foundation of supervised AKE. Using a Naive Bayes classifier and two
      features, \newcite{witten1999kea} showed the efficiency of the first
      offset position and the TF-IDF weight of a keyphrase candidate as features
      for AKE. These features are used in combination with structural,
      linguistic and other features in more complex systems such as
      HUMB~\cite{lopez2010humb}, the top-ranking system of the keyphrase
      extraction task at SemEval 2010~\cite{kim2010semeval}.

  \subsection{Automatic Keyphrase Assignment (AKA)}
  \label{subsec:aka}
    AKA aims to provide coherent keyphrases for every document of the same
    domain. Its typical usage is the indexing of documents for digital
    libraries. Keyphrases do not necessarily occur within the documents and can
    be general terms to allow a category mapping of the documents.

    \newcite{medelyan2008smalltrainingset} are, in our knowledge, the first ones
    to propose an AKA method, namely KEA++. However, KEA++ is a simplified AKA,
    refered to as automatic keyphrase indexing by
    \newcite{medelyan2008smalltrainingset}. KEA++ is an improved version of KEA.
    KEA++ selects candidate keyphrases using a domain-specific thesaurus and add
    thesaurus information as classification features. First, KEA++ selects
    n-grams and replaces ``ill-formed'' ones (e.g. with a plural flexion) with
    their corresponding thesaurus entry. N-grams with no equivalent thesaurus
    entry are either removed or replaced by a synonymous entry. The AKA
    simplification appears at this step. Only occuring candidates and synonyms
    can be extracted. Second, KEA++ improves KEA's classification with a
    thesaurus relation-based feature: the number of thesaurus links that connect
    a candidate to the other candidates. These addons to KEA induces a
    performance twice higher.

    Another notable method is the one of \newcite{liu2011vocabularygap}. They
    proposed to recast the AKA problem as a Statistical Machine Translation
    (SMT) problem, where document and keyphrases represent the same object in
    two (expressively inequal) languages. By doing so, they provide keyphrases
    that either occur or not within the document. Their experiments have
    been conducted using title and abstract phrases instead of terminological
    entries. This setting differs from classic AKA setting, but the assummption
    of \newcite{liu2011vocabularygap} makes their method applicable in more
    cases than other AKA methods.

    Our work presents a similarity with KEA++. Similar to KEA++, we leverage
    relations between candidates and controlled textual units. However, KEA++
    relies on qualitatively and linguistically richer data than our method.
    However, similar to the method of \newcite{liu2011vocabularygap}, the
    application scenarii of KEA++ are much more restricted than the application
    scenarii of our method.

  \subsection{Graph co-ranking for NLP}
  \label{subsec:graph_co_ranking_for_nlp}
    Graph co-ranking is a powerful technique that simultanously ranks different
    entities connected together. The ranking of specific entities is reinforced
    by the ranking of the other kinds of entities. In the past few years, this
    technique has been successfully employed to tackle different NLP
    tasks~\cite{wan2011corankingsummarization,liu2014corankingopinionmining}.

    \newcite{wan2011corankingsummarization} used graph co-ranking to solve
    extractive cross-language summarization. To summarize a document written in
    a source language into a target language, the common approach is to either
    translate extracted salient sentences from the source language to the target
    language or translate the document to the target language and extract
    salient sentences from the translation. However,
    \newcite{wan2011corankingsummarization} stated that, due to translation
    errors, salient sentences in the source language may not be salient in the
    target language. \newcite{wan2011corankingsummarization} solved this problem
    by co-ranking source language sentences and target language sentences. He
    achieved the best performance on the standard evaluation dataset
    DUC-2001~\cite{over2001duc}

    \newcite{liu2014corankingopinionmining} leveraged graph co-ranking to
    extract opinion targets and opinion words from online reviews. Previous work
    mainly relied on the intuitions that opinion words usually co-occur with
    opinion targets in sentences, and that there is a strong modification
    relationship between them. However, \newcite{liu2014corankingopinionmining}
    argued that only considering opinion relations is insufficient and deplore
    the ignorance of word preferred collocations. Their co-ranking algorithm
    addresses these limitations and achieves higher results than previous work.

