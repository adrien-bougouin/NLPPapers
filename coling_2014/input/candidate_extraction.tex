\section{Introduction}
\label{sec:section}

  Since the last decade, the amount of information available on the web is constantly increasing.
  While the number of documents continues to grow, the need for efficient information retrieval methods becomes increasingly important.
  One way to improve retrieval effectiveness is to use keyphrases~\cite{jones1999phrasier}.
  Keyphrases are single or multi-word expressions that represent the main content of a document.
  As they describe the key topics in documents, keyphrases are useful for many other tasks such as summarization~\cite{avanzo2005keyphrase} or document indexing~\cite{medelyan2008smalltrainingset}.
  Despite this, only a small number of documents have keyphrases associated with them.
  Therefore, the automatic keyphrase extraction task has attracted a lot of attention~\cite{kim2010semeval}.

  % Keyphrases are single or multi-word expressions that represent the main topics
  % or concepts addressed in a document. Keyphrases are useful in many tasks such
  % as document summarization~\cite{avanzo2005keyphrase} or document
  % indexing~\cite{medelyan2008smalltrainingset}. Since the last decade,
  % information mediums, such as Internet, provide access to huge amounts of
  % documents. However, many documents do not always have associated keyphrases
  % and a manual keyphrase annotation of them is not a viable solution. Therefore,
  % many researchers focus on the automatic keyphrase extraction task.

  The automatic keyphrase extraction task consists in the extraction of the
  most important textual units of a document. We distinguish two categories of
  keyphrase extraction methods: supervised methods and unsupervised methods. The
  first category of methods recasts the keyphrase extraction task as a binary
  classification task~\cite{witten1999kea}, whereas the second category often
  considers the keyphrase extraction task as a ranking
  task~\cite{hassan2010conundrums}. Although they handle the keyphrase
  extraction problem in their own different ways, supervised and unsupervised
  methods rely on the same preliminary steps. First, documents are preprocessed
  (word tokens, sentence boundaries, Part-of-Speech -- POS, etc.). Second,
  a set of keyphrase candidates is extracted from the documents. Keyphrase
  candidates are textual units that can be extracted as keyphrases; Keyphrase
  candidates must have properties known to be properties of human-assigned
  keyphrases.
  
  The selection of the keyphrase candidates (referred as candidate selection in
  the reset of this paper) is one of the most critical steps of the keyphrase
  extraction. The nature of the candidates must be carefully defined to suit
  the properties of human-assigned keyphrases and, most importantly, to avoid as
  much irrelevant candidates as possible. The range of proposed methods to
  select candidates is limitted. Commonly use methods select n-grams, NP-chunks
  or word sequences matching given patterns~\cite{hulth2003keywordextraction},
  while \newcite{kim2009termextraction} use a more sofisticated method that
  filters candidates according to statistics learnt from training data. Based on
  infered keyphrase properties
  (Section~\ref{sec:definition_of_candidate_keyphrases}), we argue the
  suitability of the commonly used methods and propose a new method that selects
  noun phrases and filters their irrelevant adjective modifiers
  (Section~\ref{sec:candidate_extraction}). Using three keyphrase extraction
  methods (Section~\ref{sec:keyphrase_extraction}), we analyse the quality of
  the candidate sets provide by each candidate selection methods and observe
  their impact on keyphrase extraction (Section~\ref{sec:evaluation}). Results
  show that the quality of the candidate sets produced by our method is better
  and induce a higher performance on each keyphrase extraction method.

\section{What is a Keyphrase?}
\label{sec:definition_of_candidate_keyphrases}
  In this section, we determine the properties of a keyphrase. First, we select
  standard keyphrase extraction datasets. Second, we extract statistics and
  infer keyphrase properties from their training sets.

  \subsection{Datasets}
  \label{subsec:keyphrase_extraction_datasets}
    Keyphrase extraction datasets are collections of documents paired with
    reference keyphrases given by authors, readers or both. In our work, we use
    three standard datasets that differ in terms of document size,  type and
    language.

    The \textbf{DUC} dataset \cite{over2001duc} is a collection of 308 English
    news articles covering about 30 topics (e.g.~tornadoes, gun control, etc.).
    This collection is the test dataset of the DUC-2001 summarization evaluation
    campaign and contains reference keyphrases annotated by
    \newcite{wan2008expandrank}. We split the collection into two sets: a
    training set containing 208 documents and a test set containing 100
    documents.

    The \textbf{SemEval} dataset \cite{kim2010semeval} contains 244 English
    scientific papers collected from the ACM Digital Libraries (conference and
    workshop papers). The papers are divided into two sets: a training set
    containing 144 documents and a test set containing 100 documents. The
    associated keyphrases are provided by both authors and readers.

    The \textbf{DEFT} dataset \cite{Paroubek2012deft} is a collection of 234
    French scientific papers belonging to the \textit{Humanities and Social
    Sciences} domain. DEFT is divided into two sets: a training set containing
    141 documents and a test set containing 93 documents. Keyphrases provided
    with the documents of DEFT are given by authors.

  \subsection{Analysis of Reference Keyphrases}
  \label{subsec:keyphrase_analysis}
    In this section, we aim to find general keyphrase properties from the
    analysis of the training sets of the previously mentioned datasets.

    Table~\ref{tab:train_dataset_statistics} shows statistics about the datasets
    and the keyphrases associated to their documents. First, keyphrases are presented
    regarding their number of words. Second, the multi-word keyphrases are
    presented regarding the Part-of-Speech of their words\footnote{We observed
    that keyphrases containing one word are mostly nouns or proper nouns. Hence,
    we only show the POS tag statistics of the multi-word keyphrases.}. To
    obtain these Part-of-Speech, we automatically POS tagged the keyphrases of
    the English datasets with the Stanford POS
    tagger~\cite{toutanova2003stanfordpostagger} and the keyphrases of the
    French dataset with MElt~\cite{denis2009melt}. To avoid tagging errors, POS
    tagged keyphrases were manually corrected. 
    \begin{table}
      \centering
      \begin{tabular}{lr|ccc}
        \toprule
        & \textbf{Statistic} & \textbf{DUC} & \textbf{SemEval} & \textbf{DEFT}\\
        \hline
        %\multirow{6}{*}[-2pt]{\begin{sideways}\textbf{Documents}\end{sideways}} & Language & English & English & French\\
        \multicolumn{2}{l|}{\textbf{Documents}}\\
        \multicolumn{2}{r|}{Number} & 208 & 144 & 141\\
        \multicolumn{2}{r|}{Tokens/document} & 912.0 & 5,134.6 & 7,276.7\\
        \multicolumn{2}{r|}{Keyphrases/document} & 8.1 & 15.4 & 5.4\\
        \multicolumn{2}{r|}{Missing keyphrases} & 3.9\% & 13.5\% & 18.2\%\\
        \hline
        \multicolumn{2}{l|}{\textbf{Keyphrases}}\\
        \multicolumn{2}{r|}{Unigrams} & 17.1\% & 20.2\% & 60.2\%\\
        \multicolumn{2}{r|}{Bigrams} & 60.8\% & 53.4\% & 24.5\%\\
        \multicolumn{2}{r|}{Trigrams} & 17.8\% & 21.3\% & $~~$8.8\%\\
        \multicolumn{2}{r|}{N-grams (N $\geq$ 4)} & $~~$4.3\% & $~~$5.2\% & $~~$6.6\%\\
        %& Quadrigrams & $~~$3.0\% & $~~$3.9\% & $~~$4.2\%\\
        %& N-grams (N $\geq$ 5) & $~~$1.3\% & $~~$1.3\% & $~~$2.4\%\\
        %\addlinespace[1.5\defaultaddspace]
        \hline
        \multicolumn{2}{l|}{\textbf{Multi-word keyphrases}}\\
        \multicolumn{2}{r|}{Containing noun(s)} & 94.5\% & 98.7\% & 93.3\%\\
        \multicolumn{2}{r|}{Containing proper noun(s)} & 17.1\% & $~~$4.3\% & $~~$6.9\%\\
        \multicolumn{2}{r|}{Containing (non-relational) adjective(s)} & 32.5\% & 40.5\% & 29.9\%\\
        \multicolumn{2}{r|}{Containing relational adjective(s)} & 20.0\% & 11.1\% & 37.2\%\\
        \multicolumn{2}{r|}{Containing verb(s)} & $~~$1.0\% & $~~$4.0\% & $~~$1.0\%\\
        \multicolumn{2}{r|}{Containing adverb(s)} & $~~$1.6\% & $~~$0.7\% & $~~$1.3\%\\
        \multicolumn{2}{r|}{Containing preposition(s)} & $~~$0.3\% & $~~$1.5\% & 31.2\%\\
        \multicolumn{2}{r|}{Containing determiner(s)} & $~~$0.0\% & $~~$0.0\% & 20.4\%\\
        %\multicolumn{2}{r}{Cont. others} & $~~$1.5\% & $~~$2.5\% & 11.8\%\\
        \bottomrule
      \end{tabular}
      \caption{Statistics of the training datasets. Missing keyphrases are
               keyphrases that do not occur in the documents.
               \label{tab:train_dataset_statistics}}
    \end{table}

    First, we observe (and confirm previous work observations) that most
    keyphrases are unigrams or bigrams ($\simeq$ 80\%). Hence, our first
    keyphrase property concerns the size of keyphrases.
    
    \begin{property}\label{prop:informativity}
      Keyphrases bear the minimum information representing an important topic or
      concept (e.g.~``T-2 Buckeye'' instead of ``two-seat T-2 Buckeye'').
    \end{property}

    Second, we observe from both English and French datasets that almost every
    keyphrase contains one or more nouns and half of the keyphrases are modified
    using one or more adjectives.
    %In French, the usage of prepositions and determiners is 
    %more frequent than in English. In English, prepositional expressions are
    %often replaced by non-prepositional variants, but such variants cannot be
    %generated in French. Finnaly, unexpected Part-of-Speech, such as foreign
    %words, are not rare in the DEFT dataset, which contains French scientific
    %papers refering to English technical terms.
    %
    Among these adjectives, it is important to note the usage of relational
    adjectives. Although they are less used than non-relational adjectives,
    their similar properties to
    nouns~\cite{bally1944linguistiquegeneraleetlinguistiquefrancaise} make them
    more likely to be relevant keyphrase modifiers than other adjectives.
    Indeed, adjectives, such as ``huge'', are not relevant keyphrase
    modifiers, but some candidate selection methods may, for example, only
    select ``huge wildfires'', leading to a miss of the rightful candidate
    ``wildfires''. In opposition, relational adjectives, such as
    ``presidential'', are noun substitutes that have a classificatory or
    taxonomic meaning~\cite{mcnally2004relationaladjectives}, which makes them
    more relevant as keyphrase modifiers.

    \begin{property}\label{prop:noun_phrases}
      Keyphrases are mostly nouns (e.g.~``storms'') that can be modified by one
      or more adjectives (e.g.~``\underline{annual} hurricane forecast'').
    \end{property}

    To give an idea of the observed POS tag patterns,
    Table~\ref{tab:best_patterns} shows the five most frequent POS tag patterns
    for English and French keyphrases.
    \begin{table}[h]
      \centering
      \begin{tabular}{r|lllll}
        \toprule
        \multicolumn{1}{r}{} & \multicolumn{4}{l}{\textbf{Pattern}} & \textbf{Example}\\
        \midrule
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{English}\end{sideways}} & \verb:Nc: & \verb:Nc: & & & \textit{``hurricane expert''}\\ % AP880409-0015
        & \verb:nA: & \verb:Nc: & & & \textit{``turbulent summer''}\\ % AP880409-0015
        & \verb:Nc: & & & & \textit{``storms''}\\ % AP880409-0015
        & \verb:rA: & \verb:Nc: & & & \textit{``Chinese earthquake''}\\ % AP890228-0019
        & \verb:nA: & \verb:Nc: & \verb:Nc: & & \textit{``annual hurricane forecast''}\\ % AP880409-0015
        \hline%\addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{French}\end{sideways}} & \verb:Nc: & & & & \textit{``patrimoine'' (``cultural heritage'')}\\ % as_2002_007048ar
        & \verb:Np: & & & & \textit{``Indonésie'' (``Indonesia'')}\\ % as_2001_000235ar
        & \verb:Nc: & \verb:rA: & & & \textit{``tradition orale'' (``oral tradition'')}\\ % as_2002_007048ar
        & \verb:Nc: & \verb:nA: & & & \textit{``anthropologie réflexive'' (``reflexive anthropology'')}\\ % as_2004_011288ar
        & \verb:Nc: & \verb:Sp: & \verb:D: & \verb:Nc: & \textit{``conservation de la nature'' (``nature conservation'')}\\ % as_2005_011742ar
        & \verb:Nc: & \verb:Sp: & \verb:Nc: & & \textit{``traduction en anglais'' (``English translation'')}\\ % meta_2003_006958ar
        \bottomrule
      \end{tabular}
      \caption{Frequent POS tag patterns. POS tags belong to the Multex format,
               except \texttt{rA} and \texttt{nA} which stands for,
               respectively, \textit{relational adjective} and
               \textit{non-relational adjective}.
               \label{tab:best_patterns}}
    \end{table}

\section{Candidate Selection}
\label{sec:candidate_extraction}
  In this section, we present the textual units that are commonly used as
  keyphrase candidates and discuss their consistency regarding the properties
  inferred in Section~\ref{sec:definition_of_candidate_keyphrases}. We also
  present a new method to selected refined noun phrases as keyphrase candidates.

  \paragraph{N-grams} are ordered sequences of $n$ words, where $n$ is usually
  set to 1 up to 3~\cite{witten1999kea}. Extracting n-grams has the benefit to
  provide almost every candidates that actually match reference keyphrases
  (maximum recall), but the counterpart is that it also provides a huge amount
  of unrelevant candidates. Therefore, \newcite{witten1999kea} propose to
  select only n-grams that do not contain a stop word (conjunction, preposition,
  determiner or common word) at their beginning or end. Filtered n-gram
  candidates are grammatically uncontrolled and do not fit
  properties~\ref{prop:informativity} and~\ref{prop:noun_phrases}.

  \paragraph{Textual units matching given POS tag patterns} are textual units of
  specific syntactic forms. Extracting such textual units ensures grammaticality
  and precisely defines the nature of the candidates. In previous work,
  \newcite{hulth2003keywordextraction} experiments with the most frequent POS
  tag patterns of her training data\footnote{Frequent patterns are the ones that
  appear at least ten times in the training data.}, whereas other researchers
  select the longest sequences of nouns, proper nouns and adjectives, namely
  the longest NPs~\cite{hassan2010conundrums}. Candidates selected using both
  approaches fit both properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}. However, the first approach needs training
  data and is, therefore, not suitable to every situation.

  \paragraph{NP-chunks} are non-recursive noun phrases.
  \newcite{hulth2003keywordextraction} uses them in her work and argues that
  they are less arbitrary and more linguistically justified than other
  candidates such as n-grams. Also, as NP-chunks are non-recursive (hence
  minimal) noun phrases, they are consistent with both
  properties~\ref{prop:informativity} and~\ref{prop:noun_phrases}.

  \paragraph{}
  As a contribution to the candidate selection, we propose to extract
  \textbf{refined noun phrases} (refined NPs) by adding a decision process
  during the selection of noun, proper noun and adjective sequences. According
  to the fact that some adjectives do not add important information to the noun
  phrase they modify (e.g.~``\underline{huge} wildfires''), we propose to
  select noun phrases and to keep their adjective modifier (if they have one)
  under specific conditions:
  \begin{enumerate}
    \item{the adjective is a relational adjective, or}
    \item{the adjective co-occurs at least two times with the noun phrase it
          modifies.}
  \end{enumerate}
  This method can be seen as a refinement of the selection of the longest noun
  phrases and NP-chunks. It also fits both properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}.

\section{Keyphrase Extraction}
\label{sec:keyphrase_extraction}
  Once candidates are select, the second step of the keyphrase extraction task
  is to classify them, as ``keyphrase'' or ``non-keyphrase'', or rank them in
  order to extract the $k$ bests as keyphrases. In this section, we detail the
  three keyphrase extraction methods that we use in our study. Two are different
  unsupervised methods (ranking methods) and one is a supervised method
  (classification method).

  \paragraph{TF-IDF~\textnormal{\cite{jones1972tfidf}}} is a weighting scheme
  that represents the significance of a word in a given document. Significant
  words must be both frequent in the document and specific to it. The
  specificity of a word is determined based on a collection of documents. That
  the lower is the amount of documents containing a given word, the higher is
  its specificity. Keyphrase candidates are scored according to the sum of the
  TF-IDF weights of their words and the $k$ best candidates are extracted as
  keyphrases.

  \paragraph{TopicRank~\textnormal{\cite{bougouin2013topicrank}}} aims to
  extract keyphrases that best represent the main topics of a document.
  Keyphrase candidates are clustered into topics using a stem overlap
  similarity, each topic is scored using the TextRank random walk
  algorithm~\cite{mihalcea2004textrank} and one representative keyphrase is
  extracted from each of the $k$ best ranked topics.

  \paragraph{KEA~\textnormal{\cite{witten1999kea}}} is a supervised method that
  uses a Naive Bayes classifier to extract keyphrases. The classifier combines
  two feature probabilities to predict whether a candidate is a ``keyphrase'' or
  a ``non-keyphrase''. The two features are the TF-IDF weight\footnote{The
    TF-IDF weight computed for KEA is based on candidate frequency, not word
  frequency.} of the candidate and the position of its first appearance in the
  document.

\section{Experiments}
\label{sec:evaluation}
  To observe the impact of the candidate selection methods, we perform two
  experiments. First, we compare the quality of the candidate selection methods
  with the set of reference keyphrases. Second, we compare their impact on the
  keyphrase extraction task, applying them to TF-IDF, TopicRank and KEA.

  \subsection{Evaluation Measures}
  \label{subsec:keyphrase_extraction_evaluation_measures}
    To quantify the capacity of the keyphrase candidate selection methods to
    provide suitable candidates and avoid irrelevant ones, we compute the
    number of selected candidates (Cand./Doc.) and confront it with the
    maximum recall (Rmax) that can be achieved. To do so, we compute a quality
    ratio (QR):
    \begin{align}
      \text{QR} &= \frac{\text{Rmax}}{\text{Cand./Doc.}} \times 100
    \end{align}
    The higher is the QR value of a candidate set, the better is its quality.

    To evaluate the performance of the keyphrase extraction methods, we use
    the common precision (P), recall (R) and f-score (F) measures, when a
    maximum of 10 keyphrases are extracted.

  \subsection{Preprocessing}
  \label{subsec:preprocessing}
    For each dataset, we apply the following preprocessing steps: sentence
    segmentation, word tokenization and Part-of-Speech tagging. For sentence
    segmentation, we use the PunktSentenceTokenizer provided by the python
    Natural Language ToolKit~\cite{bird2009nltk}. For word tokenization, we
    use the NLTK TreebankWordTokenizer for English and the Bonsai word
    tokenizer\footnote{The Bonsai word tokenizer is a tool provided with the
    Bonsai PCFG-LA parser:
    \url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}.} for
    French. As for Part-of-Speech tagging, we use the Stanford
    POS tagger~\cite{toutanova2003stanfordpostagger} for English and
    MElt~\cite{denis2009melt} for French.

  \subsection{Candidate Selection}
  \label{subsec:candidate_extraction}

    This section presents an intrinsic evaluation of the candidate selection
    methods described in Section~\ref{sec:candidate_extraction}. The aim is to
    compare the methods in terms of quantity of selected candidates and
    percentage of reference keyphrases that can be found in the best case
    (maximum recall).

    \subsubsection{Method Settings}
    \label{subsubsec:method_settings}
      For each candidate selection method presented in
      section~\ref{sec:candidate_extraction}, we configure one method with the
      parameters that best fit properties~\ref{prop:informativity}
      and~\ref{prop:noun_phrases}.

      \paragraph{}
      According to Property~\ref{prop:informativity}, we test a \textbf{filtered
      n-gram selection} method that provides small size n-grams:
      $n = \{1..3\}$. The stop words used for the filtering are part of the IR
      Multilingual
      Resources\footnote{\url{http://members.unine.ch/jacques.savoy/clef/index.html}}
      provided by the University of Neuchâtel (UniNE).

      \paragraph{}
      Following both Property~\ref{prop:noun_phrases} and previous
      work~\cite{hassan2010conundrums}, we use \textbf{pattern matching} to
      select the longest noun phrases (longest NPs), i.e.~the longest sequences
      of nouns, proper nouns and adjectives.

      \paragraph{}
      The \textbf{NP-chunk selection} is also performed using pattern matching.
      Only basic patterns are used:
      \begin{itemize}
        \item{\verb:Np+ | (A+ Nc) | Nc+:, for English datasets;}
        \item{\verb:Np+ | (A? Nc A+) | (A Nc) | Nc+:, for French datasets.}
      \end{itemize}

      \paragraph{}
      The \textbf{refined NPs} are also selected using pattern matching. The
      patterns we use are related to the position of relational adjectives in
      the target language:
      \begin{itemize}
        \item{\verb:A? (Nc | Np)+:, for English datasets;}
        \item{\verb:(Nc | Np)+ A?:, for French datasets.}
      \end{itemize}
      Relational adjectives are detected using the WordNet lexical
      database~\cite{miller1995wordnet} for English and its French translation,
      WoNeF~\cite{pradet2013wonef}, for French. Also, to detect other
      (potential) relational adjectives, we use a list of two-size suffixes
      automatically built from WordNet and WoNeF relational adjectives.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      Table~\ref{tab:candidate_extraction_statistics} shows the results of the
      candidate selection methods. The selection of n-grams provides a huge
      amount of candidates and allows a near perfect maximum
      recall\footnote{According to the amount of missing keyphrases of the test
      sets, the maximum recall that can be achieved is 97.2\% for DUC, 87.9\%
      for SemEval and 88.9\% for DEFT.}, whereas the other candidate selection
      methods provide less candidates and allow a lower maximum recall. However,
      for the selection of longest NPs, NP-chunks and refined NPs, the maximum
      recall does not significantly decrease compared to the number of selected
      candidates and according to the quality ratio, the method that selects
      better candidates is the one selecting refined NPs, followed by the ones
      selecting longest NPs and NP-chunks.
      \begin{table}
        \centering
        \begin{tabular}{r@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c@{~~}c}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule{8-10}
          & Cand./Doc. & Rmax & QR & Cand./Doc. & Rmax & QR & Cand./Doc. & Rmax & QR\\
          \midrule
          %\{1..2\}-grams & $~~~$491.0 & 76.6 & 15.6 & 1,633.6 & 61.0 & $~~~$3.7 & 2,566.4 & 67.3 & $~~~$2.6\\
          \{1..3\}-grams & $~~~$596.2 & 90.8 & 15.2 & 2,580.5 & 72.2 & $~~$2.8 & 4,070.2 & 74.1 & $~~~$1.8\\
          %Learned patterns & $~~~$317.6 & 90.6 & 28.5 & 1,227.4 & 69.8 & $~~~$5.7 & 2,148.3 & 76.5 & $~~~$3.6\\
          Longest NPs & $~~~$155.6 & 88.7 & 57.0 & $~~~$646.5 & 62.4 & $~~$9.7 & $~~~$914.5 & 61.1 & $~~$6.7\\
          NP-chunks & $~~~$149.9 & 76.0 & 50.7 & $~~~$598.4 & 56.6 & $~~$9.5 & $~~~$812.3 & 63.0 & $~~$7.8\\
          %Candidate terms & $~~~$161.8 & 46.1 & 28.5 & $~~~$498.6 & 32.4 & $~~~$6.5 & $~~~$647.0 & 52.8 & \textbf{$~~~$8.2}\\
          Refined NPs & $~~~$143.1 & 85.2 & \textbf{59.5} & $~~~$563.4 & 60.7 & \textbf{10.8} & $~~~$670.0 & 58.6 & \textbf{$~~$8.7}\\
          \bottomrule
        \end{tabular}
        \caption{Candidate selection statistics.
                 \label{tab:candidate_extraction_statistics}}
      \end{table}

      \TODO{Add an analysis of the selected candidates (as in section 2)?}

  \subsection{Keyphrase Extraction}
  \label{subsec:keyphrase_extraction}
    This section presents an extrinsic evaluation of the candidate selection
    methods. The aim is to observe the impact of the candidate selection
    methods on the keyphrase extraction task.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      Tables~\ref{tab:tfidf_results},~\ref{tab:topicrank_results}~and~\ref{tab:kea_results}
      show the performance of respectively TF-IDF, TopicRank and KEA when
      they extract keyphrases from keyphrase candidates provided by each
      candidate selection method.
      \begin{table}
        \centering
        \begin{tabular}{rccccccccc}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          %\{1..2\}-grams & 14.7 & 19.5 & 16.5 & 10.3 & $~~$7.0 & $~~$8.3 & $~~$8.1 & 15.1 & 10.4\\
          \{1..3\}-grams & 14.3 & 19.0 & 16.1 & $~~$9.0 & $~~$6.0 & $~~$7.2 & $~~$6.7 & 12.5 & $~~$8.6\\
          %\{1..4\}-grams & 13.7 & 18.2 & 15.4 & $~~$8.4 & $~~$5.6 & $~~$6.7 & $~~$6.7 & 12.5 & $~~$8.6\\
          %Learned patterns & 19.1 & 25.4 & 21.5 & 10.7 & $~~$7.3 & $~~$8.6 & $~~$7.0 & 13.1 & $~~$9.0\\
          Longest NPs & \textbf{24.2} & \textbf{31.7} & \textbf{27.1} & 11.7 & $~~$7.9 & $~~$9.3 & $~~$9.5 & 17.6 & 12.1\\
          NP-chunks & 21.1 & 28.1 & 23.8 & 11.9 & $~~$8.0 & $~~$9.5 & $~~$9.6 & 17.9 & 12.3\\
          %Sub-compounds & 22.8 & 29.9 & 25.5 & 10.8 & $~~$7.2 & $~~$8.6 & $~~$9.2 & 17.2 & 11.9\\
          %Acabit & 15.3 & 19.6 & 17.0 & $~~$8.6 & $~~$6.1 & $~~$7.1 & $~~$2.4 & $~~$5.6 & $~~$3.3\\
          %Terms & 17.4 & 23.2 & 19.6 & 11.5 & $~~$8.3 & \textbf{$~~$9.5} & 11.3 & 21.1 & \textbf{14.5}\\
          %Candidate terms & 17.4 & 23.2 & 19.6 & 11.2 & $~~$8.1 & $~~$9.3 & 11.3 & 21.1 & \textbf{14.5}\\
          Refined NPs & 24.2 & 31.6 & 27.0 & \textbf{12.3} & \textbf{$~~$8.4} &
     \textbf{$~~$9.8} & \textbf{10.0} & \textbf{18.3} & \textbf{12.7}\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate selection methods, when 10 keyphrases
                 are extracted by \textbf{TF-IDF}.
                 \label{tab:tfidf_results}}
      \end{table}
      \begin{table}
        \centering
        \begin{tabular}{rccccccccc}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          %\{1..2\}-grams & 10.2 & 14.1 & 11.7 & 11.9 & $~~$8.2 & \textbf{$~~$9.6} & $~~$5.8 & 11.0 & $~~$7.5\\
          \{1..3\}-grams & $~~$7.8 & 10.7 & $~~$8.9 & $~~$9.5 & $~~$6.7 & $~~$7.7 & $~~$6.2 & 11.4 & $~~$8.0\\
          %\{1..4\}-grams & $~~$7.1 & $~~$9.7 & $~~$8.1 & & & & & & \\
          %Learned patterns & 14.9 & 19.8 & 16.7 & 12.2 & $~~$8.5 & \textbf{$~~$9.9} & $~~$8.8 & 16.1 & 11.3\\
          Longest NPs & 17.7 & 23.2 & 19.8 & 11.6 & $~~$7.9 & $~~$9.3 & \textbf{11.6} & \textbf{21.5} & \textbf{14.9}\\
          NP-chunks & 13.3 & 21.5 & 18.3 & 11.7 & $~~$8.0 & $~~$9.4 & 11.1 & 20.7 & 14.4\\
          %Sub-compounds & 18.3 & 24.0 & \textbf{20.5} & 11.3 & $~~$7.7 & $~~$9.0 & 11.6 & 21.5 & \textbf{14.9}\\
          %Acabit & 11.2 & 14.2 & 12.3 & 10.2 & $~~$7.1 & $~~$8.3 & $~~$3.4 & $~~$7.9 & $~~$4.7\\
          %TermSuite & 10.4 & 13.9 & 11.7 & $~~$8.8 & $~~$6.4 & $~~$7.4 & $~~$9.6 & 18.5 & 12.4\\
          %Candidate terms & 10.4 & 13.9 & 11.7 & $~~$8.9 & $~~$6.5 & $~~$7.5 & $~~$9.6 & 18.5 & 12.4\\
          %Terms clusters & $~~$5.7 & $~~$7.9 & $~~$6.5 & $~~$6.4 & $~~$4.7 & $~~$5.4 & $~~$8.7 & 16.0 & 11.2\\
          % ATTENTION : le 19.9 est en fait un 19.8
          Refined NPs & \textbf{17.8} & \textbf{23.5} & \textbf{20.0} & \textbf{12.1} & \textbf{$~~$8.3} & \textbf{$~~$9.8} & 11.4 & 21.0 & 14.6\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate selection methods, when 10 keyphrases
                 are extracted by \textbf{TopicRank}.
                 \label{tab:topicrank_results}}
      \end{table}
      \begin{table}
        \centering
        \begin{tabular}{rccccccccc}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          %\{1..2\}-grams & 12.3 & 17.1 & 14.1 & 19.2 & 13.6 & 15.8 & 13.1 & 24.5 & 16.9\\
          \{1..3\}-grams & 12.0 & 16.6 & 13.7 & 19.4 & 13.7 & 15.9 & 13.4 & 25.3 & 17.3\\
          %Learned patterns & 12.9 & 17.8 & 14.8 & 19.6 & 13.8 & \textbf{16.1} & 14.7 & 27.6 & 19.0\\
          Longest NPs & 14.5 & 19.9 & 16.5 & 19.6 & 13.7 & 16.0 & 14.1 & 26.3 & 18.1\\
          NP-chunks & 13.5 & 18.6 & 15.4 & 19.5 & 13.7 & 16.0 & 14.3 & 26.8 & 18.4\\
          %Candidate terms & 12.5 & 17.2 & 14.3 & 13.9 & 10.1 & 11.6 & 14.7 & 28.0 & \textbf{19.1}\\
          Refined NPs & \textbf{14.5} & \textbf{20.1} & \textbf{16.6} & \textbf{20.5} & \textbf{14.4} & \textbf{16.8} & \textbf{14.4} & \textbf{27.1} & \textbf{18.6}\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate selection methods, when 10 keyphrases
                 are extracted by \textbf{KEA}.
                 \label{tab:kea_results}}
      \end{table}
      
      Gloabally, our method, followed by the selection of longest NPs and
      NP-chunks, is the one that induces the best performance for each method.
      This confirms that small candidate sets of high quality are better than
      exhaustive candidate sets such as n-grams. However, the results of KEA are
      more stable than the results of TF-IDF and TopicRank. KEA's learning step
      makes it less sensitive to irrelevant candidates that produce noise
      affecting the unsupervised methods. \TODO{Too short.}

\section{Conclusion}
\label{sec:conclusion}
  In this paper, we argued that the candidate selection is a critical step of
  the keyphrase extraction task and studied the impact of various candidate
  selections over different keyphrase extraction methods.

  According to the reference keyphrases of three standard datasets, we inferred
  two general keyphrase properties: (1) keyphrases are mostly noun phrases,
  which (2) are short (one, two or three words) and bear only sufficient
  information. Among the candidate selection methods, selecting textual units
  matching predefined patterns is the best ways to obtain keyphrase candidates
  that fit both properties.

  To confirm our assertions, we conducted two experiments to compare the quality
  of the candidate sets and observe their impact on the performance of different
  keyphrase extraction methods. Experimental results show that unsupervised
  methods do not have stable results depending on the candidate selection
  methods. We found that concise candidate sets allowing a high maximum recall
  induce better results than huge candidate sets allowing a highest maximum
  recall. In other words, the quality of a candidate set prevails over its
  exhaustivity.

  We also presented a new candidate selection method that defines noun phrase
  patterns and filter their adjective modifier in order to keep only relevant
  ones. Based on the assumption that relational adjectives act as nouns, we
  decided to keep them, along with the adjectives that frequently modifies the
  same noun phrase.

  Our work states that adjectives must not necessary be keyphrase modifiers and
  we showed that simple linguistic filters can increase the quality of the
  selected candidates. Hence, future work on candidate selection might focus on
  more complex linguistic and/or statistical filtering methods for adjectives.

