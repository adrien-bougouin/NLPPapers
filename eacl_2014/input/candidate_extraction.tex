\section{Introduction}
\label{sec:section}
  Keyphrases are single or multi-word expressions that represent the main topics
  or concepts addressed in a document. Keyphrases are useful in many tasks such
  as document summarization~\cite{avanzo2005keyphrase}, document
  clustering~\cite{han2007webdocumentclustering}, information
  retrieval~\cite{medelyan2008smalltrainingset} or multi-sentence
  compression~\cite{boudin2013multisentencecompression}.
  Since the last decade, information mediums, such as the Internet, give us an
  access to huge amounts of documents. To ease their retrieval and to provide an
  overview of them for end users, the mentioned tasks are needed. However,
  keyphrases are not always associated to documents. Researchers tackle this
  problem by automatically extracting keyphrases.

  The automatic keyphrase extraction task consists of the extraction of the
  textual units that represent the main topics, or concepts, of the document
  they belong to. Automatic keyphrase extraction methods are divided into two
  categories: supervised and unsupervised methods. Supervised methods typically
  recast the keyphrase extraction task as a binary classification
  task~\cite{witten1999kea}. For unsupervised methods, the keyphrase extraction
  task is often considered as a ranking task and different approaches are
  proposed~\cite{hassan2010conundrums}.

  Although supervised and unsupervised methods handle the keyphrase extraction
  problem differently, they rely on the same preprocessing steps. Documents are
  preprocessed to add linguistic knowledge, such as Part-Of-Speech (POS), and
  the textual units that fit predefined keyphrase properties are extracted as
  keyphrase candidates. In this paper, we study the impact of the candidate
  extraction step on the keyphrase extraction task. This step is the most
  critical step, because only keyphrase candidates can be extracted as
  keyphrases.

  Various methods are commonly employed to extract keyphrase candidates.
  Usually, the methods extract filtered n-grams, NP-chunks or word sequences
  matching given patterns~\cite{hulth2003keywordextraction}. Also, some
  researchers, such as \newcite{kim2009reexaminingautomatickeyphraseextraction},
  propose more specific keyphrase candidates using statistical measures.
  However, no previous work compares the existing methods and their impact on
  different categories of keyphrase extraction methods. We do this by comparing
  the number of extracting candidates and the maximum recall that can be
  achieved and by comparing the results of keyphrase extraction methods
  obtained with the candidate extraction methods.

  Another contribution of our work is the use of terminological phrases (terms)
  as keyphrase candidates. Terms are grammatically arranged sequences of
  words designating a concept and treated as single units. Hence, we believe
  that terms are suitable keyphrase candidates.

  This paper is organized as follows.
  Section~\ref{sec:definition_of_candidate_keyphrases} defines the properties of
  keyphrases, Section~\ref{sec:candidate_extraction} presents the methods for
  the extraction of keyphrase candidates and
  Section~\ref{sec:keyphrase_extraction} describes the three keyphrase
  extraction methods used in our experiments, which are presented in
  Section~\ref{sec:evaluation}. Finally, Section~\ref{sec:conclusion} concludes
  this work and discusses about future work.

\section{What is a Keyphrase?}
\label{sec:definition_of_candidate_keyphrases}
  In this section, we determine the syntactic properties of a keyphrase. To do
  that, we analyse the reference keyphrases provided with three standard
  evaluation datasets.

  \subsection{Datasets}
  \label{subsec:keyphrase_extraction_datasets}
    Keyphrase extraction datasets are collections of documents paired with
    reference keyphrases given by authors, readers or both. In our work, we use
    three standard datasets that differ in terms of document size,  type and
    language.

    The \textbf{DUC} dataset \cite{over2001duc} is a collection of 308 English
    news articles covering about 30 topics (e.g.~tornadoes, gun control, etc.).
    This collection is the test dataset of the DUC-2001 summarization evaluation
    campaign. This part of DUC-2001 contains reference keyphrases annotated by
    \newcite{wan2008expandrank}. We split the collection into two sets: a
    training set containing 208 documents and a test set containing 100
    documents.

    The \textbf{SemEval} dataset \cite{kim2010semeval} contains 284 English
    scientific papers collected from the ACM Digital Libraries (conference and
    workshop papers). The papers are divided into three sets: a trial set
    containing 40 documents (unused in this work), a training set containing 144
    documents and a test set containing 100 documents. The associated keyphrases
    are provided by both authors and readers.

    The \textbf{DEFT} dataset \cite{Paroubek2012deft} is a collection of 234
    French scientific papers belonging to the Humanities and Social Sciences
    domain. DEFT is divided into two sets: a training set containing 141
    documents and a test set containing 93 documents. Keyphrases provided with
    the documents of DEFT are given by authors.

  \subsection{Analysis of Reference Keyphrases}
  \label{subsec:keyphrase_analysis}
    In this section, we aim to find the syntactic properties of most keyphrases,
    for English and French, by analysing the training sets of the datasets. The
    datasets have different characteristics (size, type, keyphrase annotators,
    etc.), wich allows us to infer general keyphrase properties.

    Table~\ref{tab:train_dataset_statistics} shows statistics about the dataset
    documents and their associated keyphrases. First, keyphrases are presented
    regarding their number of words, which is a clue about their degree of
    conciseness. Second, the keyphrases are presented regarding the
    Part-Of-Speech of their words. To obtain these Part-Of-Speech, we
    automatically POS tagged the keyphrases with the Stanford POS
    tagger~\cite{toutanova2003stanfordpostagger} for English and
    MElt~\cite{denis2009melt} for French. To avoid tagging errors, the given POS
    tags were manually corrected.
    \begin{table}[h]
      \centering
      \begin{tabular}{@{~}r@{~}r@{~}c@{~}c@{~}c@{~}}
        \toprule
        & \multirow{2}{*}[-2pt]{\textbf{Statistic}} & \multicolumn{3}{c}{\textbf{Corpus}}\\
        \cmidrule{3-5}
        & & DUC & SemEval & DEFT\\
        \midrule
        \multirow{6}{*}[-2pt]{\begin{sideways}\textbf{Documents}\end{sideways}} & Language & English & English & French\\
        & Type & News & Papers & Papers\\
        & Number & 208 & 144 & 141\\
        & Token average & 912.0 & 5134.6 & 7276.7\\
        & Keyphrase average & 8.1 & 15.4 & 5.4\\
        & Missing keyphrases & 3.9\% & 13.5\% & 18.2\%\\
        \addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{Keyphrases}\end{sideways}} & Unigrams & 17.1\% & 20.2\% & 60.2\%\\
        & Bigrams & 60.8\% & 53.4\% & 24.5\%\\
        & Trigrams & 17.8\% & 21.3\% & $~~$8.8\%\\
        & Quadrigrams & $~~$3.0\% & $~~$3.9\% & $~~$4.2\%\\
        & N-grams (N $\geq$ 5) & $~~$1.3\% & $~~$1.3\% & $~~$2.4\%\\
        \addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{Multi-word keyphrases}\end{sideways}} & Cont. nouns & 94.5\% & 98.7\% & 93.3\%\\
        & Cont. proper nouns & 17.1\% & $~~$4.3\% & $~~$6.9\%\\
        & Cont. adjectives & 50.0\% & 50.2\% & 65.5\%\\
        & Cont. verbs & $~~$1.0\% & $~~$4.0\% & $~~$1.0\%\\
        & Cont. adverbs & $~~$1.6\% & $~~$0.7\% & $~~$1.3\%\\
        & Cont. prepositions & $~~$0.3\% & $~~$1.5\% & 31.2\%\\
        & Cont. determiners & $~~$0.0\% & $~~$0.0\% & 20.4\%\\
        & Cont. others & $~~$1.5\% & $~~$2.5\% & 11.8\%\\
        \addlinespace[.5\defaultaddspace]
        \bottomrule
      \end{tabular}
      \caption{Statistics of the training datasets.
               %As a matter of consistency regarding
               %the evaluation of keyphrase extraction methods, the missing
               %keyphrases are determined based on the stemmed forms.
               \label{tab:train_dataset_statistics}}
    \end{table}

    From Table~\ref{tab:train_dataset_statistics}, we observe that most of the
    keyphrases are unigrams or bigrams ($\simeq$~$80\%$). Keyphrases are more
    often single words or concise expressions.
    
    \begin{property}\label{prop:informativity}
      Keyphrases bear the minimum information representing an important topic or
      concept (e.g.~``T-2 Buckeye'' instead of ``two-seat T-2 Buckeye'').
    \end{property}

    Table~\ref{tab:train_dataset_statistics} shows the ratio of keyphrases that
    contain specific POS tags. We observed that keyphrases containing one word
    are mostly nouns or proper nouns. Hence we only show the ratios for the
    multiwod keyphrases. In both English and French, almost every keyphrases
    contain nouns and half of the keyphrases are modified by one or more
    adjectives. In French, we observe that the usage of prepositions and
    determiners is more frequent that in English. In English, prepositionnal
    expressions are often replaced by non-prepositionnal variants, but such
    variants cannot be generated in French. Also, unexpected Part-Of-Speech, such
    as foreign words, are not rare in the DEFT dataset, which contains French
    scientific papers that refer to English technical terms.

    \begin{property}\label{prop:noun_phrases}
      Keyphrases are mostly nouns (e.g.~``storms'') that can be modified by one
      or more adjectives (e.g.~``\underline{annual} hurricane forecast'').
    \end{property}

    To give an idea of the observed syntactic patterns,
    Table~\ref{tab:best_patterns} shows the five most frequent POS tag patterns
    for English and French.
    \begin{table*}
      \centering
      \begin{tabular}{@{~}rl@{~~}l@{~~}l@{~~}ll@{~}}
        \toprule
        & \multicolumn{4}{l}{\hspace{-.5em}\textbf{Pattern}} & \textbf{Example}\\
        \midrule
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{English}\end{sideways}} & Nc & Nc & & & \textit{``hurricane expert''}\\ % AP880409-0015
        & A & Nc & & & \textit{``turbulent summer''}\\ % AP88049-0015
        & Nc & & & & \textit{``storms''}\\ % AP880409-0015
        & A & Nc & Nc & & \textit{``annual hurricane forecast''}\\ % AP880409-0015
        & Nc & Nc & Nc & & \textit{``hurricane reconnaissance flights''}\\ % AP890529-0030
        \addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{French}\end{sideways}} & Nc & & & & \textit{``patrimoine'' (``cultural heritage'')}\\ % as_2002_007048ar
        & Nc & A & & & \textit{``tradition orale'' (``oral tradition'')}\\ % as_2002_007048ar
        & Np & & & & \textit{``Indonésie'' (``Indonesia'')}\\ % as_2001_000235ar
        & Nc & Sp & D & Nc & \textit{``conservation de la nature'' (``nature conservation'')}\\ % as_2005_011742ar
        & Nc & Sp & Nc & & \textit{``traduction en anglais'' (``English translation'')}\\ % meta_2003_006958ar
        \bottomrule
      \end{tabular}
      \caption{Frequent part-of-speech patterns (Multex format) for English and
               French keyphrases. \label{tab:best_patterns}}
    \end{table*}

\section{Candidate Extraction}
\label{sec:candidate_extraction}
  The aim of the candidate extraction is to determine the textual units that
  could be extracted as keyphrases. This step removes noise, i.e.~unrelevant
  textual units that may deteriorate the performance of the keyphrase
  extraction, and reduces the computation time required to extract the
  keyphrases. We distinguish two categories of candidates: the positive
  candidates, which match reference keyphrases, and the off-target candidates,
  which do not match reference keyphrases. Among the off-target candidates, we
  also distinguish the information bearing candidates, which can help to improve
  the keyphrase extraction, and the unrelevant candidates, which are common
  expressions or belong to an inter-/transdisciplinary lexicon.

  In this section, we present the textual units that are commonly used as
  candidates and we propose the terminological phrases as keyphrase candidates.
  We also discuss their consistency regarding the properties infered in
  Section~\ref{sec:definition_of_candidate_keyphrases}.

  \paragraph{N-grams} are ordered sequences of $n$ words. In previous work,
  every word sequences of size $n = \{1..3\}$ are
  extracted~\cite{witten1999kea,turney1999learningalgorithms,hulth2003keywordextraction}.
  Extracting n-grams is the best way to obtain as much positive or information
  bearing candidates as possible, but it also gives many unrelevant candidates.
  \newcite{witten1999kea} propose to extracted only keyphrases that do not
  contain a stop word(conjunction, preposition, determiner or common word) at
  its beginning or end. This candidate extraction method is exhaustive and do
  not strictly fit properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}.

  \paragraph{Textual units matching given POS tag patterns} are textual units of
  a specific syntactic form. Extracting such textual units have the benefit to
  ensure the grammaticality and to precisely define the nature of the
  candidates. In previous work, \newcite{hulth2003keywordextraction} experiments
  with the most frequent POS tag patterns of her training data\footnote{Frequent
  patterns are the ones that appear at least ten times in the reference.},
  whereas other researchers extract the longest sequences of nouns, proper nouns
  and adjectives, namely the longest NPs~\cite{hassan2010conundrums}. Both
  approaches fit Property~\ref{prop:noun_phrases}, but not always
  Property~\ref{prop:informativity}. Also, the first approach requires training
  data, whereas the second one defines a more generic pattern that requires a
  limited adaptation to different languages.

  \paragraph{NP-chunks} are non-recursive noun phrases.
  \newcite{hulth2003keywordextraction} uses them in her experiments and argues
  that they are less arbitrary and more linguistically justified than other
  candidates, such as n-grams. Also, as NP-chunks are minimal noun phrases,
  they are consistent with both properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}.

  \paragraph{Terminological phrases (terms)} are word sequences designating a
  concept and treated as single units for a specific domain. Considering that a
  keyphrase represents a topic or a concept, it seems relevant to extract
  keyphrases from  a set of terminological phrases. Besides, extracting
  terminological phrases as candidates is linguistically justified and
  linguistically justified. Also, terms are often extracted using POS tag
  patterns consistent with properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}~\cite{castellvi2001automatictermdetection}.

%  In their review of automatic term extraction methods,
%  \newcite{castellvi2001automatictermdetection} stated that alongside term
%  extraction there is the document indexing task. Indeed, the textual units that
%  index a given document, i.e.~best describe its content, often are
%  terminological phrases. Hence, we believe that the complex noun phrase
%  sub-compounding method of \newcite{evans1996nounphraseanalysis} is suitable
%  for keyphrase candidate extraction. The aim of their method is to extract both
%  complex noun phrases (e.g.~\textit{``the quality of surface of treated
%  stainless steel strip''}) and their meaningful sub-compounds. Four types of
%  sub-compounds are defined:
%  \begin{enumerate}
%    \item{Lexical atoms, i.e.~semantically coherent phrases
%          (e.g.~\textit{``stainless steel''})
%          \label{item:lexical_atom}}
%    \item{Head modifier pairs (e.g.~\textit{``treated steel''})
%          \label{item:head_modifier}}
%    \item{Cross preposition modification pairs (e.g.~\textit{``surface
%          quality''})
%          \label{item:cross_preposition_modifier}}
%    \item{Sub-compounds (e.g.~\textit{``stainless steel strip''})
%          \label{item:subcompound}}
%  \end{enumerate}
%  The \ref{item:lexical_atom}$^\text{st}$ and the
%  \ref{item:subcompound}$^\text{th}$ types represent continuous pairs, whereas
%  the \ref{item:head_modifier}$^\text{nd}$ and the
%  \ref{item:cross_preposition_modifier}$^\text{rd}$ types can be both continuous
%  or discontinuous pairs. In our work, we do not consider the head modifiers
%  (\ref{item:head_modifier}) and the cross preposition modification pairs
%  (\ref{item:cross_preposition_modifier}), because we aim to only extract
%  keyphrases that appear continuously in the analyzed document.

\section{Keyphrase Extraction}
\label{sec:keyphrase_extraction}
  Once keyphrase candidates are extracted, the second step of the keyphrase
  extraction task is to classify them as ``keyphrase'', or ``non-keyphrase'', or
  rank them and select the $k$ bests as keyphrases. Usually, the classification
  is performed by supervised methods, whereas the ranking is performed by
  unsupervised methods. Supervised methods are usually more efficient than
  unsupervised methods, because they learn what are keyphrases based on
  annotated data.

  In this section, we detail the three keyphrase extraction methods that we use
  in our study.

  \paragraph{TF-IDF~\textnormal{\cite{jones1972tfidf}}} is a weighting scheme
  that represents the significance of a word in a given document. A significant
  word must be both frequent in the document and specific to it. The specificity
  of a word is determined based on its appearance within the documents of a
  given collection. The intuition is that the lower is the amount of documents
  containing a word, the higher is the specificity of the word.

  The keyphrase candidates are scored according to the sum of the TF-IDF weights
  of their words and the $k$ candidates with the highest scores are extracted as
  keyphrases.

  \paragraph{TopicRank~\textnormal{\cite{bougouin2013topicrank}}} aims to
  extract keyphrases that best represent the main topics of a document. One
  keyphrase is extracted from each of the $k$ most significant topics, leading
  to a set of topically unredundant keyphrases.

  Keyphrase candidates are clustered into topics using a Jaccard similarity and
  a Hierarchical Agglomerative Clustering, each topic is scored using the
  TextRank random walk~\cite{mihalcea2004textrank} and one representative
  keyphrase from each of the $k$ best ranked topics is extracted.

  \paragraph{KEA~\textnormal{\cite{witten1999kea}}} is a supervised method that
  uses a Naive Bayes classifier to extract keyphrases. The classifier combines
  two feature probabilities to give the class ``keyphrase'' or ``non-keyphrase''
  to a candidate. The two features are the TF-IDF weight\footnote{The TF-IDF
  weight computed for KEA is based on candidate frequency, not word frequency.}
  of the candidate and the position of its first appearance in the document.

\section{Experiments}
\label{sec:evaluation}
  To better understand the candidate extraction methods, we perform two
  evaluations. The first evaluation shows the quantity of extracted candidates
  compared to the quantity of extracted positive candidates. The second
  evaluation compares the candidate extraction method when they are used by
  either TF-IDF, TopicRank or KEA.

  \subsection{Experimental Settings}
  \label{subsec:evaluation_settings}
    \subsubsection{Datasets}
    \label{subsubsec:datasets}
      The datasets used to evaluate the methods are the test sets of DUC,
      SemEval and DEFT (see Section~\ref{subsec:keyphrase_extraction_datasets}).
      Table~\ref{tab:test_dataset_statistics} reports the information about
      these test sets. As they are coherent with the training sets, we can make
      usage of the properties inferred in
      Section~\ref{sec:definition_of_candidate_keyphrases}.
      \begin{table}
        \centering
        \begin{tabular}{@{~}r@{~}c@{~}c@{~}c@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Statistic}} & \multicolumn{3}{c}{\textbf{Corpus}}\\
          \cmidrule{2-4}
          & DUC & SemEval & DEFT\\
          \midrule
          Language & English & English & French\\
          Type & News & Papers & Papers\\
          Documents & 100 & 100 & 93\\
          Token average & 877.3 & 5177.7 & 6839.4\\
          Keyphrase average & 7.9 & 14.7 & 5.2\\
          Tokens/keyphrase & 2.1 & 2.1 & 1.6\\
          Missing keyphrases & 2.8\% & 22.1\% & 21.1\% \\
          \bottomrule
        \end{tabular}
        \caption{Statistics of the test datasets.
                 %As a matter of consistency regarding
                 %the evaluation of keyphrase extraction methods, the missing
                 %keyphrases are determined based on the stemmed forms.
                 \label{tab:test_dataset_statistics}}
      \end{table}

    \subsubsection{Preprocessing}
    \label{subsubsec:preprocessing}
      For each dataset, we apply the following preprocessing steps: sentence
      segmentation, word tokenization and part-of-speech tagging. For word
      tokenization, we use the TreebankWordTokenizer provided by the python
      Natural Language ToolKit~\cite{bird2009nltk} for English and the Bonsai
      word tokenizer\footnote{The Bonsai word tokenizer is a tool provided with
      the Bonsai PCFG-LA parser:
      \url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}.} for
      French. For part-of-speech tagging, we use the Stanford
      POS-tagger~\cite{toutanova2003stanfordpostagger} for English and
      MElt~\cite{denis2009melt} for French.

    \subsubsection{Evaluation Measures}
    \label{subsubsec:keyphrase_extraction_evaluation_measures}
      To quantify the capacity of the keyphrase candidate extraction methods to
      provide suitable candidates and avoid unrelevant ones, we compute the
      number of extracted candidates and confront it with the maximum recall
      (Rmax) that can be achieved.

      The performance of the keyphrase extraction methods is expressed in terms
      of precision (P), recall (R) and f-score (f1-measure, F) when a maximum of
      10 keyphrases are extracted.

  \subsection{Candidate Extraction}
  \label{subsec:candidate_extraction}
%    \begin{table*}
%      \centering
%      \begin{tabular}{rcccccc}
%        \toprule
%        \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{2}{c}{\textbf{DUC}} & \multicolumn{2}{c}{\textbf{SemEval}} & \multicolumn{2}{c}{\textbf{DEFT}}\\
%        \cmidrule(r){2-3}\cmidrule(lr){4-5}\cmidrule(l){6-7}
%        & Candidates & Rmax & Candidates & Rmax & Candidates & Rmax\\
%        \midrule
%        \{1..2\}-grams & $~~$49098 & 76.6 & 163358 & 61.0 & 238678 & 67.3\\
%        \{1..3\}-grams & $~~$59623 & \textbf{90.8} & 258054 & \textbf{72.2} & 378526 & 74.1\\
%        %\{1..4\}-grams & $~~$78024 & 92.6 & 365151 & 74.1 & 533753 & 78.2\\
%        Learned patterns & $~~$31764 & 90.6 & 122741 & 69.8 & 199789 & \textbf{76.5}\\
%        Longest NPs & $~~$15559 & 88.7 & $~~$64649 & 62.4 & $~~$85047 & 61.1\\
%        NP-chunks & $~~$14994 & 76.0 & $~~$59839 & 56.6 & $~~$75548 & 63.0\\
%        Sub-compounds & $~~$17181 & 90.6 & $~~$71224 & 64.4 & $~~$86866 & 61.1\\
%        Acabit & $~~~~$2377 & 26.7 & $~~$13214 & 17.6 & $~~$11106 & 13.4\\
%        TermSuite & $~~$16173 & 46.1 & $~~$49449 & 32.3 & $~~$60162 & 52.8\\
%        \bottomrule
%      \end{tabular}
%      \caption{Candidate extraction statistics. Rmax stands for maximum recall,
%               i.e.~the percentage of candidates that match with reference
%               keyphrases. \label{tab:candidate_extraction_statistics}}
%    \end{table*}
    \begin{table*}
      \centering
      \begin{tabular}{@{~}rcccccc@{~}}
        \toprule
        \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{2}{c}{\textbf{DUC}} & \multicolumn{2}{c}{\textbf{SemEval}} & \multicolumn{2}{c}{\textbf{DEFT}}\\
        \cmidrule(r){2-3}\cmidrule(lr){4-5}\cmidrule(l){6-7}
        & Cand./Doc. & Rmax & Cand./Doc. & Rmax & Cand./Doc. & Rmax\\
        \midrule
        \{1..2\}-grams & $~~$491.0 & 76.6 & 1633.6 & 61.0 & 2566.4 & 67.3\\
        \{1..3\}-grams & $~~$596.2 & \textbf{90.8} & 2580.5 & \textbf{72.2} & 4070.2 & 74.1\\
        Learned patterns & $~~$317.6 & 90.6 & 1227.4 & 69.8 & 2148.3 & \textbf{76.5}\\
        Longest NPs & $~~$155.6 & 88.7 & $~~$646.5 & 62.4 & $~~$914.5 & 61.1\\
        NP-chunks & $~~$149.9 & 76.0 & $~~$598.4 & 56.6 & $~~$812.3 & 63.0\\
        %Sub-compounds & $~~$171.8 & 90.6 & $~~$712.2 & 64.4 & $~~$934.0 & 61.1\\
        %Acabit & $~~~~$23.8 & 26.7 & $~~$132.1 & 17.6 & $~~$119.4 & 13.4\\
        %Terms & $~~$161.7 & 46.1 & $~~$494.5 & 32.3 & $~~$646.9 & 52.8\\
        Terms & $~~$161.8 & 46.1 & $~~$498.6 & 32.4 & $~~$647.0 & 52.8\\
        \bottomrule
      \end{tabular}
      \caption{Candidate extraction statistics.
               \label{tab:candidate_extraction_statistics}}
    \end{table*}

    This section presents the intrinsic evaluation of the candidate extraction
    methods described in Section~\ref{sec:candidate_extraction}. The aim is to
    compare the methods in terms of quantity of extracted candidates and
    percentage of reference keyphrases that can be found in the best case
    (maximum recall).

    \subsubsection{Method Settings}
    \label{subsubsec:method_settings}
      The parameters of the methods are chosen to fit as much as possible the
      keyphrase properties inferred from the training sets (see
      Section~\ref{sec:definition_of_candidate_keyphrases}).

      According to Property~\ref{prop:informativity}, we try two \textbf{n-gram
      extraction} methods with low $n$ values: the first method extracts
      filtered n-grams where $n = \{1..2\}$ and the second method extracts
      filtered n-grams where $n = \{1..3\}$. The stop words used for the
      filtering are part of the IR Multilingual
      Resources\footnote{\url{http://members.unine.ch/jacques.savoy/clef/index.html}}
      provided by the University of Neuchâtel (UniNE).

      We define two \textbf{pattern matching} methods. The first one learns the
      keyphrase POS tag patterns from the training documents and extracts
      textual units that match one of the patterns. The second method follows
      both Property~\ref{prop:noun_phrases} and previous work about keyphrase
      extraction by looking for the longest sequences of nouns and
      adjectives~\cite{wan2008expandrank,hassan2010conundrums}, supposedly the
      longest noun phrases.

      The \textbf{NP-chunk extraction} is performed using pattern matching. Only
      basic patterns are used:
      \begin{itemize}
        \item{Np+ $|$~(A+~Nc) $|$~Nc+, for English;}
        \item{Np+ $|$~(A?~Nc~A+) $|$~(A~Nc) $|$~Nc+, for French.}
      \end{itemize}

%      The \textbf{sub-compounding} method of
%      \newcite{evans1996nounphraseanalysis} requires complex noun phrases. For
%      comparison purpose, we input the longest NPs extracted by pattern
%      matching (see above) and assume that they are complex NPs.

      \textbf{TermSuite} is used (with its default settings) to build a
      terminology for each training dataset. The terminologies contain 30807
      terms for DUC, 76597 terms for SemEval or 123796 terms for DEFT.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      Table~\ref{tab:candidate_extraction_statistics} shows the number of
      textual units extracted by each candidate extraction method and indicates
      the maximum recall that can be achieved.

      Globally, the extraction  of n-grams gives many candidates and allows a
      high maximum recall, whereas the extraction of more specific textual units
      provides few candidates and allows a lower maximum recall. However, for
      the extraction of the longest noun phrases and the NP-chunks, the drop in
      the maximum recall is not significant compared to the drop in the number
      of candidates. Also, the extraction of textual units matching learned
      patterns seems to stand as a compromise between the extraction of n-grams
      and the other extractions. Assuming that an important drop in the number
      of candidates not followed by a significant decrease of the maximum recall
      is an indicator of quality, the longest NPs and the NP-chunks are the best
      candidates, followed by the candidates extracted from learned patterns.

      TermSuite provides a few candidates and does not allow a good maximum
      recall. However, terminological phrases are coherant with our definition
      of a keyphrase and consistant with both
      properties~\ref{prop:informativity} and~\ref{prop:noun_phrases}. If the
      candidate set extracted by TermSuite has a high quality, the performance
      of the keyphrase extraction may not suffer from the important drop of the
      maximum recall. Figure~\ref{fig:candidate_intersections} shows the
      intersections between the n-gram candidates and the term candidates. We
      observe that n-grams cover almost all the terms, but n-grams that are not
      within the terms might be unrelevant candidates. The experiments of
      Section~\ref{subsec:keyphrase_extraction} can confirm whether or not terms
      are suitable for the keyphrase extraction task.
      \begin{figure*}
        \begin{minipage}{.32\linewidth}
          \centering
          \subfigure[\textbf{DUC}\label{subfig1:candidate_intersections}]{
            \begin{overpic}[height=.9\linewidth]{include/duc_refs_term_suite_1_2_3_grams.eps}
              \put(5,45){$\{1..3\}$-grams}
              \put(66,42){Terms}
              \put(80,72){Refs}
            \end{overpic}
          }
        \end{minipage}
        \hfill
        \begin{minipage}{.32\linewidth}
          \centering
          \subfigure[\textbf{SemEval}\label{subfig2:candidate_intersections}]{
            \begin{overpic}[height=.9\linewidth]{include/semeval_refs_term_suite_1_2_3_grams.eps}
              \put(7,45){$\{1..3\}$-grams}
              \put(68,40){Terms}
              \put(83,64){Refs}
            \end{overpic}
          }
        \end{minipage}
        \hfill
        \begin{minipage}{.32\linewidth}
          \centering
          \subfigure[\textbf{DEFT}\label{subfig3:candidate_intersections}]{
            \begin{overpic}[height=.9\linewidth]{include/deft_refs_term_suite_1_2_3_grams.eps}
              \put(7,46){$\{1..3\}$-grams}
              \put(70,45){Terms}
              \put(88,61){Refs}
            \end{overpic}
          }
        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \vspace{1em}
%          \subfigure[TermSuite candidates compared to longest noun phrases and \textbf{DUC} references.\label{subfig4:candidate_intersections}]{
%            \hspace{1.6em}
%            \begin{overpic}[height=.48\linewidth]{include/duc_refs_term_suite_longest_nps.eps}
%              \put(10,18){Longest}\put(19,6){NPs}
%              \put(43,45){TermSuite}
%              \put(38,74){Refs}
%            \end{overpic}
%            \hspace{1.6em}
%          }
%        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \vspace{.9em}
%          \subfigure[TermSuite candidates compared to longest noun phrases and \textbf{SemEval} references.\label{subfig5:candidate_intersections}]{
%            \hspace{1.8em}
%            \begin{overpic}[height=.48\linewidth]{include/semeval_refs_term_suite_longest_nps.eps}
%              \put(15,18){Longest}\put(24,6){NPs}
%              \put(45,45){TermSuite}
%              \put(48,75){Refs}
%            \end{overpic}
%            \hspace{1.8em}
%          }
%        \end{minipage}
%        \hfill
%        \begin{minipage}{.32\linewidth}
%          \centering
%          \vspace{1.6em}
%          \subfigure[TermSuite candidates compared to longest noun phrases and \textbf{DEFT} references.\label{subfig6:candidate_intersections}]{
%            \hspace{2.4em}
%            \begin{overpic}[height=.42\linewidth]{include/deft_refs_term_suite_longest_nps.eps}
%              \put(12,19){Longest}\put(23,5){NPs}
%              \put(40,45){TermSuite}
%              \put(48,75){Refs}
%            \end{overpic}
%            \hspace{2.4em}
%          }
%        \end{minipage}
        \caption{Intersection of TermSuite candidates with $\{1..3\}$-grams
                 % TODO update
                 \label{fig:candidate_intersections}}
      \end{figure*}

  \subsection{Keyphrase Extraction}
  \label{subsec:keyphrase_extraction}
    This section presents the extrinsic evaluation of the candidate extraction
    methods. The performances of TF-IDF, TopicRank and KEA are compared when
    they extract keyphrases from candidates provided by each method.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      Tables~\ref{tab:tfidf_results},~\ref{tab:topicrank_results}~and~\ref{tab:kea_results}
      show the performances of respectively TF-IDF, TopicRank and KEA when
      they are applied with either one of the studied candidate extraction
      methods. The keyphrase extraction methods themselves behave differently.
      KEA have competitive performances with every candidate extraction methods
      while TF-IDF and TopicRank see their performance improving when the
      quality of the candidate set improves.

      Globally, for every method the best performance is achieved when using
      pattern matching (candidates extracted from learned patterns, longest NPs
      and NP-chunks). This observation confirms the assumption, made in
      Section~\ref{subsec:candidate_extraction}, that a few candidates of a good
      quality , i.e.~candidates that fit keyphrase properties, is better than
      many candidates with various properties that not always fit with keyphrase
      properties. In most cases, the longest NPs provide the best results,
      conforting this usage in previous
      work~\cite{wan2008expandrank,hassan2010conundrums,bougouin2013topicrank}.

      TermSuite candidates allow lower performances than the pattern matching
      methods. However, the results are better than the ones obtained with
      n-grams. Also, we observe competitive results with the pattern matching
      methods when the dataset belongs to a specific domain (DEFT and SemEval).
      Thus, terms are good candidates when the processed data belong to a known
      domain.

      \begin{table*}
        \centering
        \begin{tabular}{@{~}rccccccccc@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..2\}-grams & 14.7 & 19.5 & 16.5 & 10.3 & $~~$7.0 & $~~$8.3 & $~~$8.1 & 15.1 & 10.4\\
          \{1..3\}-grams & 14.3 & 19.0 & 16.1 & $~~$9.0 & $~~$6.0 & $~~$7.2 & $~~$6.7 & 12.5 & $~~$8.6\\
          %\{1..4\}-grams & 13.7 & 18.2 & 15.4 & $~~$8.4 & $~~$5.6 & $~~$6.7 & $~~$6.7 & 12.5 & $~~$8.6\\
          Learned patterns & 19.1 & 25.4 & 21.5 & 10.7 & $~~$7.3 & $~~$8.6 & $~~$7.0 & 13.1 & $~~$9.0\\
          Longest NPs & 24.2 & 31.7 & \textbf{27.0} & 11.7 & $~~$7.9 & $~~$9.3 & $~~$9.5 & 17.6 & 12.1\\
          NP-chunks & 21.1 & 28.1 & 23.8 & 11.9 & $~~$8.0 & \textbf{$~~$9.5} & $~~$9.6 & 17.9 & 12.3\\
          %Sub-compounds & 22.8 & 29.9 & 25.5 & 10.8 & $~~$7.2 & $~~$8.6 & $~~$9.2 & 17.2 & 11.9\\
          %Acabit & 15.3 & 19.6 & 17.0 & $~~$8.6 & $~~$6.1 & $~~$7.1 & $~~$2.4 & $~~$5.6 & $~~$3.3\\
          %Terms & 17.4 & 23.2 & 19.6 & 11.5 & $~~$8.3 & \textbf{$~~$9.5} & 11.3 & 21.1 & \textbf{14.5}\\
          Terms & 17.4 & 23.2 & 19.6 & 11.2 & $~~$8.1 & $~~$9.3 & 11.3 & 21.1 & \textbf{14.5}\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate extraction methods, when extracting 10
                 keyphrases with \textbf{TF-IDF}.
                 \label{tab:tfidf_results}}
      \end{table*}
      \begin{table*}
        \centering
        \begin{tabular}{@{~}rccccccccc@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..2\}-grams & 10.2 & 14.1 & 11.7 & 11.9 & $~~$8.2 & $~~$9.6 & $~~$5.8 & 11.0 & $~~$7.5\\
          \{1..3\}-grams & $~~$7.8 & 10.7 & $~~$8.9 & $~~$9.5 & $~~$6.7 & $~~$7.7 & $~~$6.2 & 11.4 & $~~$8.0\\
          %\{1..4\}-grams & $~~$7.1 & $~~$9.7 & $~~$8.1 & & & & & & \\
          Learned patterns & 14.9 & 19.8 & 16.7 & 12.2 & $~~$8.5 & \textbf{$~~$9.9} & $~~$8.8 & 16.1 & 11.3\\
          Longest NPs & 17.7 & 23.2 & \textbf{19.8} & 11.6 & $~~$7.9 & $~~$9.3 & 11.6 & 21.5 & \textbf{14.9}\\
          NP-chunks & 13.3 & 21.5 & 18.3 & 11.7 & $~~$8.0 & $~~$9.4 & 11.1 & 20.7 & 14.4\\
          %Sub-compounds & 18.3 & 24.0 & \textbf{20.5} & 11.3 & $~~$7.7 & $~~$9.0 & 11.6 & 21.5 & \textbf{14.9}\\
          %Acabit & 11.2 & 14.2 & 12.3 & 10.2 & $~~$7.1 & $~~$8.3 & $~~$3.4 & $~~$7.9 & $~~$4.7\\
          %TermSuite & 10.4 & 13.9 & 11.7 & $~~$8.8 & $~~$6.4 & $~~$7.4 & $~~$9.6 & 18.5 & 12.4\\
          Terms & 10.4 & 13.9 & 11.7 & $~~$8.9 & $~~$6.5 & $~~$7.5 & $~~$9.6 & 18.5 & 12.4\\
          %Terms clusters & $~~$5.7 & $~~$7.9 & $~~$6.5 & $~~$6.4 & $~~$4.7 & $~~$5.4 & $~~$8.7 & 16.0 & 11.2\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate extraction methods, when extracting 10
                 keyphrases with \textbf{TopicRank}.
                 \label{tab:topicrank_results}}
      \end{table*}
      \begin{table*}
        \centering
        \begin{tabular}{@{~}rccccccccc@{~}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..2\}-grams & 12.3 & 17.1 & 14.1 & 19.2 & 13.6 & 15.8 & 13.1 & 24.5 & 16.9\\
          \{1..3\}-grams & 12.0 & 16.6 & 13.7 & 19.4 & 13.7 & 15.9 & 13.4 & 25.3 & 17.3\\
          %\{1..4\}-grams & 11.7 & 16.1 & 13.4 & 19.5 & 13.8 & 16.0 & 13.7 & 25.7 & 17.6\\
          Learned patterns & 12.9 & 17.8 & 14.8 & 19.6 & 13.8 & \textbf{16.1} & 14.7 & 27.6 & 19.0\\
          Longest NPs & 14.5 & 19.9 & \textbf{16.5} & 19.6 & 13.7 & 16.0 & 14.1 & 26.3 & 18.1\\
          NP-chunks & 13.5 & 18.6 & 15.4 & 19.5 & 13.7 & 16.0 & 14.3 & 26.8 & 18.4\\
          %Sub-compounds & 14.6 & 20.0 & \textbf{16.7} & 19.3 & 13.5 & 15.8 & 14.1 & 26.3 & 18.1\\
          %Acabit & 14.6 & 19.1 & 16.3 & 15.6 & 10.8 & 12.6 & $~~$4.7 & 10.5 & $~~$6.4\\
          %Terms & 12.5 & 17.2 & 14.3 & 13.8 & 10.0 & 11.5 & 14.7 & 28.0 & \textbf{19.1}\\
          Terms & 12.5 & 17.2 & 14.3 & 13.9 & 10.1 & 11.6 & 14.7 & 28.0 & \textbf{19.1}\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate extraction methods, when extracting 10
                 keyphrases with \textbf{KEA}.
                 \label{tab:kea_results}}
      \end{table*}
%      \begin{table*}
%        \centering
%        \begin{tabular}{rccccccccc}
%          \toprule
%          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
%          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
%          & P & R & F & P & R & F & P & R & F\\
%          \midrule
%          \{1..2\}-grams & 15.5 & 20.5 & 17.4 & 10.4 & $~~$7.0 & \textbf{$~~$8.3} & $~~$3.0 & $~~$6.2 & $~~$4.0\\
%          \{1..3\}-grams & 13.7 & 18.0 & 15.3 & $~~$3.4 & $~~$2.3 & $~~$2.7 & $~~$1.9 & $~~$4.2 & $~~$2.6\\
%          %\{1..4\}-grams & $~~$7.7 & 10.1 & $~~$8.6 & $~~$1.4 & $~~$1.0 & $~~$1.1 & $~~$1.1 & $~~$2.4 & $~~$1.5\\
%          Learned patterns & 18.7 & 24.3 & 20.8 & $~~$4.6 & $~~$3.1 & $~~$1.2 & $~~$1.9 & $~~$4.2 & $~~$2.6\\
%          Longest NPs & 22.8 & 29.5 & \textbf{25.3} & $~~$3.7 & $~~$2.5 & $~~$3.0 & $~~$4.6 & $~~$9.2 & $~~$6.1\\
%          NP-chunks & 20.6 & 27.3 & 23.1 & $~~$8.4 & $~~$5.7 & $~~$6.7 & $~~$4.9 & $~~$9.7 & $~~$6.4\\
%          Sub-compounds & 21.2 & 27.3 & 23.5 & $~~$3.5 & $~~$2.4 & $~~$2.8 & $~~$4.6 & $~~$9.2 & $~~$6.1\\
%          %Acabit & 15.3 & 19.8 & 17.0 & $~~$7.6 & $~~$5.2 & $~~$6.1 & $~~$2.9 & $~~$6.6 & $~~$4.0\\
%          TermSuite & & & & & & & & & \\
%          \bottomrule
%        \end{tabular}
%        \caption{Comparison of candidate extraction methods, when extracting 10
%                 keyphrases with \textbf{SingleRank}.
%                 \label{tab:singlerank_results}}
%      \end{table*}

\section{Conclusion}
\label{sec:conclusion}
  In this paper, we presented the candidate extraction step of the automatic
  keyphrase extraction task. This step provides the only textual units that can
  be selected as keyphrases for a document. Hence, extracting keyphrase
  candidates is one of the most critical step of the keyphrase extraction task.
  
  First, we inferred keyphrase properties from reference keyphrases provided
  along three standard evaluation datasets. Secondly, we presented various
  candidate extraction methods and discuss the consistancy with the inferred
  properties. Finally, we intrinsically and extrinsically compared the keyphrase
  candidate extraction methods.

  Our observation is that quality prevails over exhaustivity. A candidate set
  containing many candidates matching with reference keyphrases does not permit
  the best performance if it contains a huge amount of unrelevant candidates.
  This observation is true no matter the category of the keyphrase extraction
  method. However, supervised methods seem to be less affected by low quality
  candidate sets.
