\section{Introduction}
\label{sec:section}
  Keyphrases are single or multi-word expressions that represent the main topics
  of a document. Keyphrases are useful in many tasks such as information
  retrieval~\cite{medelyan2008smalltrainingset}, document
  summarization~\cite{litvak2008graphbased} or document
  clustering~\cite{han2007webdocumentclustering}. Although scientific articles
  usually provide them, most of the documents have no associated keyphrases.
  Therefore, the problem of automatically assigning keyphrases to documents is
  an active field of research.

  Automatic keyphrase extraction methods are divided into two categories:
  supervised and unsupervised methods. Supervised methods typically recast
  keyphrase extraction as a binary classification
  task~\cite{witten1999kea,sujian2003maximumentropy,eichler2010keywe}. For
  unsupervised methods, keyphrase extraction is often considered as a ranking
  task and many approaches are
  used~\cite{barker2000nounphrasehead,mihalcea2004textrank}. As distinct as they
  are, both supervised and unsupervised methods rely on a preliminary candidate
  extraction step which identifies single and multi-word expressions that have
  the same syntactic properties than a keyphrase. These expressions are the only
  textual units that can be extracted as keyphrases. Therefore, we believe that
  the extraction of candidate keyphrases plays a direct role in automatic
  keyphrase extraction.
  
  In this paper, we focus on the candidate extraction step and show its impact
  on the performance of automatic keyphrase extraction. Various methods
  are commonly employed to extract keyphrase candidates\footnote{In this work,
  we do not consider methods which use a manually defined controlled
  vocabulary.}. Usually, a set of either single words, n-grams filtered by stop
  words, NP-chunks or sequences of words matching given patterns is
  extracted~\cite{hulth2003keywordextraction}. According to the chosen method,
  the extracted set contains more or less candidates, and the amount of these
  that are actual keyphrases may vary. Hence, a few questions arise. How the
  different sets influence the keyphrase extraction? Do large candidate sets
  introduce noise that affects the performance of some keyphrase extraction
  methods?

  We seek to better understand the impact of candidate extraction methods on
  keyphrase extraction by studying the aforementioned questions. We first
  quantify the differences between the candidate sets obtained by the commonly
  used methods and we propose to use other methods developed for automatic term
  detection~\cite{castellvi2001automatictermdetection,evans1996nounphraseanalysis}
  to show that such methods provide solid keyphrase candidates. Then, we
  evaluate the impact of the candidate extraction methods over three dissimilar
  keyphrase extraction methods. We select KEA~\cite{witten1999kea} to represent
  supervised methods, TF-IDF~\cite{jones1972tfidf} to represent unsupervised
  methods that require a collection of documents and
  TopicRank~\cite{bougouin2013topicrank} to represent unsupervised methods that
  only make use of the analyzed document.

  \todo[inline]{Results show that...}

\section{Definition of Candidate Keyphrases}
\label{sec:study_of_ground_truth_keyphrases}
  Candidate keyphrases are textual units which can be selected as keyphrases
  of a document. Hence, they must have the same syntactic and linguistic
  properties than ground truth keyphrases. This section aims to determine those
  properties by analysing three standard evaluation datasets, for keyphrase
  extraction, and by providing statistics about their reference keyphrases
  (ground truth keyphrases).

  \subsection{Keyphrase Extraction Datasets}
  \label{subsec:keyphrase_extraction_datasets}
    Keyphrase extraction datasets are used to train or evaluate keyphrase
    extraction methods. Hence, they are collections of documents paired with
    reference keyphrases given by authors, readers or both. Unlike the methods
    to automatically extract keyphrases, human annotators do not only provide
    keyphrases contained into the documents. This problem of missing keyphrases
    leads to a bias of the training or evaluation of keyphrase extraction
    methods. In this work, we use three standard datasets which differ in terms
    of document size,  type and language. The problem of missing keyphrases is
    partially bypassed using stemmed forms when comparison between reference and
    candidate keyphrases is needed.

    The \textbf{DUC} dataset \cite{over2001duc} is a collection of 308 English
    news articles covering about 30 topics (e.g. tornadoes, gun control, etc.).
    This collection is the test dataset of the DUC-2001 summarization evaluation
    campaign. This part of DUC-2001 is the only one that contains keyphrases,
    annotated by \newcite{wan2008expandrank}. We split the collection into two
    sets: a training set containing 208 documents and a test set containing 100
    documents.

    The \textbf{SemEval} dataset \cite{kim2010semeval} contains 284 English
    papers collected from the ACM Digital Libraries (conference and workshop
    papers). The papers are divided into three sets: a trial set containing 40
    documents (unused in this work), a training set containing 144 documents and
    a test set containing 100 documents. As for the associated keyphrases, these
    are provided by both authors and readers.

    The \textbf{DEFT} dataset \cite{Paroubek2012deft} is a collection of 244
    French scientific papers belonging to the Humanities and Social Sciences
    domain. DEFT is divided into three sets: a trial set containing 50 documents
    (not used in this work), a training set containing 141 documents and a test
    set containing 93 documents. The only available reference keyphrases are the
    ones given by authors.

    Table \ref{tab:train_dataset_statistics} shows the statistics about the
    three datasets. As these statistics are used to guide our work, we restrain
    them to the training sets. As said before, the datasets differ in terms of
    size, type and language. Moreover, it is worth noticing that the number of
    keyphrases, as well as the ratio of missing ones and their average number of
    tokens differ too. This observation is due to the fact that there is not a
    unique methodology (guideline) to associate keyphrases to a document. To
    better fit the requirements, such guidelines should not only be used by
    human annotators, but also by automatic keyphrase extraction methods.
    \begin{table*}
      \centering
      \begin{tabular}{rrccc}
        \toprule
        & \multirow{2}{*}[-2pt]{\textbf{Statistics}} & \multicolumn{3}{c}{\textbf{Corpora}}\\
        \cmidrule{3-5}
        & & DUC & SemEval & DEFT\\
        \midrule
        \multirow{6}{*}[-2pt]{\begin{sideways}\textbf{Documents}\end{sideways}} & Language & English & English & French\\
        & Type & News & Papers & Papers\\
        & Documents & 208 & 144 & 141\\
        & Tokens/document & 912.0 & 5134.6 & 7276.7\\
        & Keyphrases/document & 8.1 & 15.4 & 5.4\\
        & Missings keyphrases & 3.9\% & 13.5\% & 18.2\%\\
        \addlinespace[1.5\defaultaddspace]
        \multirow{12}{*}[-2pt]{\begin{sideways}\textbf{Keyphrases}\end{sideways}} & Unigrams & 26.2\% & 20.2\% & 66.4\%\\
        & Bigrams & 54.1\% & 53.4\% & 20.7\%\\
        & Trigrams and more & 19.7\% & 26.4\% & 12.9\%\\
        \addlinespace[.75\defaultaddspace]
        & Containing nouns & 90.8\% & 95.9\% & 79.3\%\\
        & Containing proper nouns & 18.7\% & $~~$5.8\% & 16.8\%\\
        & Containing adjectives & 41.6\% & 40.5\% & 28.8\%\\
        & Containing verbs & $~~$0.9\% & $~~$3.4\% & $~~$0.5\%\\
        & Containing adverbs & $~~$1.3\% & $~~$0.6\% & $~~$0.5\%\\
        & Containing prepositions & $~~$0.2\% & $~~$1.2\% & 12.7\%\\
        & Containing determiners & $~~$0.0\% & $~~$0.0\% & $~~$8.1\%\\
        & Containing others & $~~$1.3\% & $~~$2.1\% & $~~$5.8\%\\
        \bottomrule
      \end{tabular}
      \caption{Training dataset statistics. As a matter of consistency regarding
               the training and the evaluation of keyphrase extraction methods,
               the percentage of missing keyphrases is determined based on the
               stemmed form of the reference keyphrases.
               \label{tab:train_dataset_statistics}}
    \end{table*}

  \subsection{Reference Keyphrases Analysis}
  \label{subsec:keyphrase_analysis}
    Despite the fact that the data are not homogeneous, this section aims to
    determine the syntactic properties of most keyphrases, for English
    (intersecting information from DUC and SemEval training sets) and for French
    (using DEFT training set).

    \todo[inline]{\begin{center}DONNER DES
                  EXEMPLES\end{center}}
    \todo[inline]{Donner les séquences de POS les plus fréquentes dans le gold
                  standard.}

\section{Candidate Extraction}
\label{sec:candidate_extraction}
  \todo[inline]{Objectif + pré-requis.}

  \subsection{N-Gram Extraction}
  \label{subsec:n_gram_extraction}
  \subsection{NP-Chunk Extraction}
  \label{subsec:np_chunk_extraction}
  \subsection{Pattern Matching}
  \label{subsec:pattern_matching}
  \subsection{Term Extraction}
  \label{subsec:term_extraction}

\section{Keyphrase Extraction}
\label{sec:keyphrase_extraction}
  \todo[inline]{Fonctionnement général.}
  \begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{include/processing_steps.eps}
    \caption{Processing steps of automatic keyphrase extraction methods.
             \label{fig:processing_steps}}
  \end{figure}

  \subsection{TF-IDF}
  \label{subsec:tfidf}
  \subsection{TopicRank}
  \label{subsec:topicrank}
  \subsection{KEA}
  \label{subsec:kea}

\section{Evaluation}
\label{sec:evaluation}
  \todo[inline]{Expliquer les deux évaluations: intrinsèque et extrinsèque.}

  \subsection{Experimental Setting}
  \label{subsec:experimental_setting}
    \begin{table*}
      \centering
      \begin{tabular}{rccc}
        \toprule
        \multirow{2}{*}[-2pt]{\textbf{Statistics}} & \multicolumn{3}{c}{\textbf{Corpora}}\\
        \cmidrule{2-4}
        & DUC & SemEval & DEFT\\
        \midrule
        Language & English & English & French\\
        Type & News & Papers & Papers\\
        Documents & 100 & 100 & 93\\
        Tokens/document & 877.3 & 5177.7 & 6839.4\\
        Keyphrases/document & 7.94 & 14.7 & 5.2\\
        Tokens/keyphrase & 2.1 & 2.1 & 1.6\\
        Missings keyphrases & 2.8\% & 22.1\% & 21.1\% \\
        \bottomrule
      \end{tabular}
      \caption{Test dataset statistics. As a matter of consistency regarding
               the training and the evaluation of keyphrase extraction methods,
               the percentage of missing keyphrases is determined based on the
               stemmed form of the reference keyphrases.
               \label{tab:test_dataset_statistics}}
    \end{table*}

  \subsection{Candidate Extraction}
  \label{subsec:candidate_extraction}
    \todo[inline]{Donner le rappel max et comparer avec la taille des différents
                  ensemble.}

    \begin{table*}[h]
      \centering
      \begin{tabular}{rcccccc}
        \toprule
        \multirow{2}{*}[-2pt]{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{DUC}} & \multicolumn{2}{c}{\textbf{SemEval}} & \multicolumn{2}{c}{\textbf{DEFT}}\\
        \cmidrule(r){2-3}\cmidrule(lr){4-5}\cmidrule(l){6-7}
        & Candidates & Rmax & Candidates & Rmax & Candidates & Rmax\\
        \midrule
        \{1..2\}-grams & 310539 & 78.8 & 949225 & 61.4 & 1374905 & 67.3\\
        \{1..3\}-grams & 515903 & 94.0 & 1816327 & 73.1 & 2593221 & 74.1\\
        \{1..4\}-grams & 714917 & 95.8 & 2705335 & 75.3 & 3847599 & 78.2\\
        \{1..5\}-grams &905094 & 96.3 & 3574701 & 75.9 & 5085447 & 78.5\\
        NP chunks\\
        Longest NPs & 49845 & 88.7 & 155189 & 62.4 & 224083 & 61.1\\
        Best patterns\\
        Subcompounds\\
        TermSuite-spec & & 23.8 & & 16.3 &  & 13.9\\
        TermSuite-full & 77357 & 46.1 & 196477 & 32.4 & 310150 & 53.4\\
        \bottomrule
      \end{tabular}
      \caption{Candidate extraction statistics. Rmax stands for maximum recall,
               i.e. it is the percentage of candidates that match with reference
               keyphrases. \label{tab:candidate_extraction_statistics}}
    \end{table*}

    \todo[inline]{Quels sont les termes candidats communs aux ensembles, les
                  propriétés ?}

  \subsection{Keyphrase Extraction}
  \label{subsec:keyphrase_extraction}
    \todo[inline]{Quelles sont les performances de chaque méthode avec chaque
                  ensemble de termes candidats ?}

    \begin{table*}[h]
      \centering
      \begin{tabular}{rccccccccc}
        \toprule
        \multirow{2}{*}[-2pt]{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
        \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
        & P & R & F & P & R & F & P & R & F\\
        \midrule
        \{1..2\}-grams & ${~~}$9.6 & 13.2 & 11.0 & 16.2 & 11.4 & 13.3 & 11.6 & 21.8 & 15.0\\
        \{1..3\}-grams & ${~~}$9.0 & 12.4 & 10.3 & 16.4 & 11.5 & 13.4 & 11.6 & 22.0 & 15.0\\
        \{1..4\}-grams & ${~~}$9.0 & 12.4 & 10.3 & 16.4 & 11.5 & 13.4 & 11.6 & 21.9 & 15.0\\
        \{1..5\}-grams & ${~~}$8.6 & 11.9 & $~~$9.8 & 16.4 & 11.5 & 13.4 & 11.6 & 22.0 & 15.0\\
        NP chunks & & & & & & & & &\\
        Longest NPs & 13.3 & 18.2 & 15.2 & 18.3 & 12.9 & 15.0 & 12.8 & 23.6 & 16.4\\
        Best patterns & & & & & & & & &\\
        Subcompounds & & & & & & & & &\\
        TermSuite-spec & $~~$9.5 & 13.2 & 10.9 & ${~~}$9.4 & ${~~}$6.8 & ${~~}$7.8 & ${~~}$5.4 & 10.7 & ${~~}$7.1\\
        TermSuite-full & 11.5 & 16.0 & 13.2 & 12.9 & ${~~}$9.3 & 10.7 & 12.5 & 23.5 & 16.1\\
        \bottomrule
      \end{tabular}
      \caption{Comparison of candidate extraction methods, when extracting 10
               keyphrases with the \textbf{TF-IDF} method. Results are expressed
               as a percentage of precision (P), recall (R) and f-score (F).
               \label{tab:keyphrase_extraction_results}}
    \end{table*}

    \begin{table*}[h]
      \centering
      \begin{tabular}{rccccccccc}
        \toprule
        \multirow{2}{*}[-2pt]{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
        \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
        & P & R & F & P & R & F & P & R & F\\
        \midrule
        \{1..2\}-grams & & & & & & & & &\\
        \{1..3\}-grams & & & & & & & & &\\
        \{1..4\}-grams & & & & & & & & &\\
        \{1..5\}-grams & & & & & & & & &\\
        NP chunks & & & & & & & & &\\
        Longest NPs & & & & & & & & &\\
        Best patterns & & & & & & & & &\\
        Subcompounds & & & & & & & & &\\
        TermSuite & & & & & & & & &\\
        \bottomrule
      \end{tabular}
      \caption{Comparison of candidate extraction methods, when extracting 10
               keyphrases with \textbf{TopicRank}. Results are expressed as a
               percentage of precision (P), recall (R) and f-score (F).
               \label{tab:keyphrase_extraction_results}}
    \end{table*}

    \begin{table*}[h]
      \centering
      \begin{tabular}{rccccccccc}
        \toprule
        \multirow{2}{*}[-2pt]{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
        \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
        & P & R & F & P & R & F & P & R & F\\
        \midrule
        \{1..2\}-grams & & & & & & & & &\\
        \{1..3\}-grams & & & & & & & & &\\
        \{1..4\}-grams & & & & & & & & &\\
        \{1..5\}-grams & & & & & & & & &\\
        NP chunks & & & & & & & & &\\
        Longest NPs & & & & & & & & &\\
        Best patterns & & & & & & & & &\\
        Subcompounds & & & & & & & & &\\
        TermSuite & & & & & & & & &\\
        \bottomrule
      \end{tabular}
      \caption{Comparison of candidate extraction methods, when extracting 10
               keyphrases with \textbf{KEA}. Results are expressed as a
               percentage of precision (P), recall (R) and f-score (F).
               \label{tab:keyphrase_extraction_results}}
    \end{table*}

