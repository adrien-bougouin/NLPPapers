\section{Introduction}
\label{sec:section}

  Since the last decade, the amount of information available on the web is
  constantly increasing. While the number of documents continues to grow, the
  need for efficient information retrieval methods becomes increasingly
  important. One way to improve retrieval effectiveness is to use
  keyphrases~\cite{jones1999phrasier}. Keyphrases are single or multi-word
  expressions that represent the main content of a document. As they describe
  the key topics in documents, keyphrases are also useful for tasks such as
  summarization~\cite{avanzo2005keyphrase} or document
  indexing~\cite{medelyan2008smalltrainingset}. There is, however, only a small
  number of documents that have keyphrases associated with them. Keyphrase
  extraction has then attracted a lot of attention recently and many different
  approaches were proposed~\cite{kim2010semeval}.

  Generally speaking, keyphrase extraction methods can be categorized into two
  main categories: supervised and unsupervised approaches. Supervised approaches
  treat keyphrase extraction as a binary classification task, where each phrase
  is labeled either as ``keyphrase'' or ``non-keyphrase'',
  e.g.~\cite{witten1999kea}. Conversely, unsupervised approaches usually rank
  phrases by importance and select the top-ranked ones as keyphrases,
  e.g.~\cite{mihalcea2004textrank}. Although they tackle the keyphrase
  extraction problem differently, both supervised and unsupervised methods rely
  on a candidate selection step. Candidate selection consists in identifying the
  textual units of a document that have properties similar to those of
  human-assigned keyphrases. Selecting appropriate keyphrase candidates is
  particularly important since it determines the upper bound performance of the
  keyphrase extraction methods.

  In previous work, candidate selection is performed either by selecting
  n-grams, noun phrase chunks (NP-chunks) or word sequences matching given
  Part-Of-Speech (POS) patterns~\cite{hulth2003keywordextraction}. In this
  study, we first analyze the properties of human-assigned keyphrases and
  discuss how candidates selected by different candidate selection methods
  satisfy these properties. We then propose a new approach that selects refined
  noun phrases by filtering out irrelevant adjective modifiers from sequences of
  nouns, proper nouns and adjectives. We demonstrate the effectiveness of our
  approach by looking at the completeness of the sets of selected candidates and
  by comparing the performance of state-of-the-art supervised and unsupervised
  keyphrase extraction methods on three standard datasets of different languages
  and nature.

  The rest of this paper is organized as follows.
  Section~\ref{sec:definition_of_candidate_keyphrases} introduces an analysis of
  the properties of human-assigned keyphrases.
  Section~\ref{sec:candidate_extraction} presents the commonly used candidate
  selection methods and describes our new approach. Experiments are discussed in
  Section~\ref{sec:evaluation} and Section~\ref{sec:conclusion} concludes this
  paper.

\section{What is a Keyphrase?}
\label{sec:definition_of_candidate_keyphrases}
  In this section, we determine two keyphrase properties from the analysis of
  human-assigned keyphrases of three standard datasets.

  \subsection{Datasets}
  \label{subsec:keyphrase_extraction_datasets}
    \paragraph{}
    The \textbf{DUC} dataset \cite{over2001duc} is a collection of 308 English
    news articles covering about 30 topics (e.g.~tornadoes, gun control, etc.).
    This collection is the test dataset of the DUC-2001 summarization evaluation
    campaign and contains reference keyphrases annotated by
    \newcite{wan2008expandrank}. We split the collection into two sets: a
    training set containing 208 documents and a test set containing 100
    documents.

    \paragraph{}
    The \textbf{SemEval} dataset \cite{kim2010semeval} contains 244 English
    scientific papers collected from the ACM Digital Libraries (conference and
    workshop papers). The papers are divided into two sets: a training set
    containing 144 documents and a test set containing 100 documents. The
    associated keyphrases are provided by both authors and readers.

    \paragraph{}
    The \textbf{DEFT} dataset \cite{paroubek2012deft} is a collection of 234
    French scientific papers belonging to the \textit{Humanities and Social
    Sciences} domain. DEFT is divided into two sets: a training set containing
    141 documents and a test set containing 93 documents. Keyphrases provided
    with the documents of DEFT are given by authors.

  \subsection{Analysis of Reference Keyphrases}
  \label{subsec:keyphrase_analysis}
    Table~\ref{tab:train_dataset_statistics} shows statistics about the datasets
    and the keyphrases associated to their documents. First, keyphrases are 
    presented regarding their number of words. Second, the multi-word keyphrases
    are presented regarding the Part-of-Speech of their words\footnote{We
    observed that keyphrases containing one word are mostly nouns or proper
    nouns. Hence, we only show the POS tag statistics of the multi-word
    keyphrases.}. To obtain these Part-of-Speech, we automatically POS tagged
    the keyphrases of the English datasets with the Stanford POS
    tagger~\cite{toutanova2003stanfordpostagger} and the keyphrases of the
    French dataset with MElt~\cite{denis2009melt}. To avoid tagging errors, POS
    tagged keyphrases were manually corrected.  From the observation of the
    statistics, we propose two properties:
    \begin{table}
      \centering
      \resizebox{\linewidth}{!}{
        \begin{tabular}{@{}r@{~}|@{~}c@{~}c@{~}c@{}}
          \toprule
          \textbf{Statistic} & \textbf{DUC} & \textbf{SemEval} & \textbf{DEFT}\\
          \hline
          \multicolumn{1}{@{}l@{~}|@{~}}{\textbf{Document properties}}\\
          Language & English & English & French\\
          Number & 208 & 144 & 141\\
          Tokens/document & 912.0 & 5134.6 & 7276.7\\
          Keyphrases/document & 8.1 & 15.4 & 5.4\\
          Missing keyphrases (\%) & 3.9 & 13.5 & 18.2\\
          \hline
          \multicolumn{1}{@{}l@{~}|@{~}}{\textbf{Keyphrase length}}\\
          Unigrams (\%) & 17.1 & 20.2 & 60.2\\
          Bigrams (\%) & 60.8 & 53.4 & 24.5\\
          Trigrams (\%) & 17.8 & 21.3 & $~~$8.8\\
          \hline
          \multicolumn{1}{@{}l@{~}|@{~}}{\textbf{Multi-word keyphrases}}\\
          \multicolumn{1}{@{}l@{~}|@{~}}{\textbf{with}\hfill{}Noun(s) (\%)} & 94.5 & 98.7 & 93.1\\
          Proper noun(s) (\%) & 17.1 & $~~$4.3 & $~~$6.9\\
          Attributive adjective(s) (\%) & 24.2 & 29.1 & $~~$8.6\\
          Relational adjective(s) (\%) & 28.9 & 24.1 & 57.6\\
          Verb(s) (\%) & $~~$1.0 & $~~$4.0 & $~~$1.0\\
          Adverb(s) (\%) & $~~$1.6 & $~~$0.7 & $~~$1.3\\
          Preposition(s) (\%) & $~~$0.3 & $~~$1.5 & 31.2\\
          Determiner(s) (\%) & $~~$0.0 & $~~$0.0 & 20.4\\
          \bottomrule
        \end{tabular}
      }
      \caption{Statistics of the training datasets. Missing keyphrases are
               keyphrases that do not occur in the documents.
               \label{tab:train_dataset_statistics}}
    \end{table}

    First, we observe that most keyphrases are unigrams or bigrams
    ($\simeq$ 80\%), which confirms previous work observation that small-sized
    keyphrases are the most frequent.
    
    \begin{property}\label{prop:informativity}
      Keyphrases are small-sized textual units; Keyphrases usually contain one
      up to three words (e.g.~``storms'', ``hurricane expert'' and ``annual
      hurricane forecast'').
    \end{property}

    \TODO{AdjR stats have changed}
    Second, we observe that almost every keyphrase contains a noun and half of
    the keyphrases are modified by an adjective. Among the adjectives, it is
    important to note the usage of relational adjectives
    (e.g.~``presidential''). \TODO{Define relational adjective} Although they are less used than attributive
    adjectives, their similar properties to
    nouns~\cite{bally1944linguistiquegeneraleetlinguistiquefrancaise} and the
    fact they have classificatory or taxonomic
    meaning~\cite{mcnally2004relationaladjectives} make them more likely to be
    relevant keyphrase modifiers than attributive adjectives, such as ``huge''
    which is very unlikely to be a relevant keyphrase modifier.

    \begin{property}\label{prop:noun_phrases}
      Keyphrases are mostly nouns (e.g.~``storms'') that can be modified by an
      adjective (e.g.~``annual hurricane forecast'').
    \end{property}

    To give an insight of the keyphrase POS tag patterns,
    Table~\ref{tab:best_patterns} shows the five most frequent patterns for
    English and French.
    \begin{table*}
      \centering
      \begin{tabular}{@{}r@{~}|@{~}l@{~}l@{~}l@{~}l@{~}l@{}}
        \toprule
        \multicolumn{1}{r}{} & \multicolumn{4}{@{}l}{\textbf{Pattern}} & \textbf{Example}\\
        \midrule
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{English}\end{sideways}}
        & \verb:Nc: & \verb:Nc: & & & \textit{``hurricane expert''}\\ % AP880409-0015
        & \verb:Nc: & & & & \textit{``storms''}\\ % AP880409-0015
        & \verb:rA: & \verb:Nc: & & & \textit{``Chinese earthquake''}\\ % AP890228-0019
        & \verb:aA: & \verb:Nc: & & & \textit{``turbulent summer''}\\ % AP880409-0015
        & \verb:aA: & \verb:Nc: & \verb:Nc: & & \textit{``annual hurricane forecast''}\\ % AP880409-0015
        \hline%\addlinespace[1.5\defaultaddspace]
        \multirow{5}{*}[-2pt]{\begin{sideways}\textbf{French}\end{sideways}}
        & \verb:Nc: & & & & \textit{``patrimoine'' (``cultural heritage'')}\\ % as_2002_007048ar
        & \verb:Nc: & \verb:rA: & & & \textit{``tradition orale'' (``oral tradition'')}\\ % as_2002_007048ar
        & \verb:Np: & & & & \textit{``Indon√©sie'' (``Indonesia'')}\\ % as_2001_000235ar
        & \verb:Nc: & \verb:Sp: & \verb:D: & \verb:Nc: & \textit{``conservation de la nature'' (``nature conservation'')}\\ % as_2005_011742ar
        & \verb:Nc: & \verb:Sp: & \verb:Nc: & & \textit{``traduction en anglais'' (``English translation'')}\\ % meta_2003_006958ar
        \bottomrule
      \end{tabular}
      \caption{Frequent POS tag patterns. POS tags belong to the Multex format,
               except \texttt{rA} and \texttt{aA} which stands for,
               respectively, \textit{relational adjective} and
               \textit{attributive adjective}.
               \label{tab:best_patterns}}
    \end{table*}

\section{Candidate Selection}
\label{sec:candidate_extraction}
  In this section, we present the textual units that are commonly used as
  keyphrase candidates and discuss their consistency regarding the properties
  inferred in Section~\ref{sec:definition_of_candidate_keyphrases}. We also
  present a new method that selects refined noun phrases as keyphrase
  candidates.

  \paragraph{N-grams} are ordered sequences of $n$ words, where $n$ is usually
  set to 1 up to 3~\cite{witten1999kea}. Extracting n-grams has the benefit to
  provide almost every candidates that actually match reference keyphrases
  (maximum recall), but the counterpart is that it also provides a huge amount
  of irrelevant candidates. Therefore, \newcite{witten1999kea} propose to
  select only n-grams that do not contain a stop word (conjunction, preposition,
  determiner or common word) at their beginning or end. Filtered n-gram
  candidates are grammatically uncontrolled and do not fit
  properties~\ref{prop:informativity} and~\ref{prop:noun_phrases}.

  \paragraph{Textual units matching given POS tag patterns} are textual units of
  specific syntactic forms. Extracting such textual units ensures grammaticality
  and precisely defines the nature of the candidates. In previous work,
  \newcite{hulth2003keywordextraction} experiments with the most frequent POS
  tag patterns of her training data\footnote{Frequent patterns are the ones that
  appear at least ten times in the training data.}, whereas other researchers
  select the longest sequences of nouns, proper nouns and adjectives, namely
  the longest NPs~\cite{hassan2010conundrums}. Candidates selected using both
  approaches fit both properties~\ref{prop:informativity}
  and~\ref{prop:noun_phrases}. However, the first approach requires training
  data and is, therefore, not suitable for every situation.

  \paragraph{NP-chunks} are non-recursive noun phrases.
  \newcite{hulth2003keywordextraction} uses them in her work and argues that
  they are less arbitrary and more linguistically justified than other
  candidates such as n-grams. Also, as NP-chunks are non-recursive (hence
  minimal) noun phrases, they are consistent with both
  properties~\ref{prop:informativity} and~\ref{prop:noun_phrases}.

  \paragraph{}
  As a contribution to the candidate selection step, we propose to extract
  \textbf{refined noun phrases} (refined NPs) by adding a decision process
  during the selection of noun, proper noun and adjective sequences. Indeed,
  we assume that adjectives are sometimes implicit or add extra information
  (e.g.~``huge wildfires''). Hence, they must be kept only under specific
  conditions. First, we assume that a frequent modification of a noun phrase by
  the same adjective (at least twice) is a clue of its usefulness. Second, we
  consider relational adjectives as a specific class of adjectives and assume
  that they are always useful. This assumption is corroborated by the usefulness
  of relational adjectives for other tasks such as topic detection or term
  extraction~\cite{daille2001relationaladjectives}.

\section{Keyphrase Extraction}
\label{sec:keyphrase_extraction}
  Once candidates are selected, the second step of the keyphrase extraction task
  is to classify them or rank them. In this section, we detail the three
  keyphrase extraction methods that we use in our study. Two are unsupervised
  (ranking methods) and one is a supervised (classification method).
  \TODO{Add SingleRank???}

  \paragraph{TF-IDF~\textnormal{\cite{jones1972tfidf}}} is a weighting scheme
  that represents the significance of a word in a given document. Significant
  words must be both frequent in the document and specific to it. The
  specificity of a word is determined based on a collection of documents: the
  lower is the amount of documents containing a given word, the higher is its
  specificity. Keyphrase candidates are scored according to the sum of the
  TF-IDF weights of their words and the $k$ best candidates are extracted as
  keyphrases.

  \paragraph{TopicRank~\textnormal{\cite{bougouin2013topicrank}}} aims to
  extract keyphrases that best represent the main topics of a document.
  Keyphrase candidates are clustered into topics using a stem overlap
  similarity, each topic is scored using the TextRank random walk
  algorithm~\cite{mihalcea2004textrank} and one representative keyphrase is
  extracted from each of the $k$ best ranked topics.

  \paragraph{KEA~\textnormal{\cite{witten1999kea}}} is a supervised method that
  uses a Naive Bayes classifier to extract keyphrases. The classifier combines
  two feature probabilities to predict whether a candidate is a ``keyphrase'' or
  a ``non-keyphrase''. The two features are the TF-IDF weight\footnote{The
    TF-IDF weight computed for KEA is based on candidate frequency, not word
  frequency.} of the candidate and the position of its first appearance in the
  document.

\section{Experiments}
\label{sec:evaluation}
  To validate the effectiveness of our approach, we perform two series of
  experiments. First, we compare the quality of the selected candidates with the
  set of reference keyphrases. Second, we compare their impact on the keyphrase
  extraction task, applying them to TF-IDF, TopicRank and KEA.

  \subsection{Evaluation Measures}
  \label{subsec:keyphrase_extraction_evaluation_measures}
    To quantify the capacity of the keyphrase candidate selection methods to
    provide suitable candidates and avoid irrelevant ones, we compute the
    number of selected candidates (Cand./Doc.) and confront it with the
    maximum recall (R$_{max}$) that can be achieved. To do so, we compute a quality
    ratio (QR):
    \begin{align}
      \text{QR} &= \frac{\text{R$_{max}$}}{\text{Cand./Doc.}} \times 100
    \end{align}
    The higher is the QR value of a candidate set, the better is its quality.

    To evaluate the performance of the keyphrase extraction methods, we use
    the common measures of precision (P), recall (R) and f-score (F), when a
    maximum of 10 keyphrases are extracted.

  \subsection{Preprocessing}
  \label{subsec:preprocessing}
    For each dataset, we apply the following preprocessing steps: sentence
    segmentation, word tokenization and Part-of-Speech tagging. For sentence
    segmentation, we use the PunktSentenceTokenizer provided by the Python
    Natural Language ToolKit~\cite[NLTK]{bird2009nltk}. For word tokenization,
    we use the NLTK TreebankWordTokenizer for English and the Bonsai word
    tokenizer\footnote{The Bonsai word tokenizer is a tool provided with the
    Bonsai PCFG-LA parser:
    \url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}.} for
    French. As for Part-of-Speech tagging, we use the Stanford
    POS tagger~\cite{toutanova2003stanfordpostagger} for English and
    MElt~\cite{denis2009melt} for French.

  \subsection{Candidate Selection}
  \label{subsec:candidate_extraction}

    This section presents an intrinsic evaluation of the candidate selection
    methods described in Section~\ref{sec:candidate_extraction}. The aim is to
    compare the methods in terms of quantity of selected candidates and
    percentage of reference keyphrases that can be found in the best case
    (maximum recall).

    \subsubsection{Method Settings}
    \label{subsubsec:method_settings}
      For each candidate selection method presented in
      section~\ref{sec:candidate_extraction}, we set the parameters in order to
      best fit as much as possible both properties~\ref{prop:informativity}
      and~\ref{prop:noun_phrases}.

      \paragraph{}
      According to Property~\ref{prop:informativity}, we test a \textbf{filtered
      n-gram selection} method that provides small-sized n-grams:
      $n = \{1..3\}$. The stop words used for the filtering are part of the IR
      Multilingual
      Resources\footnote{\url{http://members.unine.ch/jacques.savoy/clef/index.html}}
      provided by the University of Neuch√¢tel (UniNE).

      \paragraph{}
      Following both Property~\ref{prop:noun_phrases} and previous
      work~\cite{hassan2010conundrums}, we use \textbf{pattern matching} to
      select the longest noun phrases (longest NPs), i.e.~the longest sequences
      of nouns, proper nouns and adjectives.

      \paragraph{}
      The \textbf{NP-chunk selection} is also performed using pattern matching.
      Only basic patterns are used:
      \begin{itemize}
        \item{\verb:Np+ | (A+ Nc) | Nc+:, for English datasets;}
        \item{\verb:Np+ | (A? Nc A+) | (A Nc) | Nc+:, for French datasets.}
      \end{itemize}

      \paragraph{}
      The \textbf{refined NPs} are also selected using pattern matching. The
      patterns we use are related to the position of relational adjectives in
      the target language:
      \begin{itemize}
        \item{\verb:A? (Nc | Np)+:, for English datasets;}
        \item{\verb:(Nc | Np)+ A?:, for French datasets.}
      \end{itemize}
      To detect relational adjectives, we use two lists of known suffixes of
      relational adjectives in English (``al'', ``ant'', ``ary'', ``ic'',
      ``ous'' and ``ive'') and French (``ain'', ``aire'', ``al'', ``el'',
      ``eux'', ``ien'', ``ier'', ``ique'', ``ois'') combined with two lexical
      databases: WordNet's pertainym relation~\cite{miller1995wordnet} for
      English and WoNeF's ``related-to'' relation~\cite{pradet2013wonef} for
      French.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      Table~\ref{tab:candidate_extraction_statistics} shows the results of the
      candidate selection methods. The selection of n-grams provides a huge
      amount of candidates and allows a near perfect maximum
      recall\footnote{According to the amount of missing keyphrases of the test
      sets, the maximum recall that can be achieved is 97.2\% for DUC, 87.9\%
      for SemEval and 88.9\% for DEFT.}, whereas the other candidate selection
      methods provide less candidates and allow a lower maximum recall. However,
      for the selection of longest NPs, NP-chunks and refined NPs, the maximum
      recall does not significantly decrease compared to the number of selected
      candidates. According to the quality ratio, the method that selects
      better candidates is the one selecting refined NPs, followed by the ones
      selecting longest NPs and NP-chunks.
      \begin{table*}
        \centering
        \begin{tabular}{@{}r@{~~}c@{~}c@{~~}c@{~}c@{~}c@{~~}c@{~}c@{~}c@{~~}c@{}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(r){5-7}\cmidrule{8-10}
          & Cand./Doc. & R$_{max}$ & QR & Cand./Doc. & R$_{max}$ & QR & Cand./Doc. & R$_{max}$ & QR\\
          \midrule
          \{1..3\}-grams & $~~~$596.2 & 90.8 & 15.2 & 2580.5 & 72.2 & $~~$2.8 & 4070.2 & 74.1 & $~~~$1.8\\
          Longest NPs & $~~~$155.6 & 88.7 & \textbf{57.0} & $~~~$646.5 & 62.4 & $~~$9.7 & $~~~$914.5 & 61.1 & $~~$6.7\\
          NP-chunks & $~~~$149.9 & 76.0 & 50.7 & $~~~$598.4 & 56.6 & $~~$9.5 & $~~~$812.3 & 63.0 & $~~$7.8\\
          Refined NPs & $~~~$143.1 & 73.9 & 51.6 & $~~~$563.4 & 58.1 & \textbf{10.3} & $~~~$670.0 & 59.2 & \textbf{$~~$8.8}\\
          \bottomrule
        \end{tabular}
        \caption{Candidate selection statistics.
                 \label{tab:candidate_extraction_statistics}}
      \end{table*}

      \TODO{Explain why the refined NPs are so different than longest NPs and
            NP-chunks}

      \TODO{Comparer les proportions d'adjectifs extraits au total et faire le
            lien avec le tableau de la section 2}

  \subsection{Keyphrase Extraction}
  \label{subsec:keyphrase_extraction}
    This section presents an extrinsic evaluation of the candidate selection
    methods. The aim is to observe the impact of the candidate selection
    methods on the keyphrase extraction task.

    \subsubsection{Result Analysis}
    \label{subsubsec:candidate_extraction_result_analysis}
      \TODO{Add example}
      \TODO{Add t-test}
      Tables~\ref{tab:tfidf_results},~\ref{tab:topicrank_results}~and~\ref{tab:kea_results}
      show the performance of respectively TF-IDF, TopicRank and KEA when
      they extract keyphrases from keyphrase candidates provided by each
      candidate selection method. The results show a low performance of the
      three methods. However, our results are in the range of results obtained
      in previous comparative
      work~\cite{hassan2010conundrums,kim2010semeval,paroubek2012deft}\footnote{In
      the case of the SemEval and DEFT evaluation
      campaigns~\cite{kim2010semeval,paroubek2012deft}, many methods may have
      better results than our but what we present here are pure methods,
      i.e.~no parameter tuning has been done to obtain higher results on each
      dataset. \TODO{Our TF-IDF is the same as Hasan, not same as Kim}}.
      \begin{table*}
        \centering
        \begin{tabular}{@{}rccccccccc@{}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..3\}-grams & 14.3 & 19.0 & 16.1 & $~~$9.0 & $~~$6.0 & $~~$7.2 & $~~$6.7 & 12.5 & $~~$8.6\\
          Longest NPs & \textbf{24.2} & \textbf{31.7} & \textbf{27.0} & 11.7 & $~~$7.9 & $~~$9.3 & $~~$9.5 & 17.6 & 12.1\\
          NP-chunks & 21.1 & 28.1 & 23.8 & 11.9 & $~~$8.0 & $~~$9.5 & $~~$9.6 & 17.9 & 12.3\\
          Refined NPs & 22.6 & 30.0 & 25.4 & \textbf{12.3} & \textbf{$~~$8.3} &
     \textbf{$~~$9.8} & \textbf{10.1} & \textbf{18.6} & \textbf{12.9}\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate selection methods, when 10 keyphrases
                 are extracted by \textbf{TF-IDF}.
                 \label{tab:tfidf_results}}
      \end{table*}
      \begin{table*}
        \centering
        \begin{tabular}{@{}rccccccccc@{}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..3\}-grams & $~~$7.8 & 10.7 & $~~$8.9 & $~~$9.5 & $~~$6.7 & $~~$7.7 & $~~$6.2 & 11.4 & $~~$8.0\\
          Longest NPs & \textbf{17.7} & \textbf{23.2} & \textbf{19.8} & 11.6 & $~~$7.9 & $~~$9.3 & \textbf{11.6} & \textbf{21.5} & \textbf{14.9}\\
          NP-chunks & 13.3 & 21.5 & 18.3 & 11.7 & $~~$8.0 & $~~$9.4 & 11.1 & 20.7 & 14.4\\
          Refined NPs & 17.2 & 22.9 & 19.4 & \textbf{11.9} & \textbf{$~~$8.2} & \textbf{$~~$9.6} & 10.6 & 19.9 & 13.7\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate selection methods, when 10 keyphrases
                 are extracted by \textbf{TopicRank}.
                 \label{tab:topicrank_results}}
      \end{table*}
      \begin{table*}
        \centering
        \begin{tabular}{@{}rccccccccc@{}}
          \toprule
          \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
          \cmidrule(r){2-4}\cmidrule(lr){5-7}\cmidrule(l){8-10}
          & P & R & F & P & R & F & P & R & F\\
          \midrule
          \{1..3\}-grams & 12.0 & 16.6 & 13.7 & 19.4 & 13.7 & 15.9 & 13.4 & 25.3 & 17.3\\
          Longest NPs & 14.5 & 19.9 & 16.5 & 19.6 & 13.7 & 16.0 & 14.1 & 26.3 & 18.1\\
          NP-chunks & 13.5 & 18.6 & 15.4 & 19.5 & 13.7 & 16.0 & \textbf{14.3} & \textbf{26.8} & \textbf{18.4}\\
          Refined NPs & \textbf{14.7} & \textbf{20.3} & \textbf{16.8} &
         \textbf{20.8} & \textbf{14.6} & \textbf{17.0} & 14.1 & 26.5 & 18.2\\
          \bottomrule
        \end{tabular}
        \caption{Comparison of candidate selection methods, when 10 keyphrases
                 are extracted by \textbf{KEA}.
                 \label{tab:kea_results}}
      \end{table*}
      
      Globally, the selection of refined NPs, followed by the selection of
      longest NPs and NP-chunks, is the method that induces the best performance
      for each keyphrase extraction method. This shows that our candidate
      selection is able to remove more irrelevant candidates than commonly used
      methods. Also, the fact that the selection of n-grams induces the lowest
      performance confirms that small candidate sets of high quality are better
      than exhaustive (hence noisy) candidate sets allowing a better recall.
      
      Another emerging conclusion from the results is that, due to its
      learning phase, the supervised method KEA is more stable than the
      unsupervised methods TF-IDF and TopicRank. When developing unsupervised
      methods for keyphrase extraction, being able to avoid noise during the
      candidate selection is very important.

\section{Conclusion}
\label{sec:conclusion}
  In this paper, we stated that the candidate selection is a critical step of
  the keyphrase extraction task. Based on a study of human-assigned keyphrases,
  we inferred two keyphrase properties (1.~small-sized 2.~noun phrases) and
  discussed how commonly used candidate selection methods satisfy them. To best
  fit those properties, we also proposed a new method for the selection of
  keyphrase candidates. Our method rely on the intuition that although
  keyphrases are noun phrases that can be modified by an adjective, the
  adjectival modification of a keyphrase must be justified by its contribution
  to the meaning of the keyphrase. Hence, only adjectives that are frequently
  used to modify the same noun phrase and only relational adjectives are
  accepted as keyphrase candidates.

  To validate our method, we carried out two experiments on three standard
  datasets. On the first hand, we showed that our method reduces the number of
  selected candidates without significantly decreasing the best possible recall.
  On the second hand, we showed that, in most cases, our method induces the best
  results for every keyphrase extraction methods, which comforts our intuition
  that the quality of the selected candidates must prevail over their number.

  Our results showed that a simple linguistic filtering of adjectives can
  increase the quality of the selected candidates. In future work, we plan to
  focus on more complex linguistic and/or statistical adjective filters.

