\section{Supervised Keyphrase Selection\\from Topical Clusters}
\label{sec:supervised_keyphrase_selection_from_topical_clusters}
  Leveraging TopicRank's topic clustering and ranking, we propose a supervised
  method that extracts keyphrases from the important topics. To stay consistent
  with \newcite{bougouin2013topicrank}, we extract only one keyphrase per
  cluster. Instead of being the first appearing candidate keyphrase, the
  keyphrase we extract is the one that our trained classifier considers most
  appropriate. Figure~\ref{fig:topicrank_plus_svm} gives an overview of the
  processing steps of our method.

  In this work, we decide to use an SVM classifier. SVMs are very efficient
  classifiers. They support a large number of features and learn to weight them
  appropriatelty. Also, the SVM classifier is the best performing classifier in
  our approach.

  \begin{figure*}
    \tikzstyle{io}=[
      ellipse,
      minimum width=9cm,
      minimum height=2cm,
      fill=YellowGreen!20,
      draw=YellowGreen!33,
      transform shape,
      font={\huge}
    ]
    \tikzstyle{component}=[
      text centered,
      thick,
      rectangle,
      minimum width=9cm,
      minimum height=2cm,
      fill=Cerulean!20,
      draw=Cerulean!33,
      transform shape,
      font={\huge\bfseries}
    ]

    \centering
    \begin{tikzpicture}[thin,
                        align=center,
                        scale=.2125,
                        node distance=1.75cm,
                        every node/.style={text centered, transform shape}]
      % components and data
      \node[io] (training_documents) {training documents};
      \node[above=of training_documents, xshift=-5cm, yshift=-1.25cm, anchor=south west] (training) {\huge training};
      \node[component, right=of training_documents] (candidate_selection) {Candidate Selection};
      \node[component, right=of candidate_selection] (topical_clustering) {Topical Clustering};
      \node[component, right=of topical_clustering] (topic_ranking) {Topic Ranking};
      \node[component, right=of topic_ranking] (learning) {Learning};
      \node[io, below=of learning] (model) {model};

      \node[component, below=of model, yshift=-1cm] (keyphrase_selection) {Keyphrase Selection};
      \node[component, left=of keyphrase_selection] (topic_ranking2) {Topic Ranking};
      \node[component, left=of topic_ranking2] (topical_clustering2) {Topical Clustering};
      \node[component, left=of topical_clustering2] (candidate_selection2) {Candidate Selection};
      \node[io, left=of candidate_selection2] (document) {document};
      \node[component, right=of keyphrase_selection] (keyphrase_ranking) {Keyphrase Ranking};
      \node[io, right=of keyphrase_ranking] (keyphrases) {Keyphrases};
      \node[above=of document, xshift=-5cm, yshift=-1.25cm, anchor=south west] (extraction) {\huge extraction};

      \path[->] (training_documents) edge (candidate_selection);
      \path[->] (candidate_selection) edge (topical_clustering);
      \path[->] (topical_clustering) edge (topic_ranking);
      \path[->] (topic_ranking) edge (learning);
      \path[->] (learning) edge (model);
      \path[->] (model) edge (keyphrase_selection);
      \path[->] (document) edge (candidate_selection2);
      \path[->] (candidate_selection2) edge (topical_clustering2);
      \path[->] (topical_clustering2) edge (topic_ranking2);
      \path[->] (topic_ranking2) edge (keyphrase_selection);
      \path[->] (keyphrase_selection) edge (keyphrase_ranking);
      \path[->] (keyphrase_ranking) edge (keyphrases);

      \begin{pgfonlayer}{background}
        \filldraw [line width=2mm, black!10] (training_documents.north -| model.east) rectangle (model.south  -| training_documents.west);
        \filldraw [line width=2mm, black!10] (document.north -| keyphrases.east) rectangle (keyphrases.south  -| document.west);
      \end{pgfonlayer}

      % links
    \end{tikzpicture}
    \caption{Training and extraction process of our method
             \label{fig:topicrank_plus_svm}}
  \end{figure*}

  \subsection{Training samples}
  \label{subsec:training_samples}
    In order to better fit our objective, we select positive and negative
    examples from a set of relevant clusters instead of a set of all clusters.
    We consider relevant every cluster from which it is actually possible to
    discriminate one reference keyphrase out of multiple candidate keyphrases.
    As they do not fit this situation, we exclude clusters containing either a
    single candidate or only negative examples.

  \subsection{Features}
  \label{subsec:features}
    Previous work using machine learning selected features that help to
    determine the keyphrase likelyhood of a candidate. In our work, we also rely
    on such common features, but we also rely on new features related to the
    appropriateness of a candidate as a member of its topical cluster. In
    contrast, we call the first category of features ``candidate-based
    features'' and the second category ``cluster-based features''.

    \subsubsection{Candidate-based Features}
    \label{subsubsec:topically_independent_features}
      Following previous
      work~\cite{nguyen2007keadocumentstructure,lopez2010humb}, we use the
      length of a candidate (number of words) as a feature, along with other
      structural and distributional features.

      \paragraph{Structural features} Previous work showed that position-based
      features are good indicator of the keyphrase likelyhood of a candidate.
      Among others, \newcite{witten1999kea} used the position of the first
      occurrence of the candidate, while most recently
      \newcite{nguyen2007keadocumentstructure} used a vector of frequencies
      within sections of scientific papers (abstract, introduction, conclusions,
      etc.). In this work, we use four structural features: the position of the
      first occurrence of the candidate and three binary features if the
      candidate appears, respectively, in the first, second or last third of the
      document. These last features approximate the feature of
      \newcite{nguyen2007keadocumentstructure}. However, they are not restricted
      to scientific papers.

      \paragraph{Distributional features} We use three features that relies on
      candidate distributions to: determine their importance (TF-IDF, cf.
      equation~\ref{equa:tfidf}), the lexical cohesion of their
      words\footnote{We set a default value to single word candidates (0.5 for
      nouns and 0.0 for adjectives).} (GDC, cf.
      equation~\ref{equa:gdc})~\cite{lopez2010humb} and their relevancy as
      keyphrases. First introduced by \newcite{frank1999keafrequency}, the
      relevancy of a candidate as a keyphrase is simply the number of times the
      candidate is used as a keyphrase in the training corpus.

      \vspace{-1em}

      \begin{footnotesize}
        \begin{align}
          \text{TF-IDF}(c, d) &= \frac{\text{count}(c, d)}{|d|}
          \times \frac{-\text{log}_2|\{d \in D : c \in d\}|}{|D|} \label{equa:tfidf}\\
          \text{GDC}(c, d) &= \frac{|c| \times \text{count}(c, d) \times \text{log}_{10}\text{count}(c, d)}{\sum_{w \in c}\text{count}(w, d)}\label{equa:gdc}
        \end{align}
      \end{footnotesize}

      \vspace{-2.5em}

      ~\\where $D$ is the set of every document of a global corpus and
      $\text{count}(c, d)$ is the number of occurrences of the candidate $c$ in
      the document $d$.

    \subsubsection{Cluster-based Features}
    \label{subsubsec:topically_dependent_features}
      Cluster based features help to capture the appropriateness of a candidate
      as the representant of its topical cluster. The clustering strategy of
      \newcite{bougouin2013topicrank} is to add candidates to a cluster if their
      average stem overlap similarity with the candidates of the cluster is
      above a given threshold, so we decide to use this average similarity as a
      feature. However, \newcite{bougouin2013topicrank} explained that their
      clustering similarity is naive for the purpose of topic clustering.
      Therefore, we also use the average number of candidates (in the cluster)
      that do not share any word with the candidate. As a consequence, our
      cluster-based features represent both the similarity and the disimilarity
      of the candidate regarding the other candidates of the cluster.

  \subsection{Keyphrase Extraction}
  \label{subsec:keyphrase_extraction}
    To extract keyphrsaes, we propose to apply the trained SVM classifier on
    TopicRank's important topics to choose one keyphrase per each. Finally, we
    rank the selected keyphrases in order to extract only the top ones.

    \subsubsection{Keyphrase Selection}
    \label{subsubsec:keyphrase_selection}
      For each cluster built by TopicRank, we apply the SVM classifier to
      determine its best keyphrase. The best keyphrase of a cluster is the one
      that have the highest confidence overall candidates of the cluster. SVM's
      confidence score is given by the distance between the candidate and the
      separating hyperplane: the higher is the distance beetween the candidate
      and the hyperplane, the more confidence the candidate gets.

    \subsubsection{Keyphrase Ranking}
    \label{subsubsec:keyphrase_ranking}
      To rank the keyphrases, our method benefits from both topic ranking and
      machine learning. The ranking score is obtained as follow:

      \vspace{-1em}

      \begin{footnotesize}
        \begin{align}
          S(c) &= \alpha \times \text{topicrank}(c) + (1 - \alpha) \times p(c)
        \end{align}
      \end{footnotesize}

      \vspace{-2.5em}

      ~\\where $\text{topicrank}(c)$ is the importance score given by TopicRank for
      the cluster to which candidate $c$ belongs and $p(c)$ is the probability
      that candidate $c$ is a keyphrase according to the classifier. As SVMs do
      not provide probability scores, we use the Platt
      scaling~\cite{platt1999probabilisticsvm} to obtain $p(c)$. Finally, we
      empiracally set $\alpha$ to 0.75, meaning that more importance is given to
      TopicRank's score.

