\section{Supervised Keyphrase Selection\\from Topical Clusters}
\label{sec:supervised_keyphrase_selection_from_topical_clusters}
  Leveraging TopicRank's topic clustering and ranking, we propose a supervised
  method that extracts keyphrases from the best topics. To stay consistent to
  \newcite{bougouin2013topicrank} that stated keyphrases must be topically
  unredundant, we extract only one keyphrase per cluster. Instead of being the
  first appearing candidate keyphrase, the keyphrase we extract is the one that
  our trained model considers most appropriate.

  In this work, we decide to use an SVM classifier. SVMs \TODO{\dots} Also, the
  SVM classifier is the best performing classifier in our approach.

  \subsection{Training samples}
  \label{subsec:training_samples}
    In order to better fit our objective, we select positive and negative
    examples from a set of relevant clusters instead of a set of all clusters.
    We consider relevant every cluster from which it is actually possible to
    discriminate one reference keyphrase out of multiple candidate keyphrases.
    As they do not fit this situation, we exclude clusters containing either a
    single candidate or only negative examples.

  \subsection{Features}
  \label{subsec:features}
    Previous work using machine learning selected features that helps to
    determine the keyphrase likelyhood of a candidate. In our work, we also rely
    on such common features, but we also rely on new features related to the
    appropriateness of a candidate as a member of the topic. In contrast, we
    call the first category of features ``topically undependent features'' and
    the second category ``topically dependent features''.

    \subsubsection{Topically Undependent Features}
    \label{subsubsec:topically_undependent_features}
      Following previous
      work~\cite{nguyen2007keadocumentstructure,lopez2010humb}, we use the
      length of a candidate (number of words) as a feature, along with other
      structural and distributional features.

      \paragraph{Structural features} Previous work showed that position-based
      features are good indicator of the keyphrase likelyhood of a candidate.
      Among others, \newcite{witten1999kea} used the position of the first
      occurrence of the candidate, while most recently
      \newcite{nguyen2007keadocumentstructure} used a vector of frequencies
      within sections of scientific papers (abstract, introduction, conclusions,
      etc.). In this work, we use four structural features: the position of the
      first occurrence of the candidate and three binary features if the
      candidate appears, respectively, in the first, second or last third of the
      document. These last features approximate the feature of
      \newcite{nguyen2007keadocumentstructure}. However, they are not restricted
      to scientific papers.

      \paragraph{Distributional features} We use three features that relies on
      candidate distributions to: determine their importance (TF-IDF, cf.
      equation~\ref{equa:tfidf}), the lexical cohesion of their
      words\footnote{We set a default value to single word candidates (0.5 for
      nouns and 0.0 for adjectives).} (GDC, cf.
      equation~\ref{equa:gdc})~\cite{lopez2010humb} and their relevancy as
      keyphrases. First introduced by \newcite{frank1999keafrequency}, the
      relevancy of a candidate as a keyphrase is simply the number of times the
      candidate is used as a keyphrase in the training corpus.

      \vspace{-1em}

      \begin{footnotesize}
        \begin{align}
          \text{TF-IDF}(c, d) &= \frac{\text{count}(c, d)}{|d|}
          \times \frac{-\text{log}_2|\{d \in D : c \in d\}|}{|D|} \label{equa:tfidf}\\
          \text{GDC}(c, d) &= \frac{|c| \times \text{count}(c, d) \times \text{log}_{10}\text{count}(c, d)}{\sum_{w \in c}\text{count}(w, d)}\label{equa:gdc}
        \end{align}
      \end{footnotesize}

      \vspace{-1em}

      where $D$ is the set of every document of a global corpus and
      $\text{count}(c, d)$ is the number of occurrences of the candidate $c$ in
      the document $d$.

    \subsubsection{Topically Dependent Features}
    \label{subsubsec:topically_dependent_features}
      Topically dependent features, unlike topically undependent features, help
      to capture the appropriateness of a candidate as the representant of its
      topic. The clustering strategy of \newcite{bougouin2013topicrank} is to
      add candidates to a cluster if their average stem overlap similarity with
      the candidates of the cluster is above a given threshold. We decide to
      use this average similarity as a feature. However,
      \newcite{bougouin2013topicrank} explained that their clustering similarity
      is naive for the purpose of topic clustering. Therefore, we also use the
      average number of candidates (in the cluster) that do not share any word
      with the candidate. As a consequence, our topically dependent features
      represent both the similarity and the disimilarity of the candidate
      regarding the other candidates of the cluster.

  \subsection{Keyphrase Extraction}
  \label{subsec:keyphrase_extraction}
    To extract keyphrsaes, we propose to apply the trained SVM classifier on
    TopicRank's most important topics to choose one keyphrase per each.

    \subsubsection{Keyphrase Selection}
    \label{subsubsec:keyphrase_selection}
      For each cluster built by TopicRank, we apply the SVM classifier to
      determine its best keyphrase. The best keyphrase of a cluster is the one
      that have the highest confidence overall candidates of the cluster. SVM's
      confidence score is given by the distance between the candidate and the
      separating hyperplane: the higher is the distance beetween the candidate
      and the hyperplane, the more confidence the candidate gets.

    \subsubsection{Keyphrase Re-Ranking}
    \label{subsubsec:keyphrase_reranking}
      To rank the keyphrases, our method benefits from both topic ranking and
      machine learning. The ranking score is obtained as follow:
      \begin{align}
        S(c) &= \alpha \times \text{topicrank}(c) + (1 - \alpha) \times p(c)
      \end{align}
      where $\text{topicrank}(c)$ is the importance score given by TopicRank for
      the cluster to which candidate $c$ belongs and $p(c)$ is the probability
      that candidate $c$ is a keyphrase according to the SVM classifier. As SVMs
      do not provide probability scores, we use the Platt
      scaling~\cite{platt1999probabilisticsvm} to obtain $p(c)$. Finally, we
      empiracally set $\alpha$ to 0.75, meaning that more importance is given to
      TopicRank's score.

