\chapter{État de l'art}
\label{chap:etat_de_l_art}
  \section{Classification de la bibliographie}
    L'extraction automatique de termes-clés est une tâche décomposée en général
    en quatre étapes. Les méthodes traitent les documents généralement un à un.
    Ils sont tout d'abord enrichis linguistiquement (segmentés en phrases,
    segmentés en mots et étiquetés en parties du discours). Des termes-clés
    candidats en sont ensuite sélectionnés, puis ordonnés ou classifiés afin de
    ne sélectionner que les plus pertinents (cf.
    figure~\ref{fig:etapes_de_l_extraction_de_termes_cles}). L'ordonnancement
    des termes-clés candidats est principalement effectué par des méthodes
    non-supervisées, alors que leur classification est généralement effectuée
    par des méthodes supervisées, qui, contrairement aux méthodes
    non-supervisées, requière une phase préliminaire d'apprentissage (cf.
    section~\ref{sec:methods}).

    La sélection des termes-clés candidats et leur ordonnancement ou leur
    classification sont les deux étapes auxquelles nous nous intéressons dans
    cet état de l'art. En effet, l'ordonnancement ou la classification des
    termes-clés candidats est le c\oe{}ur de la tâche d'extraction de
    termes-clés et ses performances dépendent de la qualité des candidats
    préalablement sélectionnés. Dans la section~\ref{sec:selection}, nous
    présentons les différentes méthodes de sélection de candidats, puis, dans la
    section~\ref{sec:methods}, nous présentons les méthodes d'extraction de
    termes-clés indépendamment de la méthode de sélection des candidats
    utilisée.

    \begin{figure}
      \tikzstyle{io}=[
        ellipse,
        minimum width=5cm,
        minimum height=2cm,
        fill=green!20,
        draw=green!33,
        transform
        shape,
        font={\huge}
      ]
      \tikzstyle{component}=[
        text
        centered,
        thick,
        rectangle,
        minimum
        width=14cm,
        minimum
        height=2cm,
        fill=cyan!20,
        draw=cyan!33,
        transform
        shape,
        font={\huge\bfseries}
      ]

      \centering
      \begin{tikzpicture}[thin,
                          align=center,
                          scale=.45,
                          node
                          distance=2cm,
                          every
                          node/.style={text
                                       centered,
                                       transform
                                       shape}]
        \node[io] (document) {document};
        \node[component] (preprocessing) [right=of document] {Prétraitement Linguistique};
        \node[component] (candidate_extraction) [below=of preprocessing] {Sélection des candidats};
        \node[component] (candidate_classification_and_ranking) [below=of candidate_extraction] {
          \begin{tabular}{r|l}
            Ordonnancement & des candidats\\
            Classification & \\
          \end{tabular}
        };
        \node[component] (keyphrase_selection) [below=of candidate_classification_and_ranking] {Sélection des termes-clés};
        \node[io] (keyphrases) [right=of keyphrase_selection] {termes-clés};

        \path[->, thick] (document) edge (preprocessing);
        \path[->, thick] (preprocessing) edge (candidate_extraction);
        \path[->, thick] (candidate_extraction) edge (candidate_classification_and_ranking);
        \path[->, thick] (candidate_classification_and_ranking) edge (keyphrase_selection);
        \path[->, thick] (keyphrase_selection) edge (keyphrases);
      \end{tikzpicture}
      \caption{Les quatre principales étapes de l'extraction automatique de
               termes-clés.
               \label{fig:etapes_de_l_extraction_de_termes_cles}}
    \end{figure}

    \subsection{Les méthodes de sélection de termes-clés candidats}
    \label{sec:selection}
      % Quel est l'objectif ?
      L'objectif de la sélection de termes-clés candidats est de réduire
      l'espace des solutions possibles pour l'extraction des termes-clés. Il
      s'agit de ne prendre dans le texte que les unités textuelle ayant des
      particularités similaires à celles des termes-clés tels qu'ils peuvent
      être donnés par des humains. Il y a deux avantages à cela. Le premier,
      très évident, est la réduction du temps de calcul nécessaire à
      l'extraction des termes-clés. Le second avantage est la suppression en
      amont d'unités textuelles non pertinentes qui peuvent affecter
      négativement les performances de l'ordonnancement ou de la classification.
      Pour distinguer les différents candidats sélectionnés, nous définissons
      deux catégories~: les candidats positifs, qui sont présents en tant que
      termes-clés de référence dans nos collections de données, et les candidats
      non positifs. Parmi les candidats non positifs, nous distinguons deux
      sous-catégories~: les candidats porteurs d'indices de différentes natures
      pouvant influencer la promotion de candidats positifs (p. ex., la présence
      du candidat \og{}article\fg{} peut influencer l'extraction du candidat
      positif \og{}article de recherche\fg{} en tant que terme-clé, dans le
      contexte de la notice de linguistique de la
      figure~\ref{fig:exemple_notice_inist}) et les candidats non pertinents,
      que nous considérons comme des erreurs.

      % Quels sont les différentes méthodes utilisées pour extraire les
      % termes-clés candidats ?
      Dans les travaux précédents, trois méthodes de sélection de candidats sont
      classiquement utilisées~: la sélection des n-grammes, des chunks nominaux,
      et des unités textuelles respectant certains patrons grammaticaux.

      \textbf{Les n-grammes} sont toutes les séquences ordonnées de $n$ mots,
      avec $n \in 1..m$, où $m$ vaut généralement
      3~\citep{witten1999kea,turney1999learningalgorithms,hulth2003keywordextraction}.
      Leur sélection est très exhaustive, elle fournit un grand nombre de
      termes-clés candidats, maximisant ainsi la quantité de candidats positifs,
      la quantité de candidats porteurs d'indices utiles, mais aussi la quantité
      de candidats non pertinents. Pour pallier en partie ce problème, il est
      courant d'utiliser une liste de mots vides pour filtrer les candidats. Les
      mots vides regroupent les mots fonctionnels de la langue (conjonctions,
      prépositions, etc.) et les mots courants (p. ex. \og{}particulier\fg{},
      \og{}près\fg{}, \og{}beaucoup\fg{}, etc.). Ainsi, un n-gramme contenant un
      mot outil en début ou en fin n'est pas considéré comme un terme-clé
      candidat. Malgré son aspect bruité, ce type de sélection est encore
      largement utilisé parmi les méthodes
      supervisées~\citep{witten1999kea,turney1999learningalgorithms,hulth2003keywordextraction}.
      En effet, la phase d'apprentissage de celles-ci les rend moins sensibles
      aux éventuels candidats erronés (bruit) par rapport aux méthodes
      non-supervisées.

      \textbf{Les chunks nominaux} sont des syntagmes non récursifs dont la tête
      est un nom, accompagné de ses éventuels modifieurs. Ce sont des segments
      linguistiquement définis, leur sélection est donc plus fiable que celle
      des n-grammes. Les expériences menées par
      \citet{hulth2003keywordextraction} et \citet{eichler2010keywe} avec les
      chunks nominaux montrent une amélioration des performances vis-à-vis de
      l'usage des n-grammes. Cependant, \citet{hulth2003keywordextraction}
      constate qu'en tirant parti de l'étiquetage en parties du discours des
      termes-clés candidats, l'extraction supervisée de termes-clés à partir de
      n-grammes donne des performances au-dessus de celles obtenues avec les
      chunks nominaux. En effet, l'usage de ce trait supplémentaire a pour effet
      de filtrer les n-grammes grammaticalement incorrects, favorisant alors
      l'extraction des candidats positifs.

      \textbf{Les unités textuelles respectant certains patrons grammaticaux}
      ont une nature qui est mieux contrôlée. À l'instar des chunks nominaux,
      leur sélection est plus fondée linguistiquement que celle des n-grammes
      filtrés. Dans ses travaux, \citet{hulth2003keywordextraction} choisie de
      sélectionner les candidats respectant les patrons des termes-clés de
      références les plus fréquents (plus de 10 occurrences) dans sa collection
      d'apprentissage, tandis que d'autres chercheurs tels que
      \citet{wan2008expandrank} et \citet{hassan2010conundrums} se concentrent
      uniquement sur les plus longues séquences de noms (noms propres inclus) et
      d'adjectifs.

    \subsection{Les méthodes d'extraction automatique de termes-clés}
    \label{sec:methods}
      L'objectif de l'extraction de termes-clés est d'identifier parmi les
      unités textuelles (candidats) d'un document celles qui représentent un
      concept important dans celui-ci, c'est-à-dire les unités textuelles qui
      caractérisent le mieux le contenu du document.

      Nous distinguons deux catégories de méthodes d'extraction automatique de
      termes-clés, les méthodes non-supervisées et les méthodes supervisées~:

      \subsubsection{Méthodes non-supervisées}
      \label{sec:unsupervised_methods}
        Les méthodes non-supervisées d'extraction de termes-clés ont la
        particularité de s'abstraire du domaine et de la langue des documents à
        analyser\footnote{L'abstraction de la langue est vraie pour ce qui est
        de la méthodologie, cependant les prétraitements tels que la
        segmentation en phrases, en mots et l'étiquetage en parties du discours
        sont dépendants de la langue.}. Cette abstraction est due au fait que
        les termes-clés candidats sont analysés avec des règles simples déduites
        à partir de traits statistiques issus seulement du texte analysé ou bien
        d'un corpus de référence non annoté.

        De nombreuses approches sont proposées. Certaines se fondent uniquement
        sur des statistiques alors que d'autres les combinent avec des
        représentations plus complexes des documents. Ces représentations
        peuvent aller de groupes de mots sémantiquement similaires à des graphes
        dont les n\oe{}uds sont des unités textuelles (mots, locutions, phrases,
        etc.) liées par des arcs qui représentent des liens de recommandation
        entre elles\footnote{Pour une étude comparative de certaines des
        méthodes par regroupement \citep{liu2009keycluster} et à base de graphe
        \citep{mihalcea2004textrank, wan2008expandrank}, voir l'article de
        \citet{hassan2010conundrums}.}.

        \paragraph{Les approches statistiques}
          cherchent à définir ce qu'est un terme-clé en s'appuyant sur certains
          traits statistiques et en étudiant leur rapport avec la notion
          d'importance d'un terme-clé candidat. Plus un terme-clé candidat est
          jugé important vis-à-vis du document analysé, plus celui-ci est
          pertinent en tant que terme-clé.

          TF-IDF (cf. équation \ref{math:tfidf}) de \citet{jones1972tfidf} et
          Likey (cf. équation \ref{math:likey}) de \citet{paukkeri2010likey}
          sont deux méthodes qui comparent le comportement d'un terme-clé
          candidat dans le document analysé avec son comportement dans une
          collection de documents. L'objectif est de trouver ceux dont le
          comportement dans le document varie positivement comparé à leur
          comportement global dans la collection. Dans les deux méthodes ceci
          s'exprime par le fait qu'un terme-clé candidat a une forte importance
          vis-à-vis du document analysé s'il y est très présent et s'il ne l'est
          pas dans le reste de la collection.
          \begin{align}
            \text{TF-IDF}(\text{terme}) &= TF(\text{terme}) \times \log\left(\frac{N}{DF(\text{terme})}\right) \label{math:tfidf}\\
            \notag\\
            \text{Likey}(\text{terme}) &= \frac{\text{rang}_{\text{document}}(\text{terme})}{\text{rang}_{\text{corpus}}(\text{terme})} \label{math:likey}
          \end{align}\\
          Dans TF-IDF, $TF$ (\textit{Term Frequency}) représente le nombre
          d'occurrences d'un terme-clé candidat dans le document analysé et $DF$
          (\textit{Document Frequency}) représente le nombre de documents dans
          lequel il est présent, $N$ étant le nombre total de documents. Plus le
          score TF-IDF d'un terme-clé candidat est élevé, plus celui-ci est
          important dans le document analysé. Dans Likey, le rang d'un terme-clé
          candidat dans le document et dans le corpus est obtenu à partir de son
          nombre d'occurrences, respectivement dans le document et dans la
          collection de documents. Plus le rapport entre ces deux rangs est
          faible, plus le terme-clé candidat évalué est important dans le
          document analysé.

          Okapi (ou BM25) \citep{robertson1999okapi} est une mesure alternative
          à TF-IDF. En Recherche d'Information (RI), celle-ci est plus utilisée
          que le TF-IDF. Bien que l'extraction automatique de termes-clés soit
          une discipline à la frontière entre le TAL et la RI, la méthode de
          pondération Okapi n'a, à notre connaissance, pas été appliquée pour
          l'extraction de termes-clés. Dans l'article de
          \citet{claveau2012vectorisation}, Okapi est décrit comme un TF-IDF
          prenant mieux en compte la longueur des documents. Cette dernière est
          utilisée pour normaliser le $TF$ (qui devient $TF_{BM25}$)~:
          \begin{align}
            \text{Okapi}(\text{terme}) &= TF_{BM25}(\text{terme}) \times \log\left(\frac{N - DF(\text{terme}) + 0,5}{DF(\text{terme}) + 0,5}\right) \label{math:okapi}\\
            \notag\\
            TF_{BM25} &= \frac{TF(\text{terme}) \times (k_1 + 1)}{TF(\text{terme}) + k_1 \times \left(1 - b + b \times \frac{DL}{DL_{\text{moyenne}}}\right)} \label{math:tf_bm25}
          \end{align}\\
          Dans la formule (\ref{math:tf_bm25}), $k_1$ et $b$ sont des constantes
          fixées à $2$ et $0,75$ respectivement. $DL$ représente la longueur du
          document analysé et $DL_{moyenne}$ la longueur moyenne des documents
          de la collection utilisée.

          \citet{barker2000nounphrasehead} travaillent avec les groupes nominaux
          comme candidats et estiment que les groupes nominaux complexes sont
          plus informatifs que les mots simples. Pour cela, leur approche est
          très simple : plus un groupe nominal est long et fréquent dans le
          document analysé, plus il est jugé pertinent en tant que terme-clé de
          ce document. Cependant, pour éviter la répétition dans le texte, les
          auteurs des documents utilisent les mêmes expressions sous des formes
          alternatives (plus courtes, par exemple). La fréquence d'une locution
          ne reflète donc pas forcément sa fréquence réelle d'utilisation, car
          celle-ci est répartie dans les différentes alternatives. De ce fait,
          \citet{barker2000nounphrasehead} repèrent dans les groupes nominaux
          leur tête et utilisent en plus la fréquence de celle-ci.

          \citet{tomokiyo2003languagemodel} tentent de vérifier statistiquement
          deux propriétés que doit respecter un terme-clé candidat pour être
          promu terme-clé. Les deux propriétés sont~:
          \begin{itemize}
            \item{l'informativité : un terme-clé doit capturer au moins une des
                  idées essentielles exprimées dans le document analysé;}
            \item{la grammaticalité : un terme-clé doit être bien formé
                  syntaxiquement.}
          \end{itemize}
          Pour vérifier ces deux propriétés, trois modèles de langue sont
          utilisés. Un modèle uni-gramme $ML_{\text{document}}^1$ et un modèle
          n-gramme $ML_{\text{document}}^N$ sont construits à partir du document
          analysé et le dernier modèle, n-gramme, $ML_{\text{référence}}^N$ est
          construit à partir d'une collection de documents (modèle de
          référence). Le modèle de référence fournit une vision globale de la
          distribution des n-grammes dans la langue (français, anglais, etc.).
          De ce fait, plus la probabilité d'un terme-clé candidat selon le
          modèle n-gramme du document diverge par rapport à sa probabilité selon
          le modèle de référence, plus il est informatif dans le document
          analysé (cf. équation \ref{math:informativeness}). De même, plus la
          probabilité d'un terme-clé candidat selon le modèle n-gramme du
          document diverge par rapport à sa probabilité selon le modèle
          uni-gramme du document, plus il respecte la propriété de
          grammaticalité (cf. équation \ref{math:phraseness}). La divergence est
          exprimée en terme de coût avec la divergence Kullback-Leibler (cf.
          équation \ref{math:kullbackleibler}).
          \begin{align}
            \text{informativité}(\text{terme}) &= KL_{\text{terme}}(ML_{\text{analyse}}^{N} || ML_{\text{référence}}^{N}) \label{math:informativeness}\\
            \notag\\
            \text{grammaticalité}(\text{terme}) &= KL_{\text{terme}}(ML_{\text{analyse}}^{N} || ML_{\text{analyse}}^{1}) \label{math:phraseness}\\
            \notag\\
            KL_{\text{terme}}(ML || ML') &= ML(\text{terme}) \log \frac{ML(\text{terme})}{ML'(\text{terme})} \label{math:kullbackleibler}\\
            \notag\\
            ML^N(\text{terme} = m_1\ m_2\ \dots\ m_k) &= \prod_{i = 1}^k P(m_i | m_{i - (N - 1)} m_{i - ((N - 1) - 1)} \dots m_{i - 1}) \notag
          \end{align}\\
          Pour finir, les termes-clés candidats sont ordonnés dans l'ordre
          décroissant de la somme des deux divergences et les meilleurs
          termes-clés candidats pour être des termes-clés sont ceux les mieux
          classés.

          Tout comme \citet{tomokiyo2003languagemodel},
          \citet{ding2011binaryintegerprogramming} tentent de définir des
          propriétés visant à affiner l'extraction de termes-clés. Les
          propriétés qu'ils définissent sont des contraintes sur l'ensemble de
          termes-clés qui doit être extrait. Les contraintes sont les
          suivantes~:
          \begin{itemize}
            \item{la couverture : un ensemble de termes-clés doit couvrir
                  l'intégralité des sujets abordés dans le document
                  représenté;}
            \item{la cohérence : les termes-clés de l'ensemble doivent être
                  cohérents entre eux.}
          \end{itemize}
          La contrainte de couverture est calculée avec le modèle \textit{Latent
          Dirichlet Allocation} (LDA) \citep{blei2003lda} qui donne la
          probabilité d'un terme-clé candidat sachant un sujet. Dans l'ensemble,
          la contrainte de cohérence est calculée pour chaque paire de
          termes-clés avec la mesure d'information mutuelle. Ces deux
          contraintes permettent de réduire le champs des possibilités, mais il
          reste encore à trouver quel ensemble parmi ceux possibles contient les
          meilleurs termes-clés. Pour cela, le degré d'importance des
          termes-clés candidats est évalué grâce au TF-IDF ainsi qu'à des traits
          liés à la position et à la présence du terme-clé candidat dans le
          titre (cf. équation \ref{math:binaryintegerprogramming}). Les auteurs
          cherchent ensuite la meilleure solution en utilisant une méthode
          d'optimisation (programmation par les entiers), avec pour objectif de
          trouver l'ensemble qui respecte les contraintes et dont l'importance
          des termes-clés candidats qu'il contient est maximale. Cette méthode a
          la particularité de donner directement un ensemble de
          termes-clés\footnote{La taille de l'ensemble de termes-clés à extraire
          est elle aussi une contrainte.}.
          \begin{align}
            \text{importance}(\text{terme}) &= \alpha \times \text{TF-IDF}(\text{terme}) \notag\\
                              &+ \beta\ \text{poids\_est\_dans\_titre}(\text{terme}) \notag\\
                              &+ \gamma\ \text{poids\_est\_dans\_première\_phrase}(\text{terme}) \label{math:binaryintegerprogramming}\\
            \notag\\
            \alpha + \beta + \gamma &= 1 \notag
          \end{align}

          Les traits statistiques des méthodes précédentes sont uniquement
          utilisés pour déterminer un score de pertinence des candidats en tant
          que termes-clés. Une donnée statistique non citée précédemment, mais
          pourtant récurrente dans les méthodes d'extraction de termes-clés, est
          la fréquence de co-occurrences entre deux candidats. Deux candidats
          co-occurrent s'ils apparaissent ensemble dans le même contexte. La
          co-occurrence peut être calculée de manière stricte (les candidats
          doivent être côte-à-côte) ou bien dans une fenêtre de mots. Compter le
          nombre de co-occurrences entre deux candidats permet d'estimer s'ils
          sont sémantiquement liés ou non. Ce lien sémantique à lui seul ne peut
          pas servir à extraire des termes-clés, mais il permet de mieux
          organiser les locutions d'un document pour affiner l'extraction
          \citep{matsuo2004wordcooccurrence, liu2009keycluster,
          mihalcea2004textrank}.

        \paragraph{Les approches par regroupement}
          définissent des groupes dont les unités textuelles partagent une ou
          plusieurs caractéristiques communes (similarité lexicale, similarité
          sémantique, etc.). Ainsi, lorsque des termes-clés sont extraits à
          partir de chaque groupe, cela permet de mieux couvrir le document
          analysé en fonction des caractéristiques choisies.

          Dans la méthode de \citet{matsuo2004wordcooccurrence}, les candidats
          les plus fréquents sont groupés en fonction de leur lien sémantique,
          calculé avec leur nombre de co-occurrences. Après ce regroupement, la
          méthode consiste à comparer les termes-clés candidats du document
          analysé avec les groupes $g$ de candidats fréquents, en faisant
          l'hypothèse qu'un terme-clé candidat qui co-occurre plus que selon
          toute probabilité avec les candidats fréquents d'un ou plusieurs
          groupes est plus vraisemblablement un terme-clé. La mesure $\chi^2$
          permet de vérifier cette hypothèse, en calculant le biais entre la
          fréquence de co-occurrence attendue et la fréquence de co-occurrence
          réelle~:
          \begin{align}
            \chi^2(\text{terme}) = \sum_{g} \frac{(\text{fréquence}(\text{terme}, g) - n_tp_g)^2}{n_tp_g}
          \end{align}
          $n_tp_g$ représente la fréquence de co-occurrence attendue entre le
          terme-clé candidat et le groupe $g$, $n_t$ étant le nombre de
          candidats avec lesquels le terme-clé candidat analysé co-occurre et
          $p_g$ étant la probabilité d'occurrence du groupe $g$ avec d'autres
          candidats\footnote{La probabilité des groupes de candidats fréquents
          est normalisée pour que la somme des probabilités des groupes soit
          égale à $1$.}. Lors de leurs expériences, les auteurs se sont aperçus
          que certains candidats peuvent être sémantiquement liées à un
          candidat fréquent dans un domaine plus général que celui du document
          analysé. En supposant que ces cas spéciaux soient ceux ayant le plus
          fort biais, ils décident de supprimer du $\chi^2$ l'argument maximum
          de la sommation~:
          \begin{align}
            \chi^2{'}(\text{terme}) = \chi^2 - \max_{g}\left\{\frac{(\text{fréquence}(\text{terme}, g) - n_tp_g)^2}{n_tp_g}\right\}
          \end{align}
          Les termes-clés extraits sont les termes-clés candidats ayant le plus
          fort biais mesuré avec la mesure $\chi^2{'}$.

          Dans l'algorithme KeyCluster,  \citet{liu2009keycluster} utilisent
          aussi un regroupement sémantique, mais dans leur cas ils ne
          considèrent que les mots (pas les candidats) du document analysé (mots
          vides exclus). Dans chaque groupe sémantique, le mot qui est le plus
          central est sélectionné comme mot de référence. L'ensemble des mots de
          référence est ensuite utilisé pour filtrer les candidats. Les
          termes-clés sont tous les candidats qui contiennent au moins un mot de
          référence (tous les mots de référence devant être utilisés dans au
          moins un terme-clé). Cette méthode présente l'avantage d'offrir une
          bonne couverture des sujets abordés dans un document, car tous les
          groupes sémantiques sont représentés par au moins un terme-clé.
          Cependant, les termes-clés extraits ne sont pas pondérés. Il n'est
          alors pas possible de définir un classement de ceux-ci dans le but de
          n'en extraire qu'un sous ensemble\footnote{Il est toutefois
          envisageable de définir un système de pondération basé par exemple sur
          le nombre de mots de références contenus dans le terme-clé candidat,
          en  utilisant le nombre de mots du groupe auquel appartiennent les
          mots de référence du terme-clé candidat, etc.}.

        \paragraph{Les approches à base de graphe}
          sont actuellement les plus populaires. Elles consistent à représenter
          le contenu d'un document sous la forme d'un graphe. La méthodologie
          appliquée est issue de PageRank \citep{brin1998pagerank}, un
          algorithme d'ordonnancement de pages Web (n\oe{}uds du graphe) grâce
          aux liens de recommandation qui existent entre elles (arcs du graphe).
          TextRank \citep{mihalcea2004textrank} et SingleRank
          \citep{wan2008expandrank} sont les deux adaptations de base de
          PageRank pour l'extraction automatique de termes-clés\footnote{Dans
          l'article de \citet{mihalcea2004textrank}, TextRank est aussi appliqué
          pour faire du résumé automatique.}. Dans celles-ci, les pages Web sont
          remplacées par des unités textuelles dont la granularité est le mot et
          un arc est créé entre deux n\oe{}uds si les mots qu'ils représentent
          co-occurrent dans une fenêtre de mots donnée.
        
          Le graphe est noté $G = (N, A)$, où $N$ est l'ensemble des n\oe{}uds
          du graphe et où $A$ est l'ensemble de ses arcs entrants et sortant :
          $A_{\text{entrant}} \cup A_{\text{sortant}}$\footnote{Dans le cas de
          TextRank et de SingleRank\ $A_{\text{entrant}} = A_{\text{sortant}}$,
          car le graphe n'est pas orienté.}. Pour chaque n\oe{}ud du graphe, un
          score est calculé par un processus itératif destiné à simuler la
          notion de recommandation d'une unité textuelle par
          d'autres\footnote{Plus le score d'une unité textuelle est élevé, plus
          celle-ci est importante dans le document analysé.} (cf. équation
          \ref{math:textrank}). Ce score, calculé pour chaque n\oe{}ud $n_i$,
          permet d'ordonner les mots par degré d'importance dans le document
          analysé. La liste ordonnée des mots est ensuite utilisée pour extraire
          les termes-clés~: les séquences de mots importants ($k$ premiers mots
          du classement) pour TextRank et les candidats dont la somme du score
          d'importance de leurs mots est la plus élevée pour SingleRank.
          \begin{align}
            S(n_i) &= (1 - \lambda) + \lambda \times \sum_{n_j \in A_{\text{entrant}}(n_i)} \frac{p_{j, i} \times S(n_j)}{\mathlarger{\sum}_{n_k \in A_{\text{sortant}}(n_j)} p_{j, k}} \label{math:textrank}
          \end{align}
          $\lambda$ est un facteur d'atténuation qui peut être considéré ici
          comme la probabilité pour que le n\oe{}ud $n_i$ soit atteint par
          recommandation. $p_{j, i}$ représente le poids de l'arc allant du
          n\oe{}ud $n_j$ vers le n\oe{}ud $n_i$, soit le nombre de
          co-occurrences entre les deux mots $i$ et $j$\footnote{TextRank
          utilise un graphe non-pondéré. Dans ce cas, $p_{j, i}$ vaut toujours
          $1$.}.

          Dans leurs travaux, \citet{wan2008expandrank} s'intéressent à l'ajout
          d'informations dans le graphe grâce à des documents similaires
          (voisins) et aux relations de co-occurrences qu'ils possèdent. Cette
          méthode, appelée ExpandRank, a pour objectif de faire mieux ressortir
          les mots importants du graphe en ajoutant de nouveaux liens de
          recommandation ou bien en renforçant ceux qui existent déjà. Les
          termes-clés extraits sont uniquement issus du document $d$ analysé,
          mais leur extraction est affiné grâce à l'usage des document voisins
          $V_d$. L'usage de documents voisins peut cependant ajouter ou
          renforcer des liens qui ne devraient pas l'être. Pour éviter cela, les
          auteurs réduisent leur impact en utilisant leur degré de similarité
          avec le document analysé. Ce degré de similarité est utilisé lors de
          la pondération des arcs~:
          \begin{align}
            p_{j, i}^d &= \sum_{v \in V_d \cup \{d\}} \text{similarité}(d, v) \times p_{j, i}^v
          \end{align}
          La méthode CollabRank, également proposée par
          \citet{wan2008collabrank}, est une alternative à ExpandRank. Elle
          fonctionne de la même manière, mais certains choix des auteurs rendent
          impossible l'usage du degré de similarité pour réduire l'impact des
          documents voisins. En effet, dans CollabRank les documents sont
          regroupés avant tout traitement et pour chaque groupe, le graphe n'est
          pas construit à partir d'un document donné (le document à analyser,
          dans la méthode ExpandRank), mais à partir de tous les documents du
          groupe. Les résultats moins concluants de CollabRank semblent
          confirmer la pertinence de l'usage du degré de similarité pour réduire
          les effets de bruit.

          Dans l'optique d'améliorer encore TextRank/SingleRank,
          \citet{liu2010topicalpagerank} proposent une méthode qui cherche cette
          fois-ci à augmenter la couverture de l'ensemble des termes-clés
          extraits dans le document analysé (TopicalPageRank). Pour ce faire,
          ils tentent d'affiner le rang d'importance des mots dans le document
          en tenant compte de leur rang dans chaque sujet abordé. Le rang d'un
          mot pour un sujet est obtenu en intégrant à son score PageRank la
          probabilité qu'il appartienne au sujet (cf. équation
          \ref{math:topicalpagerank}). Cette probabilité est obtenue avec le
          modèle LDA \citep{blei2003lda}. Les candidats sont ensuite ordonnés,
          selon l'ordre décroissant d'un score calculé à partir des rangs, dans
          chaque sujet, des mots d'un candidat donné (cf. équation
          \ref{math:topicalpagerankfinalscore}).
          \begin{align}
            S_{\text{sujet}}(n_i) &= (1 - \lambda) \times p(\text{sujet} | i) + \lambda \times \sum_{n_j \in A_{\text{entrant}}(n_i)} \frac{p_{j, i} \times S(n_j)}{\mathlarger{\sum}_{N_k \in A_{sortant}(N_j)} p_{j, k}} \label{math:topicalpagerank}\\
            Score(\text{terme}) &= \mathlarger{\sum}_{\text{sujet}} p(\text{sujet} | \text{document}) \times \sum_{m \in \text{terme}} \text{rang}_{\text{sujet}}(m) \label{math:topicalpagerankfinalscore}
          \end{align}

          Les approches à base de graphe présentées ci-dessus effectuent toutes
          un ordonnancement des mots du document selon leur importance dans
          celui-ci. Pour extraire les termes-clés il est ensuite nécessaire
          d'effectuer un travail supplémentaire à partir de la liste ordonnée de
          mots. Dans la méthode TextRank, les $k$ mots les plus importants sont
          considérés comme les mots-clés du document et toutes les séquences de
          mots-clés adjacents dans le document sont extraites comme termes-clés.
          La technique utilisée dans les autres méthodes consiste à ordonner les
          termes-clés candidats en fonction de la somme du score des mots qui
          les composent. Cependant, puisque l'un des avantages du graphe est que
          les n\oe{}uds peuvent avoir une granularité contrôlée,
          \citet{liang2009querylog} décident d'utiliser les termes-clés
          candidats au lieu des mots simples et de tirer profit de traits
          supplémentaires pour pondérer les arêtes. Ces traits sont la taille
          des termes-clés candidats et leur première position dans le document
          analysé~:
          \begin{align}
            p'_{j, i} &= (\text{taille}(j) + \text{position}(j)) \times p_{j, i}
          \end{align}
          Selon cette formule, $\text{position}(j)$ ne semble pas correspondre à
          la position de $j$, mais plutôt à l'inverse de sa position. Ainsi,
          l'importance est donnée aux candidats situés au début du document
          analysé.

          \citet{tsatsaronis2010semanticrank} travaillent aussi directement sur
          le calcul d'un score pour les candidats. Dans leur approche, ils
          modifient les relations utilisées entre les n\oe{}uds du graphe (cf.
          équation \ref{math:semanticrank}), ainsi que le calcul du score pour
          chacun d'eux. La relation qui lie deux n\oe{}uds est toujours une
          relation sémantique, mais celle-ci est déterminée de manière plus
          précise. Pour cela, les ressources externes WordNet
          \citep{miller1995wordnet} et Wikipédia sont utilisées. WordNet est une
          base de données lexicales qui fournit un vecteur de synonymes pour les
          noms, les verbes, les adverbes et les adjectifs. Le vecteur de
          synonymes est ici considéré comme l'ensemble de tous les sens
          possibles pour une unité lexicale, c'est son vecteur sémantique. À
          partir des vecteurs sémantiques, toutes les paires sémantiques $P_{i,
          j}$ possible entre deux termes-clés candidats $t_i$ et $t_j$ (un sens
          de $t_i$ avec un sens de $t_j$) ainsi que tous les chemins $C_{i, j}$
          possible pour atteindre un sens de $t_j$ à partir d'un sens de $t_i$
          (selon les données de WordNet) sont construits. Le score de similarité
          sémantique avec WordNet est obtenu en trouvant le couple paire
          sémantique/chemin sémantique pour lequel le produit des mesures
          sémantiques \textit{Semantic Compactness} (SCM) et \textit{Semantic
          Path Elaboration} (SPE), introduites par
          \citet{tsatsaronis2010textrelatedness}, est le plus élevé (cf.
          équation \ref{math:wordnetsemanticrelatedness}). Dans le cas où l'un
          des termes-clés candidats n'est pas présent dans WordNet, la
          similarité sémantique est calculée avec les données de Wikipédia.
          Cette similarité est défini par
          \citet{milne2008wikipediasemanticrelatedness} (cf. équation
          \ref{math:wikipediasemanticrelatedness}). Pour cette similarité,
          l'intuition est que plus deux candidats sont utilisés dans les mêmes
          articles, plus ils sont liés sémantiquement.
          \begin{align}
            p_{j, i} &= \left\{\begin{array}{ll}
              1 & \text{si $t_i = t_j$}\\
              \text{Sim}_{WN} & \text{sinon, si $t_i, t_j \in \text{WordNet}$}\\
               \text{Sim}_{W} &  \text{sinon, si $t_i, t_j \in \text{Wikipedia}$}\\
              0 & \text{sinon}
            \end{array}\right. \label{math:semanticrank}\\
            \notag\\
            \text{Sim}_{WN}(t_i, t_j) &= \max_{p \in P_{i, j}}\left\{\max_{c \in C_{i, j}}\left\{SCM(p, c) \times SPE(p, c)\right\}\right\} \label{math:wordnetsemanticrelatedness}\\
            \notag\\
            \text{Sim}_{W}(t_i, t_j) &= \frac{\log(\max(|w_i|, |w_j)) - \log(|w_i \cup w_j|)}{\log(|\text{Wikipedia}|) - \log(\min(|w_i|, |w_j|))} \label{math:wikipediasemanticrelatedness}\\
            \notag\\
            w_n &= \left\{\text{article} \in \text{Wikipedia} | t_n \in \text{article}\right\} \notag
          \end{align}
          En ce qui concerne le calcul du score de chaque n\oe{}ud, les auteurs
          expérimentent leur méthode avec la formule classique de PageRank
          \citep{brin1998pagerank}, plus deux autres variantes, et la formule
          HITS \citep{kleinberg1999hits}. L'une des deux variantes de PageRank
          utilise le TF-IDF comme trait supplémentaire (cf. équation
          \ref{math:apw}) et l'autre utilise un facteur $\lambda_i$ à la place
          du facteur $\lambda$ (cf. équation \ref{math:ppr}). Ce nouveau facteur
          est spécifique au terme-clé candidat $t_i$. Aucun détail précis
          n'indique comment $\lambda_i$ est déterminé, mais sa valeur est liée à
          la présence ou non de $t_i$ dans le titre du document analysé. Le
          score calculé avec HITS repose sur les notions d'autorité et de
          centralité. Un n\oe{}ud du graphe a de l'autorité si ses arcs entrants
          proviennent de n\oe{}uds centraux (cf. équation
          \ref{math:hitsauthority}). De même, un n\oe{}ud est central s'il
          atteint des n\oe{}uds de forte autorité (cf. équation
          \ref{math:hitshub}). Il s'agit là d'un renforcement mutuel. Le score
          retenu pour l'ordonnancement des termes-clés candidats est le score
          d'autorité.
          \begin{align}
            S_{\text{TF-IDF}}(n_i) &= \frac{1}{2} \times \left(\frac{S(n_i)}{\mathlarger{\max}_{n_j \in N}(S(n_j))} + \frac{\text{TF-IDF}(t_i)}{\mathlarger{\max}_{n_j \in N}(\text{TF-IDF}(t_j))}\right) \label{math:apw}\\
            \notag\\
            S_{\lambda}(n_i) &= (1 - \lambda_i) + \lambda_i \times \sum_{n_j \in A_{\text{entrant}}(n_i)} \frac{p_{j, i} \times S_{\lambda}(n_j)}{\mathlarger{\sum}_{n_k \in A_{\text{sortant}}(n_j)} p_{j, k}} \label{math:ppr}\\
            \notag\\
            \text{autorité}(n_i) &= \sum_{n_j \in A_{\text{entrant}}(n_i)}p_{j, i} \times \text{centralité}(n_j) \label{math:hitsauthority}\\
            \notag\\
            \text{centralité}(n_i) &= \sum_{n_j \in A_{\text{sortant}}(n_i)}p_{i, j} \times \text{autorité}(n_j) \label{math:hitshub}
          \end{align}
          Dans les expériences menés par les auteurs, le score PageRank et ses
          variantes permettent d'obtenir de meilleurs résultats que la formule
          HITS. Les deux variantes $S_{\text{TF-IDF}}$ et $S_{\lambda}$ de
          PageRank permettent d'obtenir de meilleurs résultats que l'originale,
          $S_{\lambda}$ étant meilleure.

      \subsubsection{Méthodes supervisées}
      \label{sec:supervised_methods}
        Les méthodes supervisées sont des méthodes capables d'apprendre à
        réaliser une tâche particulière, soit ici l'extraction de termes-clés.
        Leur apprentissage se fait grâce à un corpus dont les documents sont
        annotés en termes-clés. L'annotation permet d'extraire les exemples et
        les contres-exemples dont les traits statistiques et/ou linguistiques
        servent, le plus souvent, à apprendre une classification binaire~:
        \textit{terme-clé} ou \textit{non terme-clé}.

        De nombreux algorithmes d'apprentissage sont utilisés dans divers
        domaines. Ils peuvent potentiellement s'adapter à n'importe quelle
        tâche, dont celle de l'extraction automatique de termes-clés. Les
        algorithmes les plus couramment utilisés pour l'extraction automatique
        de termes-clés construisent des modèles probabilistes, des arbres de
        décision, des Séparateurs à Vaste Marge (SVM) ou encore des réseaux de
        neurones\footnote{\citet{sarkar2012machinelearningcomparison} proposent
        une étude comparative de l'usage des arbres de décision, de la
        classification naïve bayésienne et des réseaux de neurones pour
        l'extraction automatique de termes-clés.}.

        \paragraph{Les modèles probabilistes}
          apprennent des distributions de probabilités pour qu'un terme-clé
          candidat soit un terme-clé en fonction de divers traits (un seul trait
          par distribution probabiliste). Les distributions sont ensuite
          combinées pour donner un score de vraisemblance permettant de classer
          un terme-clé candidat parmi les \textit{termes-clés} ou les
          \textit{non termes-clés}.

          KEA \citep{witten1999kea} est une méthode qui utilise une
          classification naïve bayésienne pour attribuer un score de
          vraisemblance à chaque terme-clé candidat, le but étant d'indiquer
          s'ils sont des termes-clés ou non\footnote{Il est important de noter
          que le score de vraisemblance pour chaque terme-clé candidat permet
          aussi de les ordonner entre eux.}. \citet{witten1999kea} utilisent
          trois distributions conditionnelles apprises à partir du corpus
          d'apprentissage. La première correspond à la probabilité pour que
          chaque terme-clé candidat soit étiqueté \textit{oui}
          (\textit{terme-clé}) ou \textit{non} (\textit{non terme-clé}). Les
          deux autres correspondent à deux différents traits qui sont le poids
          TF-IDF du terme-clé candidat et sa première position dans le document~:
          \begin{align}
            P(\text{terme}) &= \frac{P_{\text{oui}}(\text{terme})}{P_{\text{oui}}(\text{terme}) + P_{\text{non}}(\text{terme})} \label{math:kea}\\
            \notag\\
            P_{\text{oui}}(\text{terme}) &= P(\text{terme} | \text{oui}) \times \prod_{\text{trait} \in \{\text{TF-IDF}, \text{position}\}} P_{\text{trait}}\left(\text{trait}(\text{terme}) | \text{oui}\right) \notag\\
            \notag\\
            P_{\text{non}}(\text{terme}) &= P(\text{terme} | \text{non}) \times \prod_{\text{trait} \in \{\text{TF-IDF}, \text{position}\}} P_{\text{trait}}\left(\text{trait}(\text{terme}) | \text{non}\right) \notag
          \end{align}
          L'un des avantages de la classification naïve bayésienne est que
          chaque distribution est supposée indépendante. L'ajout de nouveaux
          traits dans la méthode KEA est donc très aisé.
          
          Parmi les variantes de KEA proposées, \citet{frank1999keafrequency}
          ajoutent un troisième trait : le nombre de fois que le candidat est un
          terme-clé dans le corpus d'apprentissage. L'ajout de ce trait permet
          d'améliorer les performances de la version originale de KEA, mais
          uniquement lorsque la quantité de données d'apprentissage est très
          importante.
          
          Une autre amélioration de KEA, proposée par
          \citet{turney2003keacoherence}, tente d'augmenter la cohérence entre
          les candidats les mieux classés. Pour ce faire, une première étape de
          classification est effectuée avec la méthode originale. Celle-ci
          fournit un premier classement des candidats selon leur score de
          vraisemblance. Ensuite, de nouveaux traits sont ajoutés et une
          nouvelle étape de classification est lancée. Les nouveaux traits ont
          pour but d'augmenter le score de vraisemblance des candidats ayant un
          fort lien sémantique avec certains des candidats les mieux classés
          après la première étape. Pour cela, deux scores de similarité
          sémantique sont calculés entre les candidats classés parmi les $L$
          premiers et les candidats classés parmi les $K$ ($K < L$)
          premiers\footnote{Du fait du calcul de deux scores de similarité des
          $L$ meilleurs candidats avec les $K$ ($k < L$) meilleurs, il y a $2K$
          traits qui sont ajoutés.}. Les deux scores de similarité sont obtenus
          en comptant les pages Web pour lesquelles le candidat parmi les $L$
          meilleurs et le candidat parmi les $K$ meilleurs apparaissent ensemble
          dans le titre (pour le premier score) ou dans le corps (pour le second
          score).

          \citet{nguyen2007keadocumentstructure} s'intéressent aux articles
          scientifiques et proposent aussi de modifier KEA en ajoutant cette
          fois-ci des informations concernant la structure des documents. En
          effet, certaines sections telles que l'introduction et la conclusion
          dans les articles scientifiques sont plus susceptibles de contenir des
          termes-clés qu'une section présentant des résultats expérimentaux, par
          exemple. Pour ce faire, ils utilisent un vecteur d'occurrences des
          termes-clés candidats dans les sections typique d'un article
          scientifique. Dans leur version modifiée de KEA, ils proposent aussi
          l'usage de traits linguistiques. Leur classifieur utilise l'étiquetage
          en parties du discours et les suffixes des mots du terme-clé candidat
          comme traits supplémentaires. Notez que le choix d'utiliser les
          suffixes est justifié pour l'anglais, mais peut ne pas l'être pour
          toutes les langues. En effet, pour l'anglais, les auteurs remarques
          que les suffixes des lemmes\footnote{Un lemme est une unité lexicale.
          Elle ne présente aucune flexion. Ainsi \og avion\fg\ est le lemme de
          \og avions \fg\ et \og voler \fg\ est le lemme de \og volait \fg. Ce
          sont les lemmes qui se trouvent dans un dictionnaire.} et de leurs
          modifieurs ne sont en règle générale pas les mêmes. Selon les auteurs,
          les lemmes peuvent être distingués par leur suffixes \textit{-ion},
          \textit{-ics} et \textit{-ment} tandis que les suffixes \textit{-ive},
          \textit{-al} et \textit{-ic} sont plus fréquemment utilisés dans les
          modifieurs.

          La même année que les travaux de \citet{hulth2003keywordextraction}
          sur le bien fondé d'utiliser des traits linguistiques pour
          l'extraction automatique de termes-clés,
          \citet{sujian2003maximumentropy} proposent une méthode utilisant un
          modèle d'entropie maximale (cf. équation \ref{math:maximum_entropy})
          dont l'un des traits repose sur les parties du discours des mots qui
          composent les candidats. Un modèle de maximum d'entropie consiste à
          trouver parmi plusieurs distributions (une pour chaque trait),
          laquelle a la plus forte entropie (cf. équation \ref{math:entropy}).
          La distribution ayant la plus forte entropie est par définition celle
          qui contient le moins d'information, ce qui la rend de ce fait moins
          arbitraire et donc plus appropriée pour l'extraction automatique de
          termes-clés.
          \begin{align}
            \text{Score}(\text{terme}) &= \frac{P(\text{oui} | \text{terme})}{P(\text{non} | \text{terme})} \label{math:maximum_entropy}\\
            \notag\\
            P(\text{classe} | \text{terme}) &= \frac{\exp\left(\mathlarger{\sum}_{\text{trait}} \alpha_{\text{trait}} \times \text{trait}(\text{terme}, \text{classe})\right)}{\mathlarger{\sum}_{c \in \{\text{oui}, \text{non}\}} \exp\left(\mathlarger{\sum}_{\text{trait}} \alpha_{\text{trait}} \times \text{trait}(\text{terme}, c)\right)} \label{math:entropy}
          \end{align}
          Le paramètre $\alpha_{\text{trait}}$ définit l'importance du
          $\text{trait}$ auquel il est associé.

          Dans leurs travaux, \citet{liu2011vocabularygap} proposent une méthode
          d'extraction de termes-clés utilisant aussi un modèle probabiliste.
          Leur méthode est très différente de celle de \citet{witten1999kea} et
          de \citet{sujian2003maximumentropy} puisqu'ils décident d'utiliser une
          approche de traduction automatique. L'usage original de cette approche
          est justifié par le fait qu'un ensemble de termes-clés doit décrire de
          manière synthétique le document. Leur hypothèse est donc qu'un
          ensemble de termes-clés est une traduction d'un document dans une
          autre langue, plus synthétique. Le modèle est appris à partir de
          paires de traductions dont l'une des locutions est issue des titres ou
          des résumés des documents du corpus d'apprentissage et dont l'autre
          locution est issue des corps de ces mêmes documents. Les titres et les
          résumés sont utilisés comme langue synthétique et les corps des
          documents comme le langage naturel. Le modèle appris tient compte de
          la probabilité d'avoir une locution synthétique $t_s$ (terme-clé
          candidat) sachant que le document contient la locution $t_d$~:
            \begin{align}
              P(t_d, t_s) &= \left(\frac{\lambda}{P(t_d | t_s)} + \frac{1 - \lambda}{P(t_s | t_d}\right)^{-1}
            \end{align}
          Il s'agit d'une moyenne harmonique paramétrée par le facteur
          $\lambda$. Elle sert à ordonner les termes-clés candidats selon leur
          pertinence en tant que terme-clé.

        \paragraph{Les arbres de décision}
          sont des classifieurs dont les branches représentent des tests sur des
          traits des candidats. Ces tests permettent de router les candidats
          vers les feuilles de l'arbre représentant leurs classes respectives
          (\textit{terme-clé} ou \textit{non terme-clé}).

          Dans son article sur l'apprentissage pour l'extraction automatique de
          termes-clés, \citet{turney1999learningalgorithms} présente une méthode
          qui utilise de nombreux traits pour entraîner $50$ arbres de décision
          C4.5. L'usage de plusieurs arbres de décision est une technique
          appelée \textit{Random Forest}. Grâce à cette technique, l'extraction
          automatique de termes-clés est réduite à un vote de chaque arbre pour
          chaque candidat et cela permet un classement des candidats en fonction
          de leur nombre de votes positifs. Les termes-clés extraits
          correspondent aux candidats les mieux classés.

          Tout comme \citet{turney1999learningalgorithms},
          \citet{ercan2007lexicalchains} utilisent des arbres C4.5 dans leur
          méthode d'extraction de termes-clés. Ils utilisent principalement des
          traits classiques, mais leur contribution se situe au niveau de
          l'utilisation d'un trait calculé à partir de chaînes lexicales. Une
          chaîne lexicale lie les mots d'un document selon certaines relations
          telles que la synonymie, l'hyponymie ou la méronymie. Ces différentes
          relations ont un poids en fonction de leur importance et un score pour
          chaque terme-clé candidat est calculé en faisant la somme des poids
          des relations présentes dans la chaîne lexicale. Cette approche est
          intéressante, mais du fait de la faible performance des méthodes de
          construction de chaînes lexicales elle présente l'inconvénient de ne
          retourner que des mots. Ce problème peut toutefois être pallier grâce
          aux arbres C4.5 qui permet un classement des mots à partir de leur
          nombre de votes positifs. Il est ensuite envisageable de déduire les
          termes-clés à partir de la liste ordonnée et pondérée des mots clés
          (voir les méthodes non-supervisées à base de graphe -- section
          \ref{sec:unsupervised_methods}).

        \paragraph{Les séparateurs à Vastes Marges (SVM)}
          sont aussi des classifieurs utilisés par les méthodes d'extraction
          automatique de termes-clés. Ils exploitent divers traits afin de
          projeter des exemples et des contres-exemples sur un plan, puis ils
          cherchent l'hyperplan qui les sépare. Cet hyperplan sert ensuite dans
          l'analyse de nouvelles données. Dans le contexte de l'extraction de
          termes-clés, les exemples sont les termes-clés et les contres-exemples
          sont les termes-clés candidats qui ne sont pas des termes-clés. Ce
          mode de fonctionnement des SVM est utilisé par \citet{zhang2006svm},
          mais un autre type de SVM est plus largement utilisé dans les méthodes
          supervisées d'extraction de termes-clés. Il s'agit de SVM qui
          utilisent de multiples marges représentant des rangs. Ces classifieurs
          permettent donc d'ordonner les termes-clés lors de leur extraction
          \citep{herbrich1999svm, joachims2006linearsvm, jiang2009rankingsvm}.
          La méthode KeyWE de \citet{eichler2010keywe} utilise ce type de SVM
          avec le trait TF-IDF ainsi qu'un trait booléen ayant la valeur vraie
          si le terme-clé candidat apparaît dans un titre d'un article Wikipedia
          (un terme-clé candidat apparaissant dans le titre d'un article de
          Wikipedia est considéré comme ayant une plus forte probabilité d'être
          un terme-clé). L'ordonnancement des termes-clés candidats par le SVM
          permet ensuite de contrôler le nombre de termes-clés à extraire (choix
          des $k$ termes-clés candidats les mieux classés).

        \paragraph{GenEx}
          est un algorithme génétique mis au point par
          \citet{turney1999learningalgorithms}. Il est constitué de deux
          composants. Le premier composant, le géniteur, sert à apprendre des
          paramètres lors de la phase d'apprentissage. Ces paramètres sont
          utilisés par le second composant, l'extracteur, pour donner un score
          d'importance à chaque candidat. Plus les paramètres sont optimaux,
          meilleure est la classification des candidats. De ce fait, le géniteur
          cherche leurs valeurs optimales en les représentant sous la forme de
          bits qui constituent une population d'individus qu'il fait évoluer
          jusqu'à obtenir un état stable, optimal. Les traits utilisés par
          l'extracteur concernent la longueur et la première position dans le
          document des candidats et des mots qui les composent. Les paramètres
          sont en partie des seuils définis pour les valeurs de ces traits. Ils
          servent à déterminer des facteurs multiplicateurs qui sont d'autres
          paramètres. Ces derniers agissent sur le calcul du score d'un candidat.

        \paragraph{Les perceptrons multicouches}
          peuvent aussi être utilisés pour l'extraction automatique de
          termes-clés \citep{sarkar2010neuralnetwork}. Un perceptron
          multi-couches est un réseau de neurones constitué d'au moins trois
          couches, chaque couche étant composée de neurones. Dans les deux
          couches extrêmes les neurones représentent respectivement les entrées
          et les sorties. Les couches centrales sont des couches cachées qui
          permettent d'acheminer les valeurs des entrées vers les sorties, où de
          nouvelles valeurs sont obtenues grâce à la pondération des transitions
          d'un neurone d'une couche vers un neurone de la couche suivante. Les
          entrées correspondent aux traits d'un terme-clé candidat (ici TF-IDF,
          la position, la taille, etc.) et les sorties représentent les classes
          qu'il peut prendre (ici, terme-clé ou non). La valeur obtenue pour
          chaque sortie (classe) permet d'obtenir une probabilité pour que le
          terme-clé candidat analysé soit un terme-clé ou non. Dans leur
          méthode, \citet{sarkar2010neuralnetwork} utilisent cette probabilité
          pour ordonner les termes-clés candidats afin de pouvoir extraire les
          meilleurs, lorsqu'un nombre précis de termes-clés doit être extrait.

    \subsection{Les mesures d'évaluation}
    \label{sec:evaluation}
      Lorsqu'une nouvelle méthode est proposée, il est nécessaire d'en évaluer
      les performances afin de pouvoir la situer vis-à-vis de celles qui
      existent déjà. Plusieurs mesures d'évaluation des systèmes d'extraction de
      termes-clés sont utilisées, dans un processus d'évaluation \og à la
      Cranfield \fg \ \citep{voorhees2002philosophy}. Dans ce processus, les
      évaluations sont réalisées sur les documents d'une collection de test dont
      les termes-clés à extraire sont connus (jugement de référence) et la
      moyenne des mesures obtenues pour chaque document est calculée. Le
      problème de ce paradigme est qu'il suppose que le jugement de référence
      est la réponse exacte. Il ne peut donc pas y avoir de réponse différente
      mais équivalente (avec par exemple, des synonymes dans les termes-clés) et
      de ce fait de nombreuses mesures d'évaluation sont pessimistes.

      En TAL, il est courant d'évaluer une méthode en termes de
      \textbf{précision} et de \textbf{rappel}. Ces deux mesures sont
      déterminées à partir des termes-clés de référence, des termes-clés
      extraits, des termes-clés extraits qui sont effectivement des termes-clés
      (vrais positifs) et des termes-clés extraits qui n'en sont pas réellement
      (faux positifs -- cf. tableau \ref{tab:confusionmatrix}). La précision
      s'intéresse à la quantité de vrais positifs parmi tous ceux qui sont
      extraits (cf. équation \ref{math:precision}), alors que le rappel
      s'intéresse à la quantité de vrais positifs parmi tous ceux attendus (cf.
      équation \ref{math:recall}). Suivant les systèmes, il peut être nécessaire
      de favoriser l'une ou l'autre de ces deux mesures. La précision doit être
      privilégiée lorsqu'il faut minimiser les erreurs (faux positifs), tandis
      que le rappel doit être maximisé lorsque l'on veut augmenter le nombre de
      vrais positifs sans se soucier du nombre de faux positifs. Il existe aussi
      des cas où il est souhaitable d'obtenir un compromis (avoir un maximum de
      vrais positifs lorsque le nombre de faux positifs est minimal). Ce
      compromis se calcule avec la \textbf{f-mesure} (cf. équation
      \ref{math:fmeasure}) qui peut être ajustée avec le paramètre $\beta$,
      agissant sur l'importance du rappel par rapport à la précision.
      \begin{table}
        \begin{center}
          \begin{tabular}{|c|c|c|c|}
            \cline{3-4}
            \multicolumn{2}{c|}{} & \multicolumn{2}{|c|}{\textbf{Référence}}\\
            \cline{3-4}
            \multicolumn{2}{c|}{} & Terme-clé & Non terme-clé\\
            \hline
            \multirow{2}{*}{\textbf{Système}} & Terme-clé & \textsc{VraisPositifs} & \textsc{FauxPositifs}\\
            \cline{2-4}
            & Non terme-clé & \textsc{FauxNegatifs} & \textsc{VraisNegatifs}\\
            \hline
          \end{tabular}
          \caption{Matrice de confusion des systèmes d'extraction automatique de
                   termes-clés. \label{tab:confusionmatrix}}
        \end{center}
      \end{table}
      \begin{align}
        \textsc{Positifs}_{\text{référence}} &= \textsc{VraisPositifs} \cup \textsc{FauxNegatifs}\notag\\
        \notag\\
        \textsc{Positifs}_{\text{système}} &= \textsc{VraisPositifs} \cup \textsc{FauxPositifs} \notag\\
        \notag\\
        \text{précision} &= \frac{|\textsc{VraisPositifs}|}{|\textsc{Positifs}_{\text{système}}|} \label{math:precision}\\
        \notag\\
        \text{rappel} &= \frac{|\textsc{VraisPositifs}|}{|\textsc{Positifs}_{\text{référence}}|} \label{math:recall}\\
        \notag\\
        \text{f-mesure} &= (1 + \beta^2) \times \frac{\text{précision} \times \text{rappel}}{(\beta^2 \times \text{précision}) + \text{rappel}} \label{math:fmeasure}
      \end{align}

      Dans le cas cité précédemment, où il est préférable d'extraire un maximum
      de vrais positifs sans ce soucier du nombre de faux positifs, il existe
      une variante de la mesure de précision. Cette variante, la
      \textbf{précision moyenne} (\textit{average precision}), tient compte
      uniquement du classement des vrais positifs (cf. équation
      \ref{math:averageprecision}). Ainsi, la valeur de la mesure est élevé
      quand les vrais positifs sont parmi les premiers termes-clés extraits,
      ceci quelque soit le nombre de faux positifs. Ce comportement diffère de
      celui de la précision qui a tendance à chuter lorsque le nombre de faux
      positifs grandit.
      \begin{align}
        \text{précision\_moyenne} &= \frac{\mathlarger{\sum}_{t \in {\textsc{VraisPositifs}}}\text{précision}@\text{rang}(t)}{|\textsc{Positifs}_{\text{référence}}|} \label{math:averageprecision}\\
        \notag\\
        \text{précision}@n &\equiv \text{la précision calculée pour les $n$ premiers résultats} \notag
      \end{align}
      La précision moyenne n'est pas la seule mesure qui tient compte du
      classement des vrais positifs. La \textbf{r-precision}
      \citep{zesch2009rprecision} et le \textbf{\textit{reciprocal rank}} (RR)
      \citep{voorhees1999mrr} sont deux mesures qui évaluent les systèmes
      d'extraction de termes-clés selon leur capacité à bien ordonner les
      termes-clés extraits. La mesure de r-présision représente la précision
      lorsque le système d'extraction automatique de termes-clés donne le même
      nombre de termes-clés que dans la référence (cf. équation
      \ref{math:rprecision}). Cela peut s'avérer utile pour comparer des
      systèmes qui n'extraient pas tous le même nombre de termes-clés. Quand au
      \textit{reciprocal rank}, il ne s'intéresse qu'à la position du premier
      vrai positif extrait (cf. équation \ref{math:reciprocalrank}\footnote{Le
      meilleur rang ayant la plus petite valeur (1), c'est la réciproque du plus
      petit rang qui est utilisé. Ainsi, plus la valeur de la mesure est élevé,
      meilleur est le système.}). Un utilisateur, faisant parfois attention
      seulement aux premières informations données, cette mesure est
      intéressante et peut être perçue comme la capacité à capter l'attention.
      \begin{align}
        \text{r-précision} &= \text{précision}@(|\textsc{Positifs}_{\text{référence}}|) \label{math:rprecision}\\
        \notag \\
        RR &= \frac{1}{\text{argmin}(\forall m \in \textsc{Positifs}_{\text{système}}, \text{rang}(m))} \label{math:reciprocalrank}
      \end{align}

      Toutes les mesures qui sont présentées ici ne prennent en compte que des
      correspondances parfaites entre les termes-clés extraits et ceux de
      l'ensemble de référence\footnote{En utilisant la racine des mots, des
      termes-clés extraits et des termes-clés de référence, il est possible de
      réduire cette correspondance stricte, en acceptant les variations
      flexionnelles (\textit{oranges} est accepté à la place de \textit{orange}
      par exemple).}. Certains travaux se penchent désormais sur une évaluation
      moins pessimiste des systèmes d'extraction de termes-clés, en prenant en
      compte les problèmes d'\textbf{inclusion} ou de \textbf{recouvrement
      partiel} \citep{zesch2009rprecision, kim2010rprecision} et en les
      appliquant à la r-precision. Dans leurs travaux,
      \citet{zesch2009rprecision} définissent deux nouveaux types de
      correspondance~:
      \begin{itemize}
        \item{l'inclusion : le terme-clé extrait est inclus dans le terme-clé de
              référence (par exemple, \og élection présidentielle \fg\ est inclus
              dans \og élection présidentielle américaine \fg);}
        \item{le recouvrement/alignement partiel: une sous partie (gauche ou
              droite) du terme-clé extrait est inclue dans le terme-clé de
              référence (par exemple, \og première élection présidentielle \fg\ et
              \og élection présidentielle américaine \fg\ se recouvrent
              partiellement).}
      \end{itemize}
      Là ou la correspondance parfaite ne donne que deux scores possible (0 :
      les deux termes-clés ne sont pas égaux ; 1 : les deux termes-clés sont
      égaux), la nouvelle mesure de correspondance donne un score de
      recouvrement basé sur les mots des deux termes-clés $t_i$ et $t_j$, $t_j$
      étant le terme-clé de référence~:
      \begin{align}
        \text{recouvrement}(t_{i}, t_{j}) &= \frac{|t_{i} \cap t_{j}|}{|t_{i}\ \cup\ t_{j}|}
      \end{align}
      Dans l'extension de ces travaux, \citet{kim2010rprecision} modifient le
      score d'alignement dans le but de prendre en compte l'ordre des mots dans
      le terme-clé de référence $t_j$~:
      \begin{align}
        \text{recouvrement}'(t_{i}, t_{j}) &= \frac{\mathlarger{\sum}_{m \in t_{j} \cap t_{i}} \frac{1}{|t_{j}| - (\text{position}_{t_i}(m) + 1)}}{\mathlarger{\sum}_{m' \in t_{i}} \frac{1}{|t_{i}| - (\text{position}_{t_i}(m') + 1)}}
      \end{align}

  \section{Mise en perspective}
    Dans un contexte d'accès à l'information numérique, l'indexation automatique
    joue un rôle crucial, car elle réduit chaque document à un ensemble de
    descripteurs utilisés pour évaluer la pertinence d'un document vis-à-vis
    d'un besoin d'un utilisateur. Les descripteurs sont appelés ici termes-clés,
    il s'agit de locutions ayant une forte importance dans le document qu'ils
    indexent. Cette notion d'importance est au c\oe{}ur du problème de
    l'extraction de termes-clés, problème que tentent de résoudre de nombreuses
    méthodes. Toutes les méthodes manipulent des traits statistiques et/ou
    linguistiques, soit pour vérifier des propriétés des termes-clés, dans le
    cas des méthodes non-supervisées, soit pour apprendre l'impact des
    différents traits sur l'importance d'un terme-clé candidat dans le cas des
    méthodes supervisées.

    Les méthodes non-supervisées sont des méthodes émergentes ayant la
    particularité de s'abstraire de la spécificité des données traitées. Cette
    abstraction s'explique par des approches basées sur des constatations à
    propos de ce qu'est un terme-clé au sens général : importance sémantique,
    degré d'information, structure syntaxique, etc. Contrairement aux méthodes
    non-supervisées, les méthodes supervisées n'utilisent pas de propriétés
    définies à partir de traits statistiques et linguistiques, mais elles
    utilisent des modèles de décision appris à partir de ces traits, calculés
    sur les termes-clés d'un corpus d'apprentissage. Ceci introduit une forte
    dépendance entre le système d'extraction de termes-clés et le type des
    données utilisées pour l'entraîner.

