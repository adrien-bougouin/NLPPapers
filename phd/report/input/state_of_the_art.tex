\chapter{État de l'art}
\label{chap:etat_de_l_art}
  \section{Classification de la bibliographie}
    \TODO{Introduction plus séparation entre sélection de candidats et extraction de
          termes-clés}
  \subsection{Les méthodes d'extraction automatique de termes-clés}
  \label{sec:methods}
    \TODO{A revoir}
    L'extraction automatique de termes-clés est une tâche qui consiste à analyser
    un document et à en extraire les aspects importants. Alors que les méthodes de
    résumé automatique utilisent des phrases pour construire une vision
    synthétique du document, l'extraction de termes-clés se focalise sur les
    unités textuelles qui composent ces phrases. Un ensemble de termes-clés peut
    donc être perçu comme un résumé dont les points-clés sont exprimés sans
    liaisons entre eux. Les unités textuelles sur lesquelles travaillent les
    systèmes d'extraction de termes-clés sont appelées termes candidats. Ces
    derniers sont des mots ou des phrasèmes pouvant être promus au statut de
    terme-clé.

    L'extraction de termes candidats est une étape préliminaire de l'extraction de
    termes-clés, que ce soit pour les méthodes non-supervisées ou supervisées.
    Cette étape est importante, car si certains termes-clés du document analysé ne
    sont pas présents dans l'ensemble des termes candidats, alors ceux-ci ne
    pourront pas être extraits.

    Un terme candidat, tout comme un terme-clé, est une locution. Il peut donc
    être soit un terme simple, soit un terme complexe. Un terme simple et composé
    d'un seul mot, par exemple \og patate \fg, et un terme complexe se compose
    d'au moins deux mots qui forment une expression à sens unique, par exemple
    \og pomme de terre \fg. Il existe, en TAL, plusieurs techniques permettant de
    les retrouver dans une document. La technique la plus simple consiste à
    extraire tous les n-grammes d'un document. Un n-gramme est une séquence
    ordonnée de $n$ mots, par exemple \og il mange une \fg, \og mange une
    pomme \fg, \og une pomme de \fg\ et \og pomme de terre \fg\ sont
    tous les tri-grammes de la phrase \og Il mange une pomme de terre. \fg. Tous
    les n-grammes obtenus ne sont pas toujours des termes candidats valides, dans
    l'exemple \og Il mange une pomme de terre \fg\ seul le tri-gramme \og pomme de
    terre \fg\ est un terme candidat valide. Il est donc nécessaire de filtrer les
    n-grammes en utilisant soit une liste de mots outils, c'est à dire les mots
    ayant un rôle sémantique moins important que leur rôle syntaxique (tels que
    les mots de liaison, par exemple), soit en utilisant les parties du discours
    de chaque mots du n-gramme, c'est à dire leur catégorie grammaticale (noms,
    adjectifs, etc.). Une autre technique d'extraction de termes candidat est
    l'identification, dans le document, des chunks nominaux. Un chunk est une
    unité minimale de sens constituée d'un ou de plusieurs mots. Un chunk nominal
    est un chunk dont la tête est un nom ou un pronom. Par exemple, dans \og Il
    mange une pomme de terre \fg, \og Il \fg\ et \og une pomme de terre \fg\ sont
    des chunks nominaux.
    
    Dans une étude sur le bien fondé d'utiliser des traits linguistiques pour
    l'extraction automatique de termes-clés, \citet{hulth2003keywordextraction}
    compare trois méthodes d'extraction des termes candidats. L'une consiste à
    extraire les chunks nominaux, tandis que les deux autres
    extraient tous les n-grammes et les filtrent, soit pour retirer ceux contenant
    des mots outils dans le premier cas, soit pour ne retenir que ceux respectant
    certains patrons syntaxiques dans le second cas. Dans ses expériences,
    \citet{hulth2003keywordextraction} montre que l'extraction de termes-clés à
    partir de n-grammes filtrés avec les mots outils donne les meilleurs résultats
    parmi les trois méthodes qu'elle propose.

    Les travaux de \citet{hulth2003keywordextraction} sont évalués avec un corpus
    dont les documents sont des résumés d'articles scientifiques. Cependant, dans
    d'autres domaines tels que la bio-médecine, la nature des termes-clés à
    extraire n'est pas la même. En effet, ce sont les acronymes et les entités
    nommées (noms de protéines par exemple) qu'il est nécessaire d'extraire en
    tant que termes-clés \citep{nobata2008kleio}. Pour cela, l'extraction de
    termes candidats est spécifique au domaine d'application. Les méthodes
    d'extraction de termes-clés présentées dans ce document traitent des documents
    supposés sans spécificités particulières, les méthodes d'extraction de termes
    candidats sont donc les mêmes que celles expérimentées par
    \citet{hulth2003keywordextraction}, mais il est envisageable de les adapter à
    des domaines présentant des spécificités particulières.

    Utilisés avec les méthodes non-supervisées, les termes candidats sont ordonnés
    selon un score d'importance obtenu soit à partir d'eux-mêmes, soit à partir de
    l'importance des mots qui les composent. Si une méthode s'appuie uniquement
    sur les mots, alors le score d'un terme candidat est généralement calculé en
    faisant la somme des mots qui le composent. Cependant, ceci n'est pas toujours
    juste, c'est donc un inconvénient important des méthodes travaillant sur les
    mots pour extraire les termes-clés. En effet, la sommation peut privilégier
    des termes candidats qui contiennent beaucoup de mots non-importants vis-à-vis
    de ceux contenant très peu de mots, mais importants.
    
    Utilisés dans les méthodes supervisées, les termes candidats sont classés en
    tant que termes-clés ou non termes-clés grâce à des méthodes de
    classification.

    \subsubsection{Méthodes non-supervisées}
    \label{sec:unsupervised_methods}
      Les méthodes non-supervisées d'extraction de termes-clés ont la
      particularité de s'abstraire du domaine et de la langue des documents à
      analyser\footnote{L'abstraction de la langue est vraie pour ce qui est de la
      méthodologie, cependant les pré-traitements tels que la segmentation en
      phrases, en mots et l'étiquetage en parties du discours sont eux dépendants
      de la langue.}. Cette abstraction est due au fait que les termes candidats
      sont analysés avec des règles simples déduites à partir de traits
      statistiques issus seulement du texte analysé ou bien d'un corpus de
      référence non annoté.

      De nombreuses approches sont proposées. Certaines se fondent uniquement sur
      des statistiques alors que d'autres les combinent avec des représentations
      plus complexes des documents. Ces représentations peuvent aller de groupes
      de mots sémantiquement similaires à des graphes dont les n\oe{}uds sont des
      unités textuelles (mots, locutions, phrases, etc.) liées par des arcs qui
      représentent des liens de recommandation entre elles\footnote{Pour une étude
      comparative de certaines des méthodes par regroupement
      \citep{liu2009keycluster} et à base de graphe \citep{mihalcea2004textrank,
      wan2008expandrank}, voir l'article de \citet{hassan2010conundrums}.}.

      \paragraph{Les approches statistiques}
        cherchent à définir ce qu'est un terme-clé en s'appuyant sur certains
        traits statistiques et en étudiant leur rapport avec la notion
        d'importance d'un terme candidat. Plus un terme candidat est jugé
        important vis-à-vis du document analysé, plus celui-ci est pertinent en
        tant que terme-clé.

        TF-IDF (cf. équation \ref{math:tfidf}) de \citet{jones1972tfidf} et Likey
        (cf. équation \ref{math:likey}) de \citet{paukkeri2010likey} sont deux
        méthodes qui comparent le comportement d'un terme candidat dans le
        document analysé avec son comportement dans une collection de documents.
        L'objectif est de trouver ceux dont le comportement dans
        le document varie positivement comparé à leur comportement global dans la
        collection. Dans les deux méthodes ceci s'exprime par le fait qu'un terme
        candidat a une forte importance vis-à-vis du document analysé s'il y est
        très présent lorsqu'il ne l'est pas dans le reste de la collection.
        \begin{align}
          \text{TF-IDF}(\text{terme}) &= TF(\text{terme}) \times \log\left(\frac{N}{DF(\text{terme})}\right) \label{math:tfidf}\\
          \notag\\
          \text{Likey}(\text{terme}) &= \frac{\text{rang}_{\text{document}}(\text{terme})}{\text{rang}_{\text{corpus}}(\text{terme})} \label{math:likey}
        \end{align}\\
        Dans TF-IDF, $TF$ (\textit{Term Frequency}) représente le nombre
        d'occurrences d'un terme candidat dans le document analysé et $DF$
        (\textit{Document Frequency}) représente le nombre de documents dans
        lequel il est présent, $N$ étant le nombre total de documents. Plus le
        score TF-IDF d'un terme candidat est élevé, plus celui-ci est important
        dans le document analysé. Dans Likey, le rang d'un terme candidat dans le
        document et dans le corpus est obtenu à partir de son nombre
        d'occurrences, respectivement dans le document et dans la collection de
        documents. Plus le rapport entre ces deux rangs est faible, plus le terme
        candidat évalué est important dans le document analysé.

        Okapi (ou BM25) \citep{robertson1999okapi} est une mesure alternative à
        TF-IDF. En Recherche d'Information (RI), celle-ci est plus utilisée que le
        TF-IDF. Bien que l'extraction automatique de termes-clés soit une
        discipline à la frontière entre le TAL et la RI, la méthode de pondération
        Okapi n'a, à notre connaissance, pas été appliquée pour l'extraction de
        termes-clés. Dans l'article de \citet{claveau2012vectorisation}, Okapi est
        décrit comme un TF-IDF prenant mieux en compte la longueur des documents.
        Cette dernière est utilisée pour normaliser le $TF$ (qui devient
        $TF_{BM25}$) :
        \begin{align}
          \text{Okapi}(\text{terme}) &= TF_{BM25}(\text{terme}) \times \log\left(\frac{N - DF(\text{terme}) + 0,5}{DF(\text{terme}) + 0,5}\right) \label{math:okapi}\\
          \notag\\
          TF_{BM25} &= \frac{TF(\text{terme}) \times (k_1 + 1)}{TF(\text{terme}) + k_1 \times \left(1 - b + b \times \frac{DL}{DL_{\text{moyenne}}}\right)} \label{math:tf_bm25}
        \end{align}\\
        Dans la formule (\ref{math:tf_bm25}), $k_1$ et $b$ sont des constantes
        fixées à $2$ et $0,75$ respectivement. $DL$ représente la longueur du
        document analysé et $DL_{moyenne}$ la longueur moyenne des documents de la
        collection utilisée.

        \citet{barker2000nounphrasehead} estiment que les longues locutions sont
        plus informatives que les mots simples et qu'elles doivent être privilégiées.
        Pour cela, leur approche est très simple : plus un groupe nominal est long
        et fréquent dans le document analysé, plus il est jugé pertinent en tant
        que terme-clé de ce document. Cependant, pour éviter la répétition dans le
        texte, les auteurs des documents utilisent les mêmes locutions sous des
        formes alternatives (plus courtes, par exemple). La fréquence d'une locution
        ne reflète donc pas forcément sa fréquence réelle
        d'utilisation, car celle-ci est répartie dans les différentes
        alternatives. De ce fait, \citet{barker2000nounphrasehead} repèrent dans
        les groupes nominaux leur tête et utilisent en plus la fréquence de
        celle-ci.

        \citet{tomokiyo2003languagemodel} tentent de vérifier statistiquement deux
        propriétés que doit respecter un terme candidat pour être promu terme-clé.
        Les deux propriétés sont :
        \begin{itemize}
          \item{l'informativité : un terme-clé doit capturer au moins une des
                idées essentielles exprimées dans le document analysé;}
          \item{la grammaticalité : un terme-clé doit être bien formé
                syntaxiquement.}
        \end{itemize}
        Pour vérifier ces deux propriétés, trois modèles de langue sont utilisés.
        Un modèle uni-gramme $ML_{\text{document}}^1$ et un modèle n-gramme
        $ML_{\text{document}}^N$ sont construits à partir du document analysé et
        le dernier modèle $ML_{\text{référence}}^N$ est un modèle n-gramme
        construit à partir d'une collection de document. Le modèle obtenu à partir
        de la collection permet une vision globale (non spécifique à un
        domaine particulier) de la langue des documents analysés. De ce fait, plus
        la probabilité d'un terme candidat dans le modèle n-gramme du document
        diverge par rapport à sa probabilité dans le modèle de référence, plus il
        est informatif dans le document analysé (cf. équation
        \ref{math:informativeness}). \TODO{A revoir} De même, plus la probabilité d'un terme
        candidat dans le modèle n-gramme du document diverge par rapport à sa
        probabilité dans le modèle uni-gramme du document, plus il respecte la
        propriété de grammaticalité (cf. équation \ref{math:phraseness}). La
        divergence doit être exprimée en terme de coût, les auteurs utilisent donc
        la divergence Kullback-Leibler (cf. équation \ref{math:kullbackleibler}).
        \begin{align}
          \text{informativité}(\text{terme}) &= KL_{\text{terme}}(ML_{\text{analyse}}^{N} || ML_{\text{référence}}^{N}) \label{math:informativeness}\\
          \notag\\
          \text{grammaticalité}(\text{terme}) &= KL_{\text{terme}}(ML_{\text{analyse}}^{N} || ML_{\text{analyse}}^{1}) \label{math:phraseness}\\
          \notag\\
          KL_{\text{terme}}(ML || ML') &= ML(\text{terme}) \log \frac{ML(\text{terme})}{ML'(\text{terme})} \label{math:kullbackleibler}\\
          \notag\\
          ML^N(\text{terme} = m_1\ m_2\ \dots\ m_k) &= \prod_{i = 1}^k P(m_i | m_{i - (N - 1)} m_{i - ((N - 1) - 1)} \dots m_{i - 1}) \notag
        \end{align}\\
        Pour finir, les termes candidats sont ordonnés dans l'ordre décroissant de
        la somme des deux divergences et les meilleurs termes candidats pour être
        des termes-clés sont ceux les mieux classés.

        Tout comme \citet{tomokiyo2003languagemodel},
        \citet{ding2011binaryintegerprogramming} tentent de définir des propriétés
        visant à affiner l'extraction de termes-clés. Les propriétés qu'ils
        définissent sont des contraintes sur l'ensemble de termes-clés qui doit
        être extrait. Les contraintes sont les suivantes :
        \begin{itemize}
          \item{la couverture : un ensemble de termes-clés doit couvrir
                l'intégralité des sujets abordés dans le document
                représenté;}
          \item{la cohérence : les termes-clés de l'ensemble doivent être
                cohérents entre eux.}
        \end{itemize}
        La contrainte de couverture est calculée avec le modèle \textit{Latent
        Dirichlet Allocation} (LDA) \citep{blei2003lda} qui donne la probabilité
        d'un terme candidat sachant un sujet. Dans l'ensemble, la contrainte de
        cohérence est calculée pour chaque paire de termes-clés avec la mesure
        d'information mutuelle. Ces deux contraintes permettent de réduire le
        champs des possibilités, mais il reste encore à trouver quel ensemble
        parmi ceux possibles contient les meilleurs termes-clés. Pour cela, le
        degré d'importance des termes candidats est évalué grâce au TF-IDF ainsi
        qu'à des traits liés à la position et à la présence du terme candidat dans le titre
        (cf. équation \ref{math:binaryintegerprogramming}). Les auteurs cherchent
        ensuite la meilleure solution en utilisant une méthode d'optimisation
        (programmation par les entiers), avec pour objectif de trouver l'ensemble
        qui respecte les contraintes et dont l'importance des termes candidats
        qu'il contient est maximale. Cette méthode a la particularité de donner
        directement un ensemble de termes-clés\footnote{La taille de l'ensemble de
        termes-clés à extraire est elle aussi une contrainte.}.
        \begin{align}
          \text{importance}(\text{terme}) &= \alpha \times \text{TF-IDF}(\text{terme}) \notag\\
                            &+ \beta\ \text{poids\_est\_dans\_titre}(\text{terme}) \notag\\
                            &+ \gamma\ \text{poids\_est\_dans\_première\_phrase}(\text{terme}) \label{math:binaryintegerprogramming}\\
          \notag\\
          \alpha + \beta + \gamma &= 1 \notag
        \end{align}

        Les traits statistiques des méthodes précédentes sont uniquement utilisés
        pour déterminer un score de pertinence des termes candidats en tant que
        termes-clés. Une donnée statistique non citée précédemment, mais pourtant
        récurrente dans les méthodes d'extraction de termes-clés, est la fréquence
        de co-occurrences entre deux locutions. Deux locutions
        co-occurrent si elles apparaissent ensemble dans le même contexte. La
        co-occurrence peut être calculée de manière stricte (les locutions doivent
        être côte-à-côte) ou bien dans une fenêtre de mots. Compter le nombre de
        co-occurrences entre deux locutions permet d'estimer s'ils sont
        sémantiquement liés ou non. Ce lien sémantique à lui seul ne peut pas
        servir à extraire des termes-clés, mais il permet de mieux organiser les
        locutions d'un document pour affiner l'extraction
        \citep{matsuo2004wordcooccurrence, liu2009keycluster,
        mihalcea2004textrank}.

      \paragraph{Les approches par regroupement}
        définissent des groupes dont les unités textuelles partagent une ou
        plusieurs caractéristiques communes (similarité lexicale, similarité sémantique,
        etc.). Ainsi, lorsque des termes-clés sont extraits à partir de chaque
        groupe, cela permet de mieux couvrir le document analysé en fonction des
        caractéristiques choisies.

        Dans la méthode de \citet{matsuo2004wordcooccurrence}, ce sont les
        locutions qui sont regroupées. Parmi celles-ci, seules les plus fréquentes
        sont concernées par le regroupement. Ce dernier s'effectue en fonction du
        lien sémantique entre les locutions, ici déterminé avec leur nombre de
        co-occurrences. Après le regroupement, la méthode consiste à comparer les
        termes candidats du document analysé avec les groupes $g$ de locutions
        fréquentes, en faisant l'hypothèse qu'un terme candidat qui co-occurre plus
        que selon toute probabilité avec les locutions fréquentes d'un ou plusieurs
        groupes est plus vraisemblablement un terme-clé. La mesure $\chi^2$ permet
        de vérifier cette hypothèse, en calculant le biais entre la fréquence de
        co-occurrence attendue et la fréquence de co-occurrence réelle :
        \begin{align}
          \chi^2(\text{terme}) = \sum_{g} \frac{(\text{fréquence}(\text{terme}, g) - n_tp_g)^2}{n_tp_g}
        \end{align}
        $n_tp_g$ représente la fréquence de co-occurrence attendue entre le terme candidat
        et le groupe $g$, $n_t$ étant le nombre de locutions avec lesquelless le terme candidat
        analysé co-occurre et $p_g$ étant la probabilité d'occurrence du groupe
        $g$ avec d'autres locutions\footnote{La probabilité des groupes de locutions
        fréquentes est normalisée pour que la somme des probabilités des groupes
        soit égale à $1$.}. Lors de leurs expériences, les auteurs se sont aperçus
        que certaines locutions peuvent être sémantiquement liées à une locution fréquente
        dans un domaine plus général que celui du document analysé. En supposant
        que ces cas spéciaux soient ceux ayant le plus fort biais, ils décident de
        supprimer du $\chi^2$ l'argument maximum de la sommation :
        \begin{align}
          \chi^2{'}(\text{terme}) = \chi^2 - \max_{g}\left\{\frac{(\text{fréquence}(\text{terme}, g) - n_tp_g)^2}{n_tp_g}\right\}
        \end{align}
        Les termes-clés extraits sont les termes candidats ayant le plus fort
        biais mesuré avec la mesure $\chi^2{'}$.

        Dans l'algorithme KeyCluster,  \citet{liu2009keycluster} utilisent aussi
        un regroupement sémantique, mais dans leur cas ils considèrent tous les
        mots du document analysé et ils excluent les mots outils. Dans chaque
        groupe sémantique, le mot qui est le plus central est sélectionné comme
        mot de référence. L'ensemble des mots de référence est ensuite utilisé
        pour filtrer les termes candidats en ne considérant comme termes-clés que
        ceux qui contiennent au moins un mot de référence (tous les mots de
        référence devant être utilisés dans au moins un terme-clé). Cette méthode
        présente l'avantage d'offrir une bonne couverture des sujets abordés dans
        un document, car tous les groupes sémantiques sont représentés par au
        moins un terme-clé. Cependant les termes candidats ne sont pas pondérés.
        Il n'est alors pas possible de définir un classement de ceux-ci dans le
        but de n'en extraire qu'un sous ensemble\footnote{Il est toutefois
        envisageable de définir un système de pondération basé par exemple sur le
        nombre de mots de références contenus dans le terme candidat, en utilisant le
        nombre de mots du groupe auquel appartiennent les mots de référence du
        terme candidat, etc.}.

      \paragraph{Les approches à base de graphe}
        sont actuellement les plus populaires. Elles consistent à représenter le
        contenu d'un document sous la forme d'un graphe. La méthodologie appliquée
        est issue de PageRank \citep{brin1998pagerank}, un algorithme
        d'ordonnancement de pages Web (n\oe{}uds du graphe) grâce aux liens de
        recommandation qui existent entre elles (arcs du graphe). TextRank
        \citep{mihalcea2004textrank} et SingleRank \citep{wan2008expandrank} sont
        les deux adaptations de base de PageRank pour l'extraction automatique de
        termes-clés\footnote{Dans l'article de \citet{mihalcea2004textrank},
        TextRank est aussi appliqué pour faire du résumé automatique.}. Dans
        celles-ci, les pages Web sont remplacées par des unités textuelles dont la
        granularité est le mot et un arc est créé entre deux n\oe{}uds si les mots
        qu'ils représentent co-occurrent dans une fenêtre de mots donnée.
      
        Le graphe est noté $G = (N, A)$, où $N$ est l'ensemble des n\oe{}uds du
        graphe et où $A$ est l'ensemble de ses arcs entrants et sortant :
        $A_{\text{entrant}} \cup A_{\text{sortant}}$\footnote{Dans le cas de
        TextRank et de SingleRank\ $A_{\text{entrant}} = A_{\text{sortant}}$, car
        le graphe n'est pas orienté.}. Pour chaque n\oe{}ud du graphe, un score
        est calculé par un processus itératif destiné à simuler la notion de
        recommandation d'une unité textuelle par d'autres\footnote{Plus le score
        d'une unité textuelle est élevé, plus celle-ci est importante dans le
        document analysé.} (cf. équation \ref{math:textrank}). Ce score, calculé
        pour chaque n\oe{}ud $n_i$, permet d'ordonner les mots par degré
        d'importance dans le document analysé. La liste ordonnée des mots peut
        ensuite être utilisée pour extraire les termes-clés.
        \begin{align}
          S(n_i) &= (1 - \lambda) + \lambda \times \sum_{n_j \in A_{\text{entrant}}(n_i)} \frac{p_{j, i} \times S(n_j)}{\mathlarger{\sum}_{n_k \in A_{\text{sortant}}(n_j)} p_{j, k}} \label{math:textrank}
        \end{align}
        $\lambda$ est un facteur d'atténuation qui peut être considéré ici comme
        la probabilité pour que le n\oe{}ud $n_i$ soit atteint par recommandation.
        $p_{j, i}$ représente le poids de l'arc allant du n\oe{}ud $n_j$ vers le
        n\oe{}ud $n_i$, soit le nombre de co-occurrences entre les deux mots $i$
        et $j$\footnote{TextRank utilise un graphe non-pondéré. Dans ce cas,
        $p_{j, i}$ vaut toujours $1$.}.

        Dans leurs travaux, \citet{wan2008expandrank} s'intéressent à l'ajout
        d'informations dans le graphe grâce à des documents similaires (voisins)
        et aux relations de co-occurrences qu'ils possèdent. Cette méthode,
        appelée ExpandRank, a pour objectif de faire mieux ressortir les mots
        importants du graphe en ajoutant de nouveaux liens de recommandation ou
        bien en renforçant ceux qui existent déjà. Les termes-clés extraits sont
        uniquement issus du document $d$ analysé, mais leur extraction est affiné
        grâce à l'usage de document similaires, documents voisins $V_d$.
        L'usage de documents voisins peut cependant ajouter ou renforcer des liens
        qui ne devraient pas l'être. Pour éviter cela, les auteurs réduisent
        l'impact des documents voisins en utilisant leur degré de similarité avec
        le document analysé. Ce degré de similarité est utilisé lors de la
        pondération des arcs :
        \begin{align}
          p_{j, i}^d &= \sum_{v \in V_d \cup \{d\}} \text{similarité}(d, v) \times p_{j, i}^v
        \end{align}
        La méthode CollabRank, également proposée par \citet{wan2008collabrank},
        est une alternative à ExpandRank. Elle fonctionne de la même manière, mais
        certains choix des auteurs rendent impossible l'usage du degré de
        similarité pour réduire l'impact des documents voisins. En effet, dans
        CollabRank les documents sont regroupés avant tout traitement et pour
        chaque groupe, le graphe n'est pas construit à partir d'un document donné
        (le document à analyser, dans la méthode ExpandRank), mais à partir de
        tous les documents du groupe. Les résultats moins concluants de CollabRank
        semblent confirmer la pertinence de l'usage du degré de similarité pour
        réduire les effets de bruit.

        Dans l'optique d'améliorer encore TextRank/SingleRank,
        \citet{liu2010topicalpagerank} proposent une méthode qui cherche cette
        fois-ci à augmenter la couverture de l'ensemble des termes-clés extraits
        dans le document analysé (TopicalPageRank). Pour ce faire, ils tentent
        d'affiner le rang d'importance des mots dans le document en tenant compte
        de leur rang dans chaque sujet abordé. Le rang d'un mot pour un sujet est
        obtenu en intégrant à son score PageRank la probabilité qu'il appartienne
        au sujet (cf. équation \ref{math:topicalpagerank}). Cette probabilité est
        obtenue avec le modèle LDA \citep{blei2003lda}. Les termes candidats
        sont ensuite ordonnés, selon l'ordre décroissant d'un score calculé à
        partir des rangs, dans chaque sujet, des mots d'un terme candidat donné
        (cf. équation \ref{math:topicalpagerankfinalscore}).
        \begin{align}
          S_{\text{sujet}}(n_i) &= (1 - \lambda) \times p(\text{sujet} | i) + \lambda \times \sum_{n_j \in A_{\text{entrant}}(n_i)} \frac{p_{j, i} \times S(n_j)}{\mathlarger{\sum}_{N_k \in A_{sortant}(N_j)} p_{j, k}} \label{math:topicalpagerank}\\
          Score(\text{terme}) &= \mathlarger{\sum}_{\text{sujet}} p(\text{sujet} | \text{document}) \times \sum_{m \in \text{terme}} \text{rang}_{\text{sujet}}(m) \label{math:topicalpagerankfinalscore}
        \end{align}

        Les approches à base de graphe présentées ci-dessus effectuent toutes un
        ordonnancement des mots du document analysé selon leur importance dans
        celui-ci. Pour extraire les termes-clés il est ensuite nécessaire
        d'effectuer du travail supplémentaire à partir de la liste ordonnée de
        mots. Dans la méthode TextRank, les $k$ mots les plus importants sont
        sélectionnés et retournés (après que ceux apparaissant en collocation dans
        le document aient été concaténés). La technique utilisée dans les autres
        méthodes consiste à ordonner les termes candidats en fonction de la somme
        du score des mots qui les composent. Cependant, puisque l'un des avantages
        du graphe est que les n\oe{}uds peuvent avoir une granularité contrôlée,
        \citet{liang2009querylog} décident d'utiliser les termes candidats au lieu des
        mots simples et de tirer profit de traits supplémentaires. Ces traits sont
        la taille des termes candidats et leur première position dans le document
        analysé. Ils sont appliqués lors de la pondération des arcs :
        \begin{align}
          p'_{j, i} &= (\text{taille}(j) + \text{position}(j)) \times p_{j, i}
        \end{align}
        Selon cette formule, $\text{position}(j)$ ne semble pas correspondre à
        la position de $j$, mais plutôt à l'inverse de sa position. Ainsi,
        l'importance est donnée aux termes candidats situés au début du document analysé.

        \citet{tsatsaronis2010semanticrank} travaillent aussi directement sur le
        calcul d'un score pour les termes candidats. Dans leur approche, ils modifient les
        relations utilisées entre les n\oe{}uds du graphe (cf. équation
        \ref{math:semanticrank}), ainsi que le calcul du score pour chacun d'eux.
        La relation qui lie deux n\oe{}uds est toujours une relation sémantique,
        mais celle-ci est déterminée de manière plus précise. Pour cela, les
        ressources externes WordNet \citep{fellbaum2010wordnet} et Wikipédia sont
        utilisées. WordNet est une base de données lexicales qui fournit un
        vecteur de synonymes pour les noms, les verbes, les adverbes et les
        adjectifs. Le vecteur de synonymes est ici considéré comme l'ensemble de
        tous les sens possibles pour une unité lexicale, c'est son vecteur
        sémantique. À partir des vecteurs sémantiques, toutes les paires
        sémantiques $P_{i, j}$ possible entre deux termes candidats $t_i$ et $t_j$ (un sens
        de $t_i$ avec un sens de $t_j$) ainsi que tous les chemins $C_{i, j}$
        possible pour atteindre un sens de $t_j$ à partir d'un sens de $t_i$
        (selon les données de WordNet) sont construits. Le score de similarité
        sémantique avec WordNet est obtenu en trouvant le couple paire
        sémantique/chemin sémantique pour lequel le produit des mesures
        sémantiques \textit{Semantic Compactness} (SCM) et \textit{Semantic Path
        Elaboration} (SPE), introduites par
        \citet{tsatsaronis2010textrelatedness}, est le plus élevé (cf. équation
        \ref{math:wordnetsemanticrelatedness}). Dans le cas où l'un des termes candidats
        n'est pas présent dans WordNet, la similarité sémantique est calculée avec
        les données de Wikipédia. Cette similarité est défini par
        \citet{milne2008wikipediasemanticrelatedness} (cf. équation
        \ref{math:wikipediasemanticrelatedness}). Pour cette similarité,
        l'intuition est que plus deux locutions sont utilisées dans les mêmes
        articles, plus elles sont liées sémantiquement.
        \begin{align}
          p_{j, i} &= \left\{\begin{array}{ll}
            1 & \text{si $t_i = t_j$}\\
            \text{Sim}_{WN} & \text{sinon, si $t_i, t_j \in \text{WordNet}$}\\
             \text{Sim}_{W} &  \text{sinon, si $t_i, t_j \in \text{Wikipedia}$}\\
            0 & \text{sinon}
          \end{array}\right. \label{math:semanticrank}\\
          \notag\\
          \text{Sim}_{WN}(t_i, t_j) &= \max_{p \in P_{i, j}}\left\{\max_{c \in C_{i, j}}\left\{SCM(p, c) \times SPE(p, c)\right\}\right\} \label{math:wordnetsemanticrelatedness}\\
          \notag\\
          \text{Sim}_{W}(t_i, t_j) &= \frac{\log(\max(|w_i|, |w_j)) - \log(|w_i \cup w_j|)}{\log(|\text{Wikipedia}|) - \log(\min(|w_i|, |w_j|))} \label{math:wikipediasemanticrelatedness}\\
          \notag\\
          w_n &= \left\{\text{article} \in \text{Wikipedia} | t_n \in \text{article}\right\} \notag
        \end{align}
        En ce qui concerne le calcul du score de chaque n\oe{}ud, les auteurs
        expérimentent leur méthode avec la formule classique de PageRank
        \citep{brin1998pagerank} plus deux autres variantes et la formule HITS
        \citep{kleinberg1999hits}. L'une des deux variantes de PageRank utilise le
        TF-IDF comme trait supplémentaire (cf. équation \ref{math:apw}) et l'autre
        utilise un facteur $\lambda_i$ à la place du facteur $\lambda$ (cf.
        équation \ref{math:ppr}). Ce nouveau facteur est spécifique au terme
        $t_i$. Aucun détail précis n'indique comment $\lambda_i$ est déterminé,
        mais sa valeur est liée à la présence ou non de $t_i$ dans le titre du
        document analysé. Le score calculé avec HITS repose sur les notions
        d'autorité et de centralité. Un n\oe{}ud du graphe a de l'autorité si ses
        arcs entrants proviennent de n\oe{}uds centraux (cf. équation
        \ref{math:hitsauthority}). De même, un n\oe{}ud est central s'il atteint
        des n\oe{}uds de forte autorité (cf. équation \ref{math:hitshub}). Il
        s'agit là d'un renforcement mutuel. Le score retenu pour l'ordonnancement
        des termes candidats est le score d'autorité.
        \begin{align}
          S_{\text{TF-IDF}}(n_i) &= \frac{1}{2} \times \left(\frac{S(n_i)}{\mathlarger{\max}_{n_j \in N}(S(n_j))} + \frac{\text{TF-IDF}(t_i)}{\mathlarger{\max}_{n_j \in N}(\text{TF-IDF}(t_j))}\right) \label{math:apw}\\
          \notag\\
          S_{\lambda}(n_i) &= (1 - \lambda_i) + \lambda_i \times \sum_{n_j \in A_{\text{entrant}}(n_i)} \frac{p_{j, i} \times S_{\lambda}(n_j)}{\mathlarger{\sum}_{n_k \in A_{\text{sortant}}(n_j)} p_{j, k}} \label{math:ppr}\\
          \notag\\
          \text{autorité}(n_i) &= \sum_{n_j \in A_{\text{entrant}}(n_i)}p_{j, i} \times \text{centralité}(n_j) \label{math:hitsauthority}\\
          \notag\\
          \text{centralité}(n_i) &= \sum_{n_j \in A_{\text{sortant}}(n_i)}p_{i, j} \times \text{autorité}(n_j) \label{math:hitshub}
        \end{align}
        Dans les expériences menés par les auteurs, le score PageRank et ses
        variantes permettent d'obtenir de meilleurs résultats que la formule HITS.
        Les deux variantes $S_{\text{TF-IDF}}$ et $S_{\lambda}$ de PageRank
        permettent d'obtenir de meilleurs résultats que l'originale,
        $S_{\lambda}$ étant meilleur.

    \subsubsection{Méthodes supervisées}
    \label{sec:supervised_methods}
      Les méthodes supervisées sont des méthodes capables d'apprendre à réaliser
      une tâche particulière, soit ici l'extraction de termes-clés.
      L'apprentissage se fait grâce à un corpus dont les documents sont annotés en
      termes-clés. L'annotation permet d'extraire les exemples et les
      contres-exemples dont les traits statistiques et/ou linguistiques servent à
      apprendre une classification binaire. La classification binaire consiste à
      indiquer si un terme candidat est un terme-clé ou non.

      De nombreux algorithmes d'apprentissage sont utilisés dans divers domaines.
      Ils peuvent potentiellement s'adapter à n'importe quelle tâche, dont celle
      de l'extraction automatique de termes-clés. Les algorithmes les plus
      couramment utilisés pour l'extraction automatique de termes-clés
      construisent des modèles probabilistes, des arbres de décision, des
      Séparateurs à Vaste Marge (SVM) ou encore des réseaux de
      neurones\footnote{\citet{sarkar2012machinelearningcomparison} proposent une
      étude comparative de l'usage des arbres de décision, de la classification
      naïve bayésienne et des réseaux de neurones pour l'extraction automatique de
      termes-clés.}.

      \paragraph{Les modèles probabilistes}
        apprennent des distributions de probabilités pour qu'un terme candidat
        soit un terme-clé en fonction de divers traits (un seul trait par
        distribution probabiliste). Les distributions sont ensuite combinées pour
        déterminer une probabilité globale du terme candidat. Celle-ci n'est autre
        qu'un score de vraisemblance permettant de classer un terme candidat parmi
        les termes-clés ou non.

        KEA \citep{witten1999kea} est une méthode qui utilise une classification
        naïve bayésienne pour attribuer un score de vraisemblance à chaque terme
        candidat, le but étant d'indiquer s'ils sont des termes-clés ou
        non\footnote{Il est important de noter que le score de vraisemblance pour
        chaque terme candidat permet aussi de les ordonner entre eux.}.
        \citet{witten1999kea} utilisent trois distributions conditionnelles
        apprises à partir du corpus d'apprentissage. La première correspond à la
        probabilité pour que chaque terme candidat soit étiqueté \textit{oui}
        (terme-clé) ou \textit{non} (non terme-clé). Les deux autres correspondent
        à deux différents traits qui sont le poids TF-IDF du terme candidat et sa
        première position dans le document :
        \begin{align}
          P(\text{terme}) &= \frac{P_{\text{oui}}(\text{terme})}{P_{\text{oui}}(\text{terme}) + P_{\text{non}}(\text{terme})} \label{math:kea}\\
          \notag\\
          P_{\text{oui}}(\text{terme}) &= P(\text{terme} | \text{oui}) \times \prod_{\text{trait} \in \{\text{TF-IDF}, \text{position}\}} P_{\text{trait}}\left(\text{trait}(\text{terme}) | \text{oui}\right) \notag\\
          \notag\\
          P_{\text{non}}(\text{terme}) &= P(\text{terme} | \text{non}) \times \prod_{\text{trait} \in \{\text{TF-IDF}, \text{position}\}} P_{\text{trait}}\left(\text{trait}(\text{terme}) | \text{non}\right) \notag
        \end{align}
        L'un des avantages de la classification naïve bayésienne est que chaque
        distribution est supposée indépendante. L'ajout de nouveaux traits dans la
        méthode KEA est donc très aisé.
        
        Parmi les variantes de KEA proposées, \citet{frank1999keafrequency}
        ajoutent un troisième trait : le nombre de fois que le terme candidat est
        un terme-clé dans le corpus d'apprentissage. L'ajout de ce trait permet
        d'améliorer les performances de la version originale de KEA, mais
        uniquement lorsque la quantité de données d'apprentissage est très
        importante.
        
        Une autre amélioration de KEA, proposée par
        \citet{turney2003keacoherence}, tente d'augmenter la cohérence entre les
        termes candidats les mieux classés. Pour ce faire, une première étape de
        classification est effectuée avec la méthode originale. Celle-ci fournit
        un premier classement des termes candidats selon leur score de
        vraisemblance. Ensuite, de nouveaux traits sont ajoutés et une nouvelle
        étape de classification est lancée. Les nouveaux traits ont pour but
        d'augmenter le score de vraisemblance des termes candidats ayant un fort
        lien sémantique avec certains des termes candidats les mieux classés après la
        première étape. Pour cela, deux scores de similarité sémantique sont
        calculés entre les termes candidats classés parmi les $L$ premiers et les
        termes candidats classés parmi les $K$ ($K < L$) premiers\footnote{Du fait
        du calcul de deux scores de similarité des $L$ meilleurs termes candidats
        avec les $K$ ($k < L$) meilleurs, il y a $2K$ traits qui sont ajoutés.}.
        Les deux scores de similarité sont obtenus en comptant les pages Web pour
        lesquelles le terme candidat parmi les $L$ meilleurs et le terme candidat parmi les
        $K$ meilleurs apparaissent ensemble dans le titre (pour le premier score)
        ou dans le corps (pour le second score).

        \citet{nguyen2007keadocumentstructure} s'intéressent aux articles
        scientifiques et proposent aussi de modifier KEA en ajoutant cette fois-ci
        des informations concernant la structure des documents. En effet,
        certaines sections telles que l'introduction et la conclusion dans les
        articles scientifiques sont plus susceptibles de contenir des termes-clés
        qu'une section présentant des résultats expérimentaux, par exemple. Pour
        ce faire, ils utilisent un vecteur d'occurrences des termes candidats dans les
        sections typique d'un article scientifique. Dans leur version modifiée de
        KEA, ils proposent aussi l'usage de traits linguistiques. Leur classifieur
        utilise l'étiquetage en parties du discours et les suffixes des mots du
        terme candidat comme traits supplémentaires. Notez que le choix d'utiliser les
        suffixes est justifié pour l'anglais, mais peut ne pas l'être pour toutes
        les langues. En effet, pour l'anglais, les auteurs remarques que les
        suffixes des lemmes\footnote{Un lemme est une unité lexicale. Elle ne
        présente aucune flexion. Ainsi \og avion\fg\ est le lemme de \og avions
        \fg\ et \og voler \fg\ est le lemme de \og volait \fg. Ce sont les lemmes
        qui se trouvent dans un dictionnaire.} et de leurs
        modificateurs\footnote{Un modificateur donne plus de précision sur le
        nom, le verbe ou la locution auxquels il se rapporte.} ne sont en règle
        générale pas les mêmes. Selon les auteurs, les lemmes peuvent être
        distingués par leur suffixes \textit{-ion}, \textit{-ics} et
        \textit{-ment} tandis que les suffixes \textit{-ive}, \textit{-al} et
        \textit{-ic} sont plus fréquemment utilisés dans les modificateurs.

        La même année que les travaux de \citet{hulth2003keywordextraction} sur
        le bien fondé d'utiliser des traits linguistiques pour l'extraction
        automatique de termes-clés, \citet{sujian2003maximumentropy} proposent une
        méthode utilisant un modèle d'entropie maximale (cf. équation
        \ref{math:maximum_entropy}) dont l'un des traits repose sur les parties du
        discours des mots qui composent les termes candidats. Un modèle de maximum
        d'entropie consiste à trouver parmi plusieurs distributions (une pour
        chaque trait), laquelle a la plus forte entropie (cf. équation
        \ref{math:entropy}). La distribution ayant la plus forte entropie est par
        définition celle qui contient le moins d'information, ce qui la rend de ce
        fait moins arbitraire et donc plus appropriée pour l'extraction
        automatique de termes-clés.
        \begin{align}
          \text{Score}(\text{terme}) &= \frac{P(\text{oui} | \text{terme})}{P(\text{non} | \text{terme})} \label{math:maximum_entropy}\\
          \notag\\
          P(\text{classe} | \text{terme}) &= \frac{\exp\left(\mathlarger{\sum}_{\text{trait}} \alpha_{\text{trait}} \times \text{trait}(\text{terme}, \text{classe})\right)}{\mathlarger{\sum}_{c \in \{\text{oui}, \text{non}\}} \exp\left(\mathlarger{\sum}_{\text{trait}} \alpha_{\text{trait}} \times \text{trait}(\text{terme}, c)\right)} \label{math:entropy}
        \end{align}
        Le paramètre $\alpha_{\text{trait}}$ définit l'importance du
        $\text{trait}$ auquel il est associé.

        Dans leurs travaux, \citet{liu2011vocabularygap} proposent une méthode
        d'extraction de termes-clés utilisant aussi un modèle probabiliste. Leur
        méthode est très différente de celle de \citet{witten1999kea} et de
        \citet{sujian2003maximumentropy} puisqu'ils décident d'utiliser une
        approche de traduction automatique. L'usage original de cette approche est
        justifié par le fait qu'un ensemble de termes-clés doit décrire de manière
        synthétique le document. Leur hypothèse est donc qu'un ensemble de
        termes-clés est une traduction d'un document dans une autre langue, plus
        synthétique. Le modèle est appris à partir de paires de traductions dont
        l'une des locutions est issue des titres ou des résumés des documents du corpus
        d'apprentissage et dont l'autre locution est issue des corps de ces mêmes
        documents. Les titres et les résumés sont utilisés comme langue
        synthétique et les corps des documents comme le langage naturel. Le modèle
        appris tient compte de la probabilité d'avoir une locution synthétique $t_s$
        (terme candidat) sachant que le document contient la locution $t_d$ :
          \begin{align}
            P(t_d, t_s) &= \left(\frac{\lambda}{P(t_d | t_s)} + \frac{1 - \lambda}{P(t_s | t_d}\right)^{-1}
          \end{align}
        Il s'agit d'une moyenne harmonique paramétrée par le facteur $\lambda$.
        Elle sert à ordonner les termes candidats selon leur pertinence en tant
        que terme-clé.

      \paragraph{Les arbres de décision}
        sont des classifieurs dont les branches représentent des tests sur des
        traits des termes candidats. Ces tests permettent de router les termes
        candidats vers les feuilles de l'arbre représentant leurs classes
        respectives (terme-clé ou non).

        Dans son article sur l'apprentissage pour l'extraction automatique de
        termes-clés, \citet{turney1999learningalgorithms} présente une méthode qui
        utilise de nombreux traits pour entraîner $50$ arbres de décision C4.5.
        L'usage de plusieurs arbres de décision est une technique appelée
        \textit{Random Forest}. Grâce à cette technique, l'extraction automatique
        de termes-clés est réduite à un vote de chaque arbre pour chaque terme
        candidat et cela permet un classement des termes candidats en fonction de
        leur nombre de votes positifs. Les termes-clés extraits correspondent aux
        termes candidats les mieux classés.

        \TODO{Ne pas parler de forêt (\textit{Random Forest})} Tout comme \citet{turney1999learningalgorithms},
        \citet{ercan2007lexicalchains} utilisent eux aussi une forêt d'arbres C4.5
        dans leur méthode d'extraction de termes-clés. Ils utilisent
        principalement des traits classiques, mais leur contribution se situe au
        niveau de l'utilisation d'un trait calculé à partir de chaînes lexicales.
        Une chaîne lexicale lie les mots d'un document selon certaines relations
        telles que la synonymie, l'hyponymie ou la méronymie. Ces différentes
        relations ont un poids en fonction de leur importance et un score pour
        chaque terme candidat est calculé en faisant la somme des poids des
        relations présentes dans la chaîne lexicale. Cette approche est
        intéressante, mais du fait de la faible performance des méthodes de
        construction de chaînes lexicales elle présente l'inconvénient de ne
        retourner que des mots. Ce problème peut toutefois être
        pallier grâce à la forêt d'arbres C4.5 qui permet un classement des mots à
        partir de leur nombre de votes positifs. Il est en effet envisageable de
        déduire les termes-clés à partir de la liste ordonnée et pondérée des mots
        clés (voir les méthodes non-supervisées à base de graphe -- section
        \ref{sec:unsupervised_methods}).

      \paragraph{Les séparateurs à Vastes Marges (SVM)}
        sont aussi des classifieurs utilisés par les méthodes d'extraction
        automatique de termes-clés. Ils exploitent divers traits afin de projeter
        des exemples et des contres-exemples sur un plan, puis ils cherchent
        l'hyperplan qui les sépare. Cet hyperplan sert ensuite dans l'analyse de
        nouvelles données. Dans le contexte de l'extraction de termes-clés, les
        exemples sont les termes-clés et les contres-exemples sont les termes
        candidats qui ne sont pas des termes-clés. Ce mode de fonctionnement des
        SVM est utilisé par \citet{zhang2006svm}, mais un autre type de SVM est
        plus largement utilisé dans les méthodes supervisées d'extraction de
        termes-clés. Il s'agit de SVM qui utilisent de multiples marges
        représentant des rangs. Ces classifieurs permettent donc d'ordonnancer les
        termes-clés lors de leur extraction \citep{herbrich1999svm,
        joachims2006linearsvm, jiang2009rankingsvm}. La méthode KeyWE de
        \citet{eichler2010keywe} utilise ce type de SVM avec le trait TF-IDF ainsi
        qu'un trait booléen ayant la valeur vraie si le terme candidat apparaît
        dans un titre d'un article Wikipedia (un terme candidat apparaissant dans
        le titre d'un article de Wikipedia est considéré comme ayant une plus
        forte probabilité d'être un terme-clé). L'ordonnancement des termes
        candidats par le SVM permet ensuite de contrôler le nombre de termes-clés
        à extraire (choix des $k$ termes candidats les mieux classés).

      \paragraph{GenEx}
        est un algorithme génétique mis au point par
        \citet{turney1999learningalgorithms}. Il est constitué de deux composants.
        Le premier composant, le géniteur, sert à apprendre des paramètres lors de
        la phase d'apprentissage. Ces paramètres sont utilisés par le second
        composant, l'extracteur, pour donner un score d'importance à chaque terme
        candidat. Plus les paramètres sont optimaux, meilleure est la
        classification des termes candidats. De ce fait, le géniteur cherche leurs valeurs
        optimales en les représentant sous la forme de bits qui constituent une
        population d'individus qu'il fait évoluer jusqu'à obtenir un état stable,
        optimal. Les traits utilisés par l'extracteur concernent la longueur et la
        première position dans le document des termes candidats et des mots qui
        les composent. Les paramètres sont en partie des seuils définis pour les
        valeurs de ces traits. Ils servent à déterminer des facteurs
        multiplicateurs qui sont d'autres paramètres. Ces derniers agissent sur le
        calcul du score d'un terme candidat.

      \paragraph{Les perceptrons multicouches}
        peuvent aussi être utilisés pour l'extraction automatique de termes-clés
        \citep{sarkar2010neuralnetwork}. Un perceptron multi-couches est un réseau
        de neurones constitué d'au moins trois couches, chaque couche étant
        composée de neurones. Dans les deux couches extrêmes les neurones
        représentent respectivement les entrées et les sorties. Les couches
        centrales sont des couches cachées qui permettent d'acheminer les valeurs
        des entrées vers les sorties, où de nouvelles valeurs sont obtenues grâce
        à la pondération des transitions d'un neurone d'une couche vers un neurone
        de la couche suivante. Les entrées correspondent aux traits d'un terme
        candidat (ici TF-IDF, la position, la taille, etc.) et les sorties
        représentent les classes qu'il peut prendre (ici, terme-clé ou non). La
        valeur obtenue pour chaque sortie (classe) permet d'obtenir une
        probabilité pour que le terme candidat analysé soit un terme-clé ou non.
        Dans leur méthode, \citet{sarkar2010neuralnetwork} utilisent cette
        probabilité pour ordonner les termes candidats afin de pouvoir extraire
        les meilleurs, lorsqu'un nombre précis de termes-clés doit être extrait.

  \subsection{Les mesures d'évaluation}
  \label{sec:evaluation}
    Lorsqu'une nouvelle méthode est proposée, il est nécessaire d'en évaluer les
    performances afin de pouvoir la situer vis-à-vis de celles qui existent déjà.
    Plusieurs mesures d'évaluation des systèmes d'extraction de termes-clés sont
    utilisées, dans un processus d'évaluation \og à la Cranfield \fg
    \ \citep{voorhees2002philosophy}. Dans ce processus, les évaluations sont
    réalisées sur les documents d'une collection de test dont les termes-clés à
    extraire sont connus (jugement de référence) et la moyenne des mesures
    obtenues pour chaque document est calculée. Le problème de ce paradigme est
    qu'il suppose que le jugement de référence est la réponse exacte. Il ne peut
    donc pas y avoir de réponse différente mais équivalente (avec par exemple, des
    synonymes dans les termes-clés) et de ce fait de nombreuses mesures
    d'évaluation sont pessimistes.

    En TAL, il est courant d'évaluer une méthode en termes de \textbf{précision}
    et de \textbf{rappel}. Ces deux mesures sont déterminées à partir des
    termes-clés de référence, des termes-clés extraits, des termes-clés extraits
    qui sont effectivement des termes-clés (vrais positifs) et des termes-clés
    extraits qui n'en sont pas réellement (faux positifs -- cf. tableau
    \ref{tab:confusionmatrix}). La précision s'intéresse à la quantité de vrais
    positifs parmi tous ceux qui sont extraits (cf. équation
    \ref{math:precision}), alors que le rappel s'intéresse à la quantité de vrais
    positifs parmi tous ceux attendus (cf. équation \ref{math:recall}). Suivant
    les systèmes, il peut être nécessaire de favoriser l'une ou l'autre de ces
    deux mesures. La précision doit être privilégiée lorsqu'il faut minimiser les
    erreurs (faux positifs), tandis que le rappel doit être maximisé lorsque l'on
    veut augmenter le nombre de vrais positifs sans se soucier du nombre de faux
    positifs. Il existe aussi des cas où il est souhaitable d'obtenir un compromis
    (avoir un maximum de vrais positifs lorsque le nombre de faux positifs est
    minimal). Ce compromis se calcule avec la \textbf{f-mesure} (cf. équation
    \ref{math:fmeasure}) qui peut être ajustée avec le paramètre $\beta$, agissant
    sur l'importance du rappel par rapport à la précision.
    \begin{table}
      \begin{center}
        \begin{tabular}{|c|c|c|c|}
          \cline{3-4}
          \multicolumn{2}{c|}{} & \multicolumn{2}{|c|}{\textbf{Référence}}\\
          \cline{3-4}
          \multicolumn{2}{c|}{} & Terme-clé & Non terme-clé\\
          \hline
          \multirow{2}{*}{\textbf{Système}} & Terme-clé & \textsc{VraisPositifs} & \textsc{FauxPositifs}\\
          \cline{2-4}
          & Non terme-clé & \textsc{FauxNegatifs} & \textsc{VraisNegatifs}\\
          \hline
        \end{tabular}
        \caption{Matrice de confusion des systèmes d'extraction automatique de
                 termes-clés. \label{tab:confusionmatrix}}
      \end{center}
    \end{table}
    \begin{align}
      \textsc{Positifs}_{\text{référence}} &= \textsc{VraisPositifs} \cup \textsc{FauxNegatifs}\notag\\
      \notag\\
      \textsc{Positifs}_{\text{système}} &= \textsc{VraisPositifs} \cup \textsc{FauxPositifs} \notag\\
      \notag\\
      \text{précision} &= \frac{|\textsc{VraisPositifs}|}{|\textsc{Positifs}_{\text{système}}|} \label{math:precision}\\
      \notag\\
      \text{rappel} &= \frac{|\textsc{VraisPositifs}|}{|\textsc{Positifs}_{\text{référence}}|} \label{math:recall}\\
      \notag\\
      \text{f-mesure} &= (1 + \beta^2) \times \frac{\text{précision} \times \text{rappel}}{(\beta^2 \times \text{précision}) + \text{rappel}} \label{math:fmeasure}
    \end{align}

    Dans le cas cité précédemment, où il est préférable d'extraire un maximum de
    vrais positifs sans ce soucier du nombre de faux positifs, il existe une
    variante de la mesure de précision. Cette variante, la \textbf{précision
    moyenne} (\textit{average precision}), tient compte uniquement du classement
    des vrais positifs (cf. équation \ref{math:averageprecision}). Ainsi, la
    valeur de la mesure est élevé quand les vrais positifs sont parmi les premiers
    termes-clés extraits, ceci quelque soit le nombre de faux positifs. Ce comportement
    diffère de celui de la précision qui a tendance à chuter lorsque le nombre de
    faux positifs grandit.
    \begin{align}
      \text{précision\_moyenne} &= \frac{\mathlarger{\sum}_{t \in {\textsc{VraisPositifs}}}\text{précision}@\text{rang}(t)}{|\textsc{Positifs}_{\text{référence}}|} \label{math:averageprecision}\\
      \notag\\
      \text{précision}@n &\equiv \text{la précision calculée pour les $n$ premiers résultats} \notag
    \end{align}
    La précision moyenne n'est pas la seule mesure qui tient compte du classement
    des vrais positifs. La \textbf{r-precision} \citep{zesch2009rprecision} et le
    \textbf{\textit{reciprocal rank}} (RR) \citep{voorhees1999mrr} sont deux
    mesures qui évaluent les systèmes d'extraction de termes-clés selon leur
    capacité à bien ordonner les termes-clés extraits. La mesure de r-présision
    représente la précision lorsque le système d'extraction automatique de
    termes-clés donne le même nombre de termes-clés que dans la référence (cf.
    équation \ref{math:rprecision}). Cela peut s'avérer utile pour comparer des
    systèmes qui n'extraient pas tous le même nombre de termes-clés. Quand au
    \textit{reciprocal rank}, il ne s'intéresse qu'à la position du premier vrai
    positif extrait (cf. équation \ref{math:reciprocalrank}\footnote{Le meilleur
    rang ayant la plus petite valeur (1), c'est la réciproque du plus petit rang
    qui est utilisé. Ainsi, plus la valeur de la mesure est élevé, meilleur est
    le système.}). Un utilisateur, faisant parfois attention seulement aux
    premières informations données, cette mesure est intéressante et peut être
    perçue comme la capacité à capter l'attention.
    \begin{align}
      \text{r-précision} &= \text{précision}@(|\textsc{Positifs}_{\text{référence}}|) \label{math:rprecision}\\
      \notag \\
      RR &= \frac{1}{\text{argmin}(\forall m \in \textsc{Positifs}_{\text{système}}, \text{rang}(m))} \label{math:reciprocalrank}
    \end{align}

    Toutes les mesures qui sont présentées ici ne prennent en compte que des
    correspondances parfaites entre les termes-clés extraits et ceux de l'ensemble
    de référence\footnote{En utilisant la racine des mots, des termes-clés extraits et
    des termes-clés de référence, il est possible de réduire cette correspondance
    stricte, en acceptant les variations flexionnelles (\textit{oranges} est
    accepté à la place de \textit{orange} par exemple).}. Certains travaux se
    penchent désormais sur une évaluation moins pessimiste des systèmes
    d'extraction de termes-clés, en prenant en compte les problèmes
    d'\textbf{inclusion} ou de \textbf{recouvrement partiel}
    \citep{zesch2009rprecision, kim2010rprecision} et en les appliquant à la
    r-precision. Dans leurs travaux, \citet{zesch2009rprecision} définissent deux
    nouveaux types de correspondance :
    \begin{itemize}
      \item{l'inclusion : le terme-clé extrait est inclus dans le terme-clé de
            référence (par exemple, \og élection présidentielle \fg\ est inclus
            dans \og élection présidentielle américaine \fg);}
      \item{le recouvrement/alignement partiel: une sous partie (gauche ou
            droite) du terme-clé extrait est inclue dans le terme-clé de
            référence (par exemple, \og première élection présidentielle \fg\ et
            \og élection présidentielle américaine \fg\ se recouvrent
            partiellement).}
    \end{itemize}
    Là ou la correspondance parfaite ne donne que deux scores possible (0 : les
    deux termes-clés ne sont pas égaux ; 1 : les deux termes-clés sont égaux), la nouvelle
    mesure de correspondance donne un score de recouvrement basé sur les mots des
    deux termes-clés $t_i$ et $t_j$, $t_j$ étant le terme-clé de référence :
    \begin{align}
      \text{recouvrement}(t_{i}, t_{j}) &= \frac{|t_{i} \cap t_{j}|}{|t_{i}\ \cup\ t_{j}|}
    \end{align}
    Dans l'extension de ces travaux, \citet{kim2010rprecision} modifient le score
    d'alignement dans le but de prendre en compte l'ordre des mots dans le terme-clé
    de référence $t_j$ :
    \begin{align}
      \text{recouvrement}'(t_{i}, t_{j}) &= \frac{\mathlarger{\sum}_{m \in t_{j} \cap t_{i}} \frac{1}{|t_{j}| - (\text{position}_{t_i}(m) + 1)}}{\mathlarger{\sum}_{m' \in t_{i}} \frac{1}{|t_{i}| - (\text{position}_{t_i}(m') + 1)}}
    \end{align}

  \section{Mise en perspective}
    \TODO{Vérifier}
    Dans un contexte d'accès à l'information numérique, l'indexation automatique
    joue un rôle crucial, car elle réduit chaque document à un ensemble de
    descripteurs utilisés pour évaluer la pertinence d'un document vis-à-vis d'une
    requête utilisateur. Les descripteurs sont appelés ici termes-clés, il s'agit
    de mots simples ou des phrasèmes contenant plusieurs mots ayant une forte
    importance dans le document qu'ils indexent. Cette notion d'importance est au
    c\oe{}ur du problème de l'extraction de termes-clés, problème que tentent de
    résoudre de nombreuses méthodes. Toutes les méthodes manipulent des traits
    statistiques et/ou linguistiques, soit pour vérifier des propriétés des
    termes-clés, dans le cas des méthodes non-supervisées, soit pour apprendre
    l'impact des différents traits sur l'importance d'un terme candidat dans le cas des
    méthodes supervisées.

    Les méthodes non-supervisées sont des méthodes émergentes ayant la
    particularité de s'abstraire de la spécificité des données traitées. Cette
    abstraction s'explique par des approches basées sur des constatations à propos
    de ce qu'est un terme-clé au sens général : importance sémantique, degré
    d'information, structure syntaxique, etc. Contrairement aux méthodes
    non-supervisées, les méthodes supervisées n'utilisent pas de propriétés
    définies à partir de traits statistiques et linguistiques, mais elles
    utilisent des modèles de décision appris à partir de ces traits, calculés sur
    les termes-clés d'un corpus d'apprentissage. Ceci introduit une forte
    dépendance entre le système d'extraction de termes-clés et le type des données
    utilisées pour l'entraîner.

