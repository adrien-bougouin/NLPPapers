\chapter{Contributions à l'indexation automatique par termes-clés}
\chaptermark{Contribution à l'indexation par termes-clés}
\label{chap:main-automatic_keyphrase_annotation}
  \smallchaptercite{
    [\dots] there is still room for improvement over the task.
  }{
    \newcite{kim2010semeval}
  }

  \section{Introduction}
  \label{sec:main-automatic_keyphrase_annotation-introduction}
    Nos travaux de recherche en indexation automatique par termes-clés se
    fondent sur l'analyse que nous présentons dans le
    \ANNOTATE{chapitre~\ref{chap:main-state_of_the_art}
    (page~\pageref{chap:main-state_of_the_art})}{état de l'art}. Nous nous intéressons tout
    d'abord à l'étape préliminaire de sélection des termes-clés candidats, nous
    proposons ensuite une nouvelle méthode non supervisée d'extraction de
    termes-clés, puis nous l'étendons pour répondre au problème d'indexation par
    termes-clés dans sa globalité~: extraction et assignement.

    La sélection des termes-clés candidats est une étape importante de
    l'indexation par termes-clés, car elle définit la frontière entre les unités
    textuelles qui sont potentiellement des termes-clés et celles qui n'en sont
    pas. Elle fixe ainsi la performance maximale que peuvent atteindre les
    méthodes d'indexation par termes-clés, leur le rappel maximal. Dans la
    littérature, cette étape fait l'objet de peu d'études. Nous proposons donc
    une analyse des caractéristiques linguistiques des termes-clés de référence
    de nos collections de données et proposons une méthode de sélection fine des
    termes-clés candidats pour l'extraction automatique de termes-clés.

    L'extraction automatique de termes-clés est la catégorie d'indexation par
    termes-clés qui fait l'objet du plus grand nombre de travaux. Celle-ci se
    concentre sur les unités textuelles présentes dans le document. Elle
    peut-être réalisée de manière supervisée, en apprenant à reconnaître les
    termes-clés selon des caractéristiques diverses, ou de manière non
    supervisée, en ordonnant les termes-clés candidats par importance. Les
    méthodes non supervisées ayant l'avantage d'avoir un couplage faible (voir
    inexistant) avec les caractéristiques des données traitées, nous définissons
    les bases d'une nouvelle méthode d'indexation par termes-clés sur le modèle
    des méthodes non supervisées à base de graphe pour l'extraction de
    termes-clés.

    Notre dernière contribution reprend notre travail en extraction automatique
    de termes-clés et lui ajoute la capacité à assigner des termes-clés,
    c'est-à-dire à fournir des termes-clés (d'un vocabulaire contrôlé) qui
    n'apparaissent pas dans le document. Peu de travaux s'intéressent à
    l'assignement de termes-clés. Le vocabulaire contrôlé que requièrent les
    méthodes de cette catégorie est une ressource qui n'est pas toujours
    disponible et qui est coûteuse à produire (en termes de temps et de
    ressources humaines). Pour réaliser l'assignement de termes-clés, nous
    proposons d'utiliser les termes-clés de référence des documents
    d'entraînement de nos données pour créer un vocabulaire contrôlé intégré au
    sein du graphe de notre méthode d'extraction de termes-clés à base de
    graphe.

  %-----------------------------------------------------------------------------

  \section{Sélection des termes-clés candidats}
  \label{sec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection}
    Dans cette section, nous nous intéressons à l'étape de sélection des
    termes-clés candidats, aux propriétés que ces derniers doivent respecter et
    à la façon de les sélectionner dans le document.

    La majorité des méthodes d'indexation par termes-clés de la littérature
    travaillent à partir d'un ensemble de termes-clés candidats sélectionnés
    préalablement dans le document. Bien que cette étape préliminaire commence à
    susciter de l'intérêt~\cite{wang2014keyphraseextractionpreprocessing}, très
    peu de méthodes de sélection sont proposées et les méthodes les plus simples
    restent celles qui sont actuellement les plus utilisées (sélection des
    $\{1..3\}$-grammes, des NP-\textit{chunks} ou des séquences grammaticalement
    définies). Toutefois, nous estimons que la sélection des termes-clés
    candidats ne doit pas être négligée, car elle influence aussi bien la
    performance maximale pouvant être atteinte que la difficulté de l'indexation
    automatique par termes-clés. Prenons l'exemple de l'approche naïve assurant
    la performance maximale optimale. Pour assurer cette performance maximale
    optimale, il suffit de fournir toutes les séquences possibles de mots
    présents dans le document -- soit pour chaque phrase de longueur
    variable $n$, la liste de ses $\{1..n\}$-grammes. Cependant, cette
    approche a pour conséquence de sélectionner un nombre considérable de
    candidats et parmi eux un nombre important de candidats négatifs qui
    augmentent la difficulté de la tâche d'indexation par
    termes-clés~\cite{hasan2014state_of_the_art}. Nous émettons l'hypothèse
    qu'il est préférable d'utiliser des méthodes de sélection qui proposent un
    minimum de candidats tout en assurant une performance maximale
    quasi-optimale.

    Dans la suite, nous présentons une analyse des termes-clés de référence
    associés aux documents de nos collections de données, nous discutons la
    pertinence de certains adjectifs en tant que modificateurs au sein d'un
    terme-clé et nous proposons une méthodes pour sélectionner plus finement les
    candidats en portant une attention particulière à leurs adjectifs. Nous
    montrons le bien fondé de ce travail au travers de deux évaluations~: l'une
    intrinsèque, l'autre extrinsèque. L'évaluation intrinsèque compare la
    qualité de l'ensemble de termes-clés candidats sélectionnés par notre
    méthode à ceux sélectionnés par les méthodes les plus utilisées~;
    l'évaluation extrinsèque compare l'impact de notre méthode de sélection sur
    deux méthodes d'extraction de termes-clés à celui des méthodes de sélection
    les plus utilisées.

    \subsection{Analyse des propriétés linguistiques des termes-clés}
    \label{subsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-analysis_of_keyphrase_properties}
      Afin de sélectionner plus finement les termes-clés candidats, nous
      extrayons et analysons diverses statistiques concernant les termes-clés~:
      leur taille (en nombre de mots) et la catégorie grammaticale des mots qui
      les composent. Cela nous permet de confirmer les observations faites dans
      les travaux précédents et d'en faire de nouvelles, concentrées sur la
      catégorie grammaticale des mots des termes-clés.

      Pour ce travail, nous analysons les termes-clés de référence des
      collections de données \textsc{Deft}, SemEval et \textsc{Duc} afin de
      couvrir les deux langues française et anglaise. Pour ne pas influencer
      l'évaluation de la méthode de sélection de candidat que nous proposons en
      aval de cette analyse, nous étudions uniquement les données
      d'entraînement. Dans le cas de \textsc{Duc}, nous divisons la collection
      en deux ensembles~: un ensemble d'entraînement composé de 208 documents et
      un ensemble de test constitué de 100 documents. Le
      tableau~\ref{tab:candidate_selection-corpora_recap} rappelle les
      caractéristiques de ces trois collections de données.
      \begin{table}[!h]
        \centering
        \resizebox{\linewidth}{!}{
          \begin{tabular}{l@{~}l|c@{~~}c@{~~}c@{~~}c|c@{~~}c@{~~}c@{~~}c}
            \toprule
            \multicolumn{2}{l|}{\multirow{2}{*}{\textbf{Corpus}}} & \multicolumn{4}{c|}{\textbf{Documents}} & \multicolumn{4}{c}{\textbf{Termes-clés}}\\
            \cline{3-10}
            & & Langue & Genre & Quantité & Mots moy. & Annotateur & Quantité moy. & \og{}À assigner\fg{} & Mots moy.\\
            \hline
            \multicolumn{2}{l|}{\textsc{Deft}} & & & & & & & &\\
            $\drsh$ & Appr. & Français & Scientifique & 141 & 7~276,7 & Auteur & $~~$5,4 & 18,2~\% & 1,7\\
            \multicolumn{2}{l|}{Semeval} & & & & & & & &\\
            $\drsh$ & Appr. & Anglais & Scientifique & 144 & 5~134,6 & Auteur~/~Lecteur & 15,4 & 13,5~\% & 2,1\\
            \multicolumn{2}{l|}{\textsc{Duc}} & & & & & & & &\\
            $\drsh$ & Appr. & Anglais & Journalistique & 208 & $~~~~$912,0 & Lecteur & $~~$8,1 & $~~$3,9~\% & 2,1\\
            \bottomrule
          \end{tabular}
        }

        \caption{Rappel des corpus utilisés pour analyser les termes-clés de
                 référence
                 \label{tab:candidate_selection-corpora_recap}}
      \end{table}

      Le tableau~\ref{tab:candidate_selection-train_stats} montre la proportion
      de termes-clés uni-grammes, bi-grammes et tri-grammes, ainsi que la
      proportion de termes-clés multi-mots contenant au moins un mot de l'une
      des sept catégories grammaticales que nous observons en leur
      sein\footnote{ Nous nous focalisons sur les expressions (termes-clés
      multi-mots), car nous avons observé que \TODO{\dots}~\% des termes-clés
      qui sont des mots simples sont des noms. }~: nom commun, nom propre,
      adjectif, verbe, adverbe, préposition et déterminant. Pour obtenir ces
      informations, les termes-clés ont été automatiquement segmentés en mots,
      étiquetés grammaticalement et manuellement corrigés. Les termes-clés sont
      automatiquement segmentés en mots avec la classe
      \texttt{TreeBankWordTokenizer}, du module Python
      \textsc{Nltk}~\cite[\textit{Natural Language ToolKit}]{bird2009nltk}, pour
      l'anglais et avec l'outil Bonsai, du \textit{Bonsai \textsc{Pcfg-La}
      parser}\footnote{\url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}},
      pour le français. Les mots sont automatiquement étiquetés en catégories
      grammaticales à l'aide du \textit{Stanford \textsc{Pos}
      tagger}~\cite{toutanova2003stanfordpostagger} pour l'anglais et à l'aide
      de l'outil MElt~\cite{denis2009melt} pour le français.
      \begin{table}[!h]
        \centering
        \begin{tabular}{ll|ccc}
          \toprule
          & & \textbf{\textsc{Duc}} & \textbf{SemEval} & \textbf{\textsc{Deft}}\\
          \hline
          \multicolumn{2}{l|}{\textbf{Taux (en \%) de termes-clés~:}}\\
          & Uni-grammes & 17,1 & 20,2 & 60,2\\
          & Bi-grammes & 60,8 & 53,4 & 24,5\\
          & Tri-grammes & 17,8 & 21,3 & 8,8\\
          \hline
          \multicolumn{2}{l|}{\textbf{Taux (en \%) de termes-clés}} & & &\\
          \multicolumn{2}{l|}{\textbf{contenant au moins un(e)~:}} & & &\\
          & Nom commun & 94,5 & 98,7 & 93,1\\
          & Nom propre & 17,1 & $~~$4,3 & $~~$6,9\\
          & Adjectif & 50,0 & 50,2 & 65,5\\
          & Verbe & $~~$1,0 & $~~$4,0 & $~~$1,0\\
          & Adverbe & $~~$1,6 & $~~$0,7 & $~~$1,3\\
          & Préposition & $~~$0,3 & $~~$1,5 & 31,2\\
          & Déterminant & $~~$0,0 & $~~$0,0 & 20,4\\
          \bottomrule
        \end{tabular}
        \caption{Statistiques concernant les termes-clés de référence des
                 collections \textsc{Deft}, SemEval et \textsc{Duc}
                 \label{tab:candidate_selection-train_stats}}
      \end{table}

      Concernant la taille des termes-clés de référence, ceux composés de un à
      trois mots couvrent plus de 90~\% des termes-clés de références. En
      français, ce sont les uni-grammes qui sont les plus présents, suivis par
      les bi-grammes, tandis qu'en anglais, ce sont les bi-grammes qui sont les
      plus présents, avec des proportions équivalentes d'uni-grammes et de
      tri-grammes. Ces premières observations font écho de celles que nous
      trouvons dans la littérature, nous pouvons donc en conclure qu'il s'agit
      de propriétés stables des termes-clés. Une approche qui se restreint aux
      $\{1..3\}$-grammes, telle que celle de \newcite{witten1999kea}, est donc
      pertinente.

      Concernant la catégorie des mots que contiennent les termes-clés de
      référence, nous observons sans surprise que la quasi-totalité des
      termes-clés contiennent un nom~; ce sont principalement des groupes
      nominaux. Nous remarquons aussi que la moitié d'entre eux est modifiée par
      un adjectif, tandis que très peu contiennent des verbes et des adverbes.
      L'usage de ces derniers au sein de termes-clés semble être exceptionnel.
      Les déterminant et préposition ont un usage presque exclusivement français
      au sein des termes-clés de référence. Ceci s'explique par le fait, qu'en
      anglais, les formes nominales sont préférées aux expressions
      prépositionnelles (par exemple~: \textit{\og{}nature conservation\fg{}} --
      \og{}conservation de la nature\fg{} -- est préférée à
      \textit{\og{}conservation of nature\fg{}}), alors que ce n'est pas
      toujours possible en français (par exemple \og{}conservation de la
      nature\fg{} ne peut pas être remplacé par \og{}conservation
      naturelle\fg{}).

      Dans le tableau~\ref{tab:candidate_selection-adjective_categories}, nous
      nous intéressons plus particulièrement aux adjectifs et à leurs
      catégories. Plus précisément, nous cherchons à voir parmi les adjectifs
      relationnels, les adjectifs composés et les adjectifs qualificatifs autres
      que composés, quels sont ceux les plus présents au sein des termes-clés de
      référence.
      \begin{table}[!ht]
        \centering
          \begin{tabular}{l|ccc}
            \toprule
            & \textbf{\textsc{Duc}} & \textbf{SemEval} & \textbf{\textsc{Deft}} \\
            \hline
            Adjectifs relationnels \hfill(\%) & 53,1 & 43,6 & 87,1\\
            Adjectifs composés \hfill(\%) & 10,6 & 16,4 & $~~$3,3\\
            Autres adjectifs qualificatifs \hfill(\%) & 36,3 & 40,0 & $~~$9,6\\
            \bottomrule
        \end{tabular}
        \caption{Taux d'adjectifs (de termes-clés) par catégorie (relationnel,
                 composé ou qualificatif)}
        \label{tab:candidate_selection-adjective_categories}
      \end{table}
      
      Nous observons que la majorité des adjectifs présents au sein des
      termes-clés de référence sont des adjectifs relationnels. Ceux-ci sont des
      adjectifs dérivés de nom (par exemple~: l'adjectif relationnel
      \og{}culturel\fg{} est dérivé du nom \og{}culture\fg{}) et équivalents à
      un complément de nom (par exemple~: \og{}héritage culturel\fg{} équivaut à
      \og{}héritage de la
      culture\fg{})~\cite{bally1944linguistiquegeneraleetlinguistiquefrancaise}.
      Leur lien étroit avec le nom leur confère le statut de modificateurs au
      sein de catégories, telles que celles de Wikipedia (par exemple
      \og{}héritage culturel\fg{}), ces mêmes catégories pouvant être utilisées
      comme termes-clés~\cite{medelyan2008smalltrainingset,eichler2010keywe}.
      Nous supposons donc, confortés par l'étude de
      \newcite{maniez2009denominaladjectives} dans le domaine médical, que
      l'adjectif relationnel est un élément-clé d'un terme-clé. Nos propos se
      confirment dans le tableau~\ref{tab:candidate_selection-best_patterns},
      qui montre les patrons grammaticaux les plus fréquents pour les
      termes-clés de référence.
      
      Les adjectifs composés sont la réunion de mots équivalents à un adjectif,
      nous parlons aussi de locutions adjectifs (par exemple~:
      \og{}socio-culturel\fg{}). Bien qu'ils ne représentent qu'une faible
      quantité des adjectifs présents au sein des termes-clés de référence, nous
      trouvons les adjectifs composés pertinents en tant que modificateurs de
      termes-clés. Ceux-ci apportent généralement un spécialisation hiérarchique
      du nom qu'ils modifient (par exemple, \og{}évênement socio-culturel\fg{}
      est un hyponyme de \og{}évênement\fg{}), rendant alors un terme-clé
      beaucoup plus précis.
      
      Les adjectifs qualificatifs sont des mots qui donnent une qualification à
      un nom~; ils en désignent la qualité ou manière d'être (par exemple
      \og{}grand\fg{}). Ceux-ci sont présents en nombre non négligeable au sein
      des termes-clés de référence. Cependant, la catégorie des adjectifs
      qualificatifs regroupe un grand nombre d'adjectifs qui ne sont
      intuitivement pas tous pertinent en tant que modificateur au sein d'un
      terme-clés (\TODO{exemple}). \TODO{renforcer l'argumentation en comparant
      la proportion d'adjectifs qualificatis avec la proportion d'adjectifs
      relationnels dans les documents.}

      \begin{table}[!h]
        \centering
        \begin{tabular}{r@{~}|@{~}l@{~}l@{~}l@{~}llr}
          \toprule
          \multicolumn{1}{r@{~}|@{~}}{} & \multicolumn{4}{@{}l}{\textbf{Pattern}} & \textbf{Example} & \textbf{\%}\\
          \hline
          \multirow{5}{*}{\begin{sideways}\textbf{Français}\end{sideways}}
          & \texttt{Nc} & \texttt{rA} & & & \TODO{exemple} & 46,4\\
          & \texttt{NC} & \texttt{Sp} & \texttt{D} & \texttt{Nc} & \TODO{exemple} & 12,5\\
          & \texttt{Nc} & \texttt{Sp} & \texttt{Nc} & & \TODO{exemple} & $~~$8,2\\
          & \texttt{Nc} & \texttt{A} & & & \TODO{exemple} & $~~$4,3\\
          & \texttt{Np} & \texttt{Np} & & & \TODO{exemple} & $~~$3,0\\
          \hline
          \multirow{5}{*}{\begin{sideways}\textbf{Anglais}\end{sideways}}
          & \texttt{Nc} & \texttt{Nc} & & & \TODO{exemple} & 32,5\\
          & \texttt{rA} & \texttt{Nc} & & & \TODO{exemple} & 15,1\\
          & \texttt{A} & \texttt{Nc} & & & \TODO{exemple} & $~~$9,5\\
          & \texttt{Nc} & \texttt{Nc} & \texttt{Nc} & & \TODO{exemple} & $~~$5,3\\
          & \texttt{cA} & \texttt{Nc} & & & \TODO{exemple} & $~~$4,9\\
          \bottomrule
        \end{tabular}
        \caption[
          Patrons grammaticaux les plus fréquents parmi les termes-clés
          français et anglais
        ]{
          Patrons grammaticaux les plus fréquents parmi les termes-clés
          français et anglais. Les classes grammaticales sont exprimées au
          format Multext~\cite{ide1994multext}, sauf \texttt{rA} et \texttt{cA}
          qui représentent, respectivement, un adjectif relationnel et un
          adjectif composé.
          \label{tab:candidate_selection-best_patterns}
        }
      \end{table}

    \subsection{Sélection fine des termes-clés candidats}
    \label{subsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-modifiers_filtering}
      Pour sélectionner les termes-clés candidats, nous proposons une méthode
      qui se fonde sur les observations ci-avant. Cette méthode commence par
      présélectionner les termes-clés candidats à l'aide d'un patron
      grammatical, elle détecte ensuite la catégorie de leurs adjectifs (s'ils
      en possèdent), puis elle filtre ceux qui ne sont pas jugés pertinents.

      \subsubsection{Présélection des termes-clés candidats}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-modifiers_filtering-candidate_pre_selection}
        L'étape de présélection des candidats utilise un patron grammatical
        défini sous la forme d'une expression rationnelle. Celui-ci est appliqué
        aux catégories grammaticales des séquences de mots adjacents dans le
        document et sélectionne celles-ci qui le respectent. Ce patron est
        gourmand, c'est-à-dire qu'il capture les plus longues séquences
        possibles. Ainsi, il n'y a pas de surproduction de candidats qui se
        recouvrent (comme c'est le cas avec les n-grammes) et augmentent le
        risque d'extraire des termes-clés
        redondants~\cite{hasan2014state_of_the_art}.

        D'après les observations de la
        section~\ref{subsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-analysis_of_keyphrase_properties},
        seuls les noms, les adjectifs, les prépositions et les déterminants sont
        utiles pour sélectionner des candidats permettant une performance
        maximale quasi-optimale. Dans le cas de l'anglais, les prépositions et
        les déterminants sont en proportions très faibles et peuvent être
        ignorés. Dans le cas du français, les prépositions et les déterminant
        couvrent plus de 30~\% des termes-clés de référence, cependant leur
        nombre est tellement important au sein d'un document \TODO{donner les
        chiffres} qu'il est préférable de les ignorés afin d'éviter d'ajouter un
        grand nombre de candidats erronés pouvant dégrader la performance des
        méthodes d'extraction de termes-clés.
        
        Pour le français, nous définissons le patron \texttt{/N+ A?/}, qui
        accepte une séquence de noms (ou noms propres) se terminant
        optionnellement par un adjectif. En français, l'adjectif peut être soit
        antéposé, soit postposé. Le patron que nous avons défini n'accepte que
        les adjectifs postposés pour deux raisons. La première raison est que
        les adjectifs relationnels, que nous jugeons les plus pertinents en tant
        que modificateurs au sein des termes-clés, se trouvent toujours en
        postposition. La seconde raison est que l'antéposition et la
        postposition des adjectifs marquent deux aspects différents du
        discours~: la subjectivité et l'objectivité,
        respectivement~\cite{eskenazi2005adjectifavantapres}. Nous supposons,
        avec l'appui des patrons grammaticaux les plus fréquents présentés dans
        le tableau~\ref{tab:candidate_selection-best_patterns}, que la
        subjectivité n'a pas ça place au sein d'un terme-clé.
        
        Pour l'anglais, nous définissons le parton \texttt{/A? N+/}, qui
        accepte une séquence de noms (ou noms propres) modifié par un adjectifs
        antéposé. En anglais, tous les adjectifs sont antéposés. Ce patron ne
        filtre donc aucun adjectif à cette étape de présélection.

      \subsubsection{Détection de la catégorie de l'adjectif}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-modifiers_filtering-relational_adjective_detection}
        Lorsqu'un terme-clé candidat contient un adjectif, nous l'analysons afin
        d'en connaître la catégorie~: relationnel, composé ou autre. Pour
        détecter les adjectifs composés, nous nous limitons à la présence du
        trait d'union, marque de composition en français et en anglais~; pour
        détecter les adjectifs relationnels, nous utilisons une technique
        simple, adaptée (ou adaptable) à plusieurs langues, et ne requérant pas
        nécessairement de données riches.

        La détection des adjectifs relationnels se fonde sur les deux propriétés
        linguistiques suivantes~:
        \begin{enumerate}
          \item{L'adjectif relationnel est dénominal, c'est-à-dire
                \textbf{dérivé d'un
                nom}~\cite{bally1944linguistiquegeneraleetlinguistiquefrancaise}.}
          \item{L'adjectif relationnel est dérivé du nom par
                \textbf{suffixation}~\cite{dubois1999derivation}.}
        \end{enumerate}

        Dans un premier temps, les adjectifs relationnels sont détectés avec une
        base de données lexicale. Si l'adjectif d'un candidat présélectionné est
        marqué comme dérivé d'un nom, alors nous le considérons comme
        relationnel. Pour le français, nous utilisons la base
        WoNeF~\cite{pradet2013wonef} dont la propriété \texttt{[DERIVED]}
        indique que l'adjectif est dérivé d'un nom. Pour l'anglais, nous
        utilisons la base  WordNet~\cite{miller1995wordnet} dont la propriété
        \texttt{[PERTAINYM]} indique que l'adjectif est relationnel.

        Dans un second temps, les adjectifs relationnels qui ne sont pas
        présents dans la base de données lexicale, ou si cette base n'existe
        pas, sont détectés à partir de leur suffixe. Une liste des suffixes les
        plus productifs pour les adjectifs relationnels est dressée et utilisée
        pour identifier les adjectifs relationnels potentiels. En français, les
        suffixes les plus productifs sont \textit{-ain}, \textit{-aire},
        \textit{-al}, \textit{-el}, \textit{-esque}, \textit{-estre},
        \textit{-eux}, \textit{-ien}, \textit{-ier}, \textit{-if}, \textit{-il},
        \textit{-in}, \textit{-ique}, \textit{-ois}, et
        \textit{-é}~\cite{guyon1993adjectifsrelationnels}~; en anglais, les
        suffixes utilisés sont \textit{al}, \textit{ant},
        \textit{ary}, \textit{ic}, \textit{ous} et
        \textit{ive}~\cite{grabar2006terminologystructuring}.

        La détection des adjectifs relationnels telle que réalisée dans ce
        travail, n'est pas exacte. En effet, les adjectifs qualificatifs se
        terminant par un suffixe d'adjectif relationnel sont détectés comme
        relationnels, et les adjectifs à usage qualificatif ou relationnel selon
        le contexte~\cite{maniez2009denominaladjectives} sont toujours détectés
        comme relationnels. Les précédentes approches pour identifier les
        adjectifs relationnels (dans le cadre de l'extraction terminologique) se
        reposent sur une analyse en
        corpus~\cite{daille2001relationaladjectives,maniez2005automaticrelationaladjectiveidentification,harastani2013relationaladjectivetranslation},
        où il s'agit notamment de trouver des paraphrases avec compléments de
        noms. Dans le contexte de l'extraction de termes-clés, où de larges
        corpus ne sont pas toujours disponibles, il n'est pas toujours possible
        d'utiliser ce type d'approche contrairement à celle que nous proposons.

      \subsubsection{Filtrage des adjectifs non pertinents}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-modifiers_filtering-adjective_filtering}
        L'orsqu'un candidat contient un adjectif, notre méthode juge sa
        pertinence en tant que modificateur d'un terme-clé, autrement dit s'il
        apporte au sens du terme-clé. Si l'adjectif n'est pas jugé pertinent,
        alors il est enlevé du terme-clé candidats.

        Toujours d'après les observations et hypothèses de la
        section~\ref{subsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-analysis_of_keyphrase_properties},
        les adjectifs relationnels et composés sont systématiquement jugés
        pertinents. Dans le cas des adjectifs qualificatifs (autres que
        composés), c'est l'usage des candidats présélectionnés dans le document
        qui détermine leur pertinence. Tout adjectif qualificatif modifiant une
        même séquence de nom plus souvent que cette dernière apparaît de manière
        autonome est jugé pertinent et est conservé en tant que modificateur de
        cette séquence.

        \paragraph{}
        L'algorithme~\ref{algo:candidate_pruning} résume le
        fonctionnement de notre méthode de sélection des termes-clés candidats.
        Les lignes \ref{algo:line:start_preselection} à
        \ref{algo:line:end_preselection} concernent la présélection des
        candidats et les lignes \ref{algo:line:start_filtering} à
        \ref{algo:line:end_filtering} identifient et filtrent les adjectifs
        qualificatifs non pertinent (\TODO{exemple}).
        \begin{algorithm}[h!]
          \SetKwInOut{kwInput}{Entrée}
          \SetKwInOut{kwOutput}{Sortie}
          \SetKwFor{For}{Pour chaque}{faire}{}
          \SetKwIF{If}{ElseIf}{Else}{Si}{alors}{Sinon si}{Sinon}{}
          \SetKw{KwRet}{Retourner}
          \DontPrintSemicolon{}

          \kwInput{document}
          \kwOutput{candidats}
          \BlankLine

          patron $\leftarrow$ Nil\;\label{algo:line:start_preselection}
          \If{\textnormal{document.langue = "francais"}}{
            patron $\leftarrow$ \texttt{/N+ A?/}\;
          }\Else{
            \If{\textnormal{document.langue = "anglais"}}{
              patron $\leftarrow$ \texttt{/A? N+/}\;
            }
          }

          candidats $\leftarrow$ \{\}\;
          candidats\_preliminaires $\leftarrow$ preselection(document, patron)\;\label{algo:line:end_preselection}

          \For{\textnormal{cdt} $\in$ \textnormal{candidats\_preliminaires}}{\label{algo:line:start_filtering}
            \If{$\exists{}\textnormal{mot} \in$ \textnormal{cdt},
            \textnormal{est\_adjectif(mot)} $\wedge$ $\overline{\textnormal{est\_relationnel(mot)}}$ $\wedge$ $\overline{\textnormal{est\_compose(mot)}}$}{
              tete\_cdt $\leftarrow$ cdt $-$ mot\;
              freq\_cdt $\leftarrow$ \textnormal{document.conter(cdt)}\;
              freq\_tete\_cdt $\leftarrow$ \textnormal{document.conter(tete\_cdt)} $-$ freq\_cdt\;
              \If{\textnormal{freq\_cdt} $>$ \textnormal{freq\_tete\_cdt}}{
                candidats $\leftarrow$ candidats $\cup$ \{cdt\}\;
              }\Else{
                candidats $\leftarrow$ candidats $\cup$ \{tete\_cdt\}\;
              }
            }\Else{
              candidats $\leftarrow$ candidats $\cup$ \{cdt\}\;
            }
          }\label{algo:line:end_filtering}

          \Return{\textnormal{candidats}}

          \caption{Sélection fine des termes-clés candidats
                   \label{algo:candidate_pruning}}
        \end{algorithm}

    \subsection{Évaluation}
    \label{subsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-evaluation}
      Afin de montrer la validité de notre méthode de sélection de candidats,
      nous réalisons deux expériences~: l'une intrinsèque, où les ensembles de
      termes-clés candidats de différentes méthodes de sélections sont comparés
      qualitativement, l'autre extrinsèque, où les différentes méthodes de
      sélections sont comparés d'après les performances de deux méthodes
      d'extraction de termes-clés.

      \subsubsection{Méthodes de référence}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-evaluation-baselines}
        Nous comparons notre méthode de sélection de termes-clés candidats a
        trois autres méthodes utilisées dans les travaux précédents en
        extraction automatique de termes-clés~:
        \begin{itemize}
          \item{sélection des n-grammes ($1 \leq n \leq 3$)~;}
          \item{sélection des plus longues séquences de noms et d'adjectifs~:
                \texttt{/(N|A)+/}~;}
          \item{sélection des \textit{NP-chunks}~:}
          \begin{itemize}
            \item{\texttt{/Np+|(A? Nc A+)|(A Nc)|Nc+/} en français~;}
            \item{\texttt{/Np+|(A+ Nc)|Nc+/} en anglais.}
          \end{itemize}
        \end{itemize}

        Pour l'évaluation extrinsèque, nous utilisons deux méthodes d'extraction
        automatique de termes-clés très utilisées~: la méthode non supervisée
        \textsc{Tf-Idf} et la méthode supervisée \textsc{Kea}. Bien que des
        méthodes plus récentes donnent de meilleures performances que
        \textsc{Tf-Idf} et \textsc{Kea}~\cite{kim2010semeval}, ces dernières
        sont reproductibles et présentent l'avantage d'être robustes.

      \subsubsection{Collections de données}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-evaluation-evaluation_data}
        Pour évaluer ce travail, nous utilisons les ensembles de test des
        collections \textsc{Deft}, SemEval et \textsc{Duc}, dont les ensembles
        d'entraînement sont utilisées pour l'analyse proposée dans la
        section~\ref{subsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-analysis_of_keyphrase_properties}.
        Le tableau~\ref{tab:candidate_filtering:test_corpora_recap} rappelle les
        caractéristiques des ensembles de test, qui sont sensiblement
        équivalentes à celles des ensembles d'entraînement.
        \begin{table}[!h]
          \centering
          \resizebox{\linewidth}{!}{
            \begin{tabular}{l@{~}l|c@{~~}c@{~~}c@{~~}c|c@{~~}c@{~~}c@{~~}c}
              \toprule
              \multicolumn{2}{l|}{\multirow{2}{*}{\textbf{Corpus}}} & \multicolumn{4}{c|}{\textbf{Documents}} & \multicolumn{4}{c}{\textbf{Termes-clés}}\\
              \cline{3-10}
              & & Langue & Genre & Quantité & Mots moy. & Annotateur & Quantité moy. & \og{}À assigner\fg{} & Mots moy.\\
              \hline
              \multicolumn{2}{l|}{\textsc{Deft}} & & & & & & & &\\
              $\drsh$ & Test & Français & Scientifique & $~~$93 & 6~839,4 & Auteur & $~~$5,2 & 21,1~\% & 1,6\\
              \multicolumn{2}{l|}{Semeval} & & & & & & & &\\
              $\drsh$ & Test & Anglais & Scientifique & 100 & 5~177,7 & Auteur~/~Lecteur & 14,7 & 22,1~\% & 2,1\\
              \multicolumn{2}{l|}{\textsc{Duc}} & & & & & & & &\\
              $\drsh$ & Test & Anglais & Journalistique & 100 & $~~$877,2 & Lecteur & $~~$8,1 & $~~$2,7~\% & 2,1\\
              \bottomrule
            \end{tabular}
          }

          \caption{Rappel des corpus utilisés pour évaluer la sélection des
                   candidats
                   \label{tab:candidate_filtering:test_corpora_recap}}
        \end{table}

      \subsubsection{Prétraitements}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-evaluation-preprocessing}
        Pour chaque collection de données, et quelque soit la méthode utilisée,
        les documents subissent les mêmes prétraitements~: segmentation en
        phrases, segmentation des phrases en mots et étiquetage grammatical des
        mots. Quelque soit la langue, la segmentation en phrases est mise en
        \oe{}uvre avec la classe \texttt{PunktSentenceTokenizer} de
        \textsc{Nltk}~\cite{bird2009nltk}. En anglais, la segmentation des
        phrases en mots est réalisée avec la classe
        \texttt{TreeBankWordTokenizer} de \textsc{Nltk} et l'étiquetage
        grammaticale est effectué avec le \textit{Stanford \textsc{Pos}
        tagger}~\cite{toutanova2003stanfordpostagger}. En français, la
        segmentation des phrases en mots est mise en \oe{}uvre avec le
        segmenteur Bonsai, du \textit{Bonsai \textsc{Pcfg-La}
        parser}\footnote{\url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}},
        dont se fonde l'outil MElt~\cite{denis2009melt} utilisé pour
        l'étiquetage grammatical. Tous ces outils sont appliqués avec leur
        configuration par défaut.
      
      \subsubsection{Mesures d'évaluation}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-evaluation-evaluation_measures}
        Pour évaluer la qualité des ensembles de termes-clés candidats
        sélectionnés, nous comparons leur nombre de candidats (Cand./Soc.), le
        rappel maximum (R$_\textnormal{max}$) pouvant être atteint et un ratio
        exprimant le compromis en ces deux critères (QR)~:
        \begin{align}
          \textnormal{QR} &= \frac{\textnormal{R}_{\textnormal{max}}}{\textnormal{Cand./Doc.}} \times 100
        \end{align}
        Plus QR est élevé, meilleure est la qualité de l'ensemble des
        termes-clés candidats sélectionnés.

        Les performances des méthodes d'extraction de termes-clés sont exprimées
        en termes de précision (P), rappel (R) et f-mesure (f1-mesure, F). En
        accord avec l'évaluation menée dans les travaux précédents, nous
        considérons correcte l'extraction d'une variante flexionnelle d'un
        terme-clé de référence~\cite{kim2010semeval}. Les opérations de
        comparaison entre les termes-clés de référence et les termes-clés
        extraits sont donc effectuées à partir de la racine des mots qui les
        composent, d'après la méthode de \newcite{porter1980suffixstripping}.

      \subsubsection{Évaluation intrinsèque}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-evaluation-intrinsic_evaluation}
        L'évaluation intrinsèque a pour objectif d'évaluer la qualité de
        l'ensemble des termes-clés candidats sélectionnés par notre méthode
        (LR-NP, pour \textsc{Linguistically-Refined Noun Phrases}) et
        de la comparer à celle des ensembles de termes-clés sélectionnés par les
        méthodes de référence. Nous faisons l'hypothèse que plus une
        méthode de sélection de candidats permet un rappel maximum élevé
        (quasi-optimal) tout en limitant la quantité de termes-clés candidats
        sélectionnés, alors plus elle fournit un ensemble de candidats de bonne
        qualité.

        Le tableau~\ref{tab:candidate_extraction_statistics} présente les
        résultats de l'évaluation intrinsèque. Le nombre de candidats
        sélectionnés par chaque méthode, le rappel maximum pouvant être atteint
        avec ceux-ci et la mesure QR sont reportés pour chacune des
        trois collections de données utilisées pour cette évaluation.
        \begin{table}[!h]
          \centering
          \resizebox{\linewidth}{!}{
            \begin{tabular}{r|ccc|ccc|ccc}
              \toprule
              \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{DUC}} & \multicolumn{3}{c|}{\textbf{SemEval}} & \multicolumn{3}{c}{\textbf{DEFT}}\\
              \cline{2-10}
              & Cand./Doc. & R$_{\text{max}}$ & QR & Cand./Doc. & R$_{\text{max}}$ & QR & Cand./Doc. & R$_{\text{max}}$ & QR\\
              \hline
              n-grammes & $~~~$596.2 & \textbf{90.8} & 15.2 & 2580.5 & \textbf{72.2} & $~~$2.8 & 4070.2 & \textbf{74.1} & $~~~$1.8\\
              \texttt{/(N|A)+/} & $~~~$155.6 & 88.7 & 57.0 & $~~~$646.5 & 62.4 & $~~$9.7 & $~~~$914.5 & 61.1 & $~~$6.7\\
              \textit{NP-chunks} & $~~~$149.9 & 76.0 & 50.7 & $~~~$598.4 & 56.6 & $~~$9.5 & $~~~$812.3 & 63.0 & $~~$7.8\\
              LR-NP & \textbf{$~~~$143.8} & 85.3 & \textbf{59.3} & \textbf{$~~~$538.2} & 59.4 & \textbf{11.0} & \textbf{$~~~$738.2} & 60.1 & \textbf{$~~$8.1}\\
              \bottomrule
            \end{tabular}
          }
          \caption{Résultats de l'évaluation intrinsèque des méthodes de
                   sélection des termes-clés candidats
                   \label{tab:candidate_extraction_statistics}}
        \end{table}
        
        Globalement, nous remarquons que notre méthode sélectionne le moins de
        candidats sans nécessairement induire le moins bon rappel maximum
        (obtenu avec les \textit{NP-chunks}).
        C'est, sans surprise, la sélections des n-grammes qui induit le meilleur
        rappel maximum. Celui-ci est très proche du rappel maximum optimal, mais
        au prix d'un nombre de candidats 4 à 5 fois supérieur à celui des autres
        méthodes. Comme le montre la mesure de compromis entre nombre de
        candidats sélectionnés et rappel maximum (QR), notre méthode
        est de meilleure qualité que les autres, suivit par la méthode de
        sélection des \texttt{/(N|A)+/}, par la méthode de sélection des
        \textit{NP-chunks} et, de loin, par la méthode de sélection des
        n-grammes.

      \subsubsection{Évaluation extrinsèque}
      \label{subsubsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-evaluation-extrinsic_evaluation}
        L'évaluation extrinsèque a pour objectif d'évaluer l'efficacité de notre
        méthode de sélection de termes-clés en situation réelle d'extraction de
        termes-clés et de la comparer à celle des méthodes de référence.
        Il s'agit aussi de valider notre hypothèse par laquelle plus une méthode
        de sélection de candidats permet un rappel maximum élevé tout en
        limitant la quantité de termes-clés candidats sélectionnés, alors plus
        elle fournit un ensemble de candidats de bonne qualité.

        Le tableau~\ref{tab:keyphrase_extraction_results_with_filtering}
        présente les résultats de l'évaluation extrinsèque. La performance, en
        termes de précision, rappel et f-mesure, des méthodes \textsc{Tf-Idf} et
        \textsc{Kea} est reportée pour chacune des trois collections de données
        utilisées pour cette évaluation.
        \begin{table}[h!]
          \centering
          \resizebox{\linewidth}{!}{
            \begin{tabular}{r@{~}|c@{~~}c@{~~}c@{~}|@{~}c@{~~}c@{~~}c@{~}|@{~}c@{~~}c@{~~}c@{~}|@{~}c@{~~}c@{~~}c@{~}|@{~}c@{~~}c@{~~}c@{~}|@{~}c@{~~}c@{~~}c}
              \toprule
              \multirow{2}{*}[-2pt]{\textbf{Method}} & \multicolumn{6}{c@{~}|@{~}}{\textbf{DUC}} & \multicolumn{6}{c@{~}|@{~}}{\textbf{SemEval}} & \multicolumn{6}{c}{\textbf{DEFT}}\\
              \cline{2-19}
              & \multicolumn{3}{c@{~}|@{~}}{TF-IDF} & \multicolumn{3}{c@{~}|@{~}}{KEA} & \multicolumn{3}{c@{~}|@{~}}{TF-IDF} & \multicolumn{3}{c@{~}|@{~}}{KEA} & \multicolumn{3}{c@{~}|@{~}}{TF-IDF} & \multicolumn{3}{c}{KEA}\\
              \cline{2-19}
              & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F & P & R & F\\
              \hline
              n-grammes & 14.3 & 19.0 & 16.1$~~$ & 12.0 & 16.6 & 13.7$~~$ & $~~$9.0 & $~~$6.6 & $~~$7.2$~~$ & 19.4 & 13.7 & 15.9 & $~~$6.7 & 12.5 & $~~$8.6 & 13.4 & 25.3 & 17.3\\
              \texttt{/(N|A)+/} & 24.2 & 31.7 & 27.0$~~$ & \textbf{14.5} & 19.9 & 16.5$~~$ & 11.7 & $~~$7.9 & $~~$9.3$~~$ & 19.6 & 13.7 & 16.0 & $~~$9.5 & 17.6 & 12.1 & 14.1 & 26.3  &18.1\\
              \textit{NP-chunks} & 21.1 & 28.1 & 23.8$~~$ & 13.5 & 18.6 & 15.4$~~$ & 11.9 & $~~$8.0 & $~~$9.5$~~$ & 19.5 & 13.7 & 16.0 & $~~$9.6 & 17.9 & 12.3 & 14.3 & 26.8 & 18.4\\
              LR-NP & \textbf{24.3} & \textbf{32.0} & \textbf{27.2$^\dagger$} & \textbf{14.5} & \textbf{20.0} & \textbf{16.6$^\ddagger$} & \textbf{12.4} & \textbf{$~~$8.4} & \textbf{$~~$9.9$^\ddagger$} & \textbf{20.4} & \textbf{14.4} & \textbf{16.7}& \textbf{10.1} & \textbf{18.5} & \textbf{12.9} & \textbf{14.4} & \textbf{27.0} & \textbf{18.6}\\
              \bottomrule
            \end{tabular}
          }
          \caption[
            Résultats de \textsc{Tf-Idf} et \textsc{Kea} selon la
            méthode de sélection des termes-clés candidats utilisée
          ]{
            Résultats de \textsc{Tf-Idf} et \textsc{Kea} d'après la
            méthode de sélection des termes-clés candidats utilisée.
            $\ddagger$ indique une amélioration significative par rapport à
            toutes les autres méthodes de sélection de candidats et $\dagger$
            indique une amélioration significative par rapport à toutes les
            méthodes sauf celle qui extrait les \texttt{/(N|A)+/}, à 0.001 pour
            le t-test de Student.
           \label{tab:keyphrase_extraction_results_with_filtering}}
        \end{table}

        Globalement, la performance des méthodes d'extraction de termes-clés est
        corrélée à la qualité de l'ensemble des termes-clés candidats
        sélectionnés. Les candidats sélectionnés avec notre méthode induisent
        les meilleures performances dans les six cas de figure étudiés et, dans
        la moitié des cas, l'amélioration vis-à-vis des méthodes de référence
        est significative. Au delà de montrer la pertinence de filtrer certains
        adjectifs lors de la sélection des termes-clés candidats, les résultats
        montrent la validité de notre hypothèse par laquelle plus une méthode de
        sélection de candidats permet un rappel maximum élevé tout en limitant
        la quantité de termes-clés candidats sélectionnés, alors plus
        elle fournit un ensemble de candidats de bonne qualité et plus aisée
        sera l'extraction de termes-clés.

    \subsection{Bilan}
    \label{subsec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection-conclusion}
      Avec ce travail, nous proposons une méthode de sélection des termes-clés
      candidats d'un document. Développée à l'issue d'une analyse des propriétés
      linguistiques des termes-clés de référence de trois collections de
      données, notre méthode préselectionne des termes-clés candidats composés
      uniquement de noms et d'au plus un adjectif, puis détermine si l'adjectif
      des candidats apporte du sens selon sa catégorie (relationnel, composé ou
      qualificatif) et son usage dans le document. Vis-à-vis des méthodes de
      sélection de termes-clés candidats les plus utilisées, celle-ci présente
      l'avantage de sélectionner moins de candidats sans réduire
      significativement le nombre de candidats positifs qui s'y trouvent. La
      qualité de l'ensemble de candidats proposés est donc supérieure et permet
      de réduire aussi bien le temps de traitement pour l'extraction de
      termes-clés que les risques d'erreurs.

  %-----------------------------------------------------------------------------

  \section{Extraction automatique non supervisée de termes-clés}
  \label{sec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction}
    Dans cet section, nous présentons TopicRank, une méthode non supervisée à
    base de graphe pour l'extraction automatique de termes-clés. Ce travail se
    fonde sur celui de \newcite{mihalcea2004textrank}, qui ont proposé TextRank.
    Notre objectif est de résoudre les faiblesses de l'approche à base de
    graphe, que nous identifions dans le
    \ANNOTATE{chapitre~\ref{chap:main-state_of_the_art}
    (page~\pageref{chap:main-state_of_the_art})}{état de l'art}.

    Les méthodes à base de graphe actuelles déterminent l'importance des mots du
    document, puis utilisent cette importance soit pour générer les
    termes-clés~\cite{mihalcea2004textrank}, soit pour déterminer l'importance
    des termes-clés candidats en faisant la somme du score d'importance de leurs
    mots~\cite{wan2008expandrank}. Nous jugeons qu'il est plus pertinent
    d'utiliser l'algorithme d'ordonnancement à base de graphe pour ordonner
    directement les termes-clés candidats et ainsi éviter les problèmes de
    redondance que nous évoquons dans le
    \ANNOTATE{chapitre~\ref{chap:main-state_of_the_art}
    (page~\pageref{chap:main-state_of_the_art})}{état de l'art} et que nous montrons dans
    l'exemple \TODO{ref vers l'exemple à mettre dans l'état de l'art}
    (\TODO{page ???}).
    
    En plus de cela, les méthodes actuelles ne tiennent pas compte du phénomène
    de variation lexicale, qui consiste à utiliser des unités textuelles
    différentes d'un point de vue lexicale mais équivalentes d'un point de vue
    sémantique afin d'éviter les répétitions dans le texte. Ne pas tenir compte
    de ce phénomène engendre une dispersion, dans le graphe, d'informations
    relatives à un même sujet.
    
    Enfin, les expériences réalisées dans le travail de
    \newcite{mihalcea2004textrank} et dans celui de \newcite{wan2008expandrank}
    montrent un comportement marginal des différentes approches à base de graphe
    vis-à-vis de la valeur de la fenêtre de cooccurrence. Les performances
    relevées dans les expériences de \newcite{mihalcea2004textrank} montrent que
    plus la fenêtre de cooccurrence est élevée, moins l'extraction de
    termes-clés est performante, alors que celles relevées par
    \newcite{wan2008expandrank} montrent le comportement inverse. \TODO{mettre
    les courbes de Mihalcea et celles de Wan}

    Avec TopicRank, nous proposons une solution pour résoudre les trois
    problèmes énoncés ci-dessus. Dans la suite, nous présentons TopicRank, nous
    l'évaluons, nous le comparons à l'existant, puis nous en analysons les
    erreurs.

    \subsection{TopicRank}
    \label{subsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-topicrank}
      TopicRank est une méthode à base de graph pour extraire des termes-clés
      représentant chacun un sujet important dans le document.
      % Quel en est le fonctionnement général ?
      Elle repose sur les quatre étapes suivantes, qui sont détaillées dans
      la suite~: identification des sujets, construction d'un graphe de sujets,
      ordonnancement des sujets et sélection du terme-clé candidat le plus
      représentatif de chaque sujet.

      \subsubsection{Identification des sujets}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-topicrank-topic_identification}
        Dans ce travail, un sujet représente un thème du document qui est
        véhiculé par une ou plusieurs unités textuelles partageant le même
        sens. Il s'agit d'un groupe de termes-clés candidats qui partagent le
        plus d'unités de sens, de mots.

        % Que nous faut-il pour identifier les sujets ?
        La première étape de l'identification des sujets consiste à sélectionner
        les termes-clés candidats.
        % Quels candidats composent les sujets ?
        Pour ce travail, nous suivons \newcite{wan2008expandrank} et
        sélectionnons les plus longues séquences de noms, de noms propres et
        d'adjectifs à partir du patron grammatical suivant~:\texttt{/(N|A)+}.
        Celui-ci présente l'avantage d'être simple et adapté à plusieurs
        langues, telles que les langues latines (anglais, français, etc.),
        lorsque les outils d'étiquetage grammatical sont disponibles pour la
        langue concernée. De plus, ce patron est gourmand, c'est-à-dire qu'il
        capture les séquences les plus longues qui le respectent, et il est donc
        adapté pour le groupement que nous effectuons ensuite.

        % Comment détectons nous deux candidats appartenant au même sujet ?
        La seconde étape de l'identification des sujets consiste à grouper les
        termes-clés candidats lorsqu'ils appartiennent au même sujet. Afin de
        proposer une méthode qui n'utilise pas de données
        supplémentaires, nous optons pour un groupement naïf des
        candidats. Deux candidats $c_1$ et $c_2$ sont groupés selon leur degré
        de similarité de Jaccard. Ils sont considérés comme des sacs de
        mots tronqués par la méthode de racinisation\footnote{Cette
        racinisation a pour effet de grouper les candidats qui varient
        uniquement en termes de flexion ou de dérivation.} de
        \newcite{porter1980suffixstripping} et leur degré de similarité est
        d'autant plus élevé qu'ils partagent de mots racinisés~:
        \begin{align}
          \text{sim}(c_1, c_2) &= \frac{|\textnormal{Porter}(c_1)\ \cap\ \textnormal{Porter}(c_2)|}{|\textnormal{Porter}(c_1)\ \cup\ \textnormal{Porter}(c_2)|} \label{equa:jaccard}
        \end{align}
        Cette mesure est naïve, car l'ordre des mots, leur ambiguïté
        et leur synonymie ne sont pas pris en compte. À cela s'ajoute
        aussi des erreurs introduites par l'usage de la méthode de
        \newcite{porter1980suffixstripping} (par exemple les mots
        \og{}empire\fg{} et \og{}empirique\fg{} partagent le même radical
        \og{}empir\fg{}).

        % Comment groupons nous les candidats d'un même sujet ?
        La similarité est calculée entre toutes les paires de candidats. Nous
        appliquons l'algorithme de groupement hiérarchique agglomératif
        (\textit{Hierarchical Agglomerative Clustering -- \textsc{HAC}}) pour
        grouper les candidats les plus similaires. Initialement, chaque candidat
        représente un groupe et, jusqu'à l'obtention d'un nombre prédéfini de
        groupes, ceux ayant la plus forte similarité sont unis pour n'en former
        qu'un seul. Afin de ne pas fixer le nombre de sujets à créer comme
        condition d'arrêt de l'algorithme, nous définissons un seuil de
        similarité $\zeta$ entre les groupes deux à deux. Cette similarité entre
        deux groupes est déterminée à partir de la similarité de Jaccard
        calculée entre les candidats de chaque groupe (\TODO{algorithme}). Il
        existe trois stratégies pour calculer la similarité entre deux groupes~:
        \begin{itemize}
          \item{simple~: la plus grande valeur de similarité entre les candidats
                des deux groupes sert de similarité entre eux~;}
          \item{complète~: la plus petite valeur de similarité entre les
                candidats des deux groupes sert de similarité entre eux~;}
          \item{moyenne~: la moyenne de toutes les similarités entre les
                candidats des deux groupes sert de similarité entre eux
                (compromis entre les stratégies simple et complète).}
        \end{itemize}
        L'une ou l'autre de ces stratégies est à privilégier en fonction du type
        des candidats extraits. Pour des candidats qui ont de forts
        recouvrements, tels que les n-grammes, il serait plus pertinent
        d'utiliser la stratégie complète qui est la moins agglomérative. Dans le
        cas de TopicRank, où les candidats sont de meilleure qualité que les
        n-grammes, la stratégie moyenne est une meilleure alternative.

        \TODO{Exemple étape par étape}
        \TODO{illustrer les stratégies dans l'exemple (ou un autre ?)}

      \subsubsection{Construction du graphe}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-topicrank-graph_construction}
        Afin d'identifier les sujets les plus importants du documents, nous
        utilisons un graphe qui représente tous les sujets du document avec les
        relations qu'ils entretiennent.

        % Comment le graphe est-il construit ?
        Soit le graphe complet $G = (N, A)$ non orienté, composé d'un ensemble
        de n\oe{}uds $N$ et d'arêtes $A$\footnote{$A = \{(n_1, n_2)\ |\
        \forall{n_1, n_2 \in N}, n_1 \neq n_2\}$, car $G$ est un graphe
        complet.}. Les sujets sont représentés par les n\oe{}uds du graphe et
        les arêtes qui les connectent représentent la force de leur lien
        sémantique. Contrairement aux travaux précédent, nous ne souhaitons pas
        utiliser de fenêtre de cooccurrence et ne pouvons donc pas exprimer la
        force du lien sémantique entre deux sujets par leur nombre de
        cooccurrences. Pour préserver l'intuition derrière l'usage du nombre de
        cooccurrences, nous connectons tous les n\oe{}uds deux à deux et
        exprimons la force de leur lien sémantique à partir de la distance (en
        nombre de mots) qui les sépare dans le document~:
        \begin{align}
          \text{poids}(n_i, n_j) &= \sum_{c_i \in n_i}\ \sum_{c_j \in n_j} \text{dist}(c_i, c_j) \label{math:ponderation}\\
          \text{dist}(c_i, c_j) &= \sum_{p_i \in \text{pos}(c_i)}\ \sum_{p_j \in \text{pos}(c_j)} \frac{1}{|p_i - p_j|} \label{math:distance}
        \end{align}
        où $\text{poids}(n_i, n_j)$ est le poids de l'arête entre les sujets
        $n_i$ et $n_j$, et où $\text{dist}(c_i, c_j)$ représente la force
        sémantique entre les candidats $c_i$ et $c_j$, calculée à partir de
        leurs positions respectives, $\text{pos}(c_i)$ et $\text{pos}(c_j)$,
        dans le document.

      \subsubsection{Ordonnancement des sujets}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-topicrank-topic_ranking}
        % Quel est le but de l'ordonnancement ?
        % Comment est-il effectué ?
        L'ordonnancement des sujets doit établir un ordre d'importance des
        sujets du document.
        % Comment le graphe est-il utilisé pour ordonner les sujets ?
        % Quelle est l'intuition de PageRank/TextRank ?
        Pour cela, nous appliquons l'algorithme d'ordonnancement de
        SingleRank~\cite{wan2008expandrank} à notre graphe
        de sujets. Cet algorithme se fonde sur le principe de recommandation,
        ou de vote, c'est-à-dire un sujet est d'autant plus important s'il est
        fortement connecté avec un grand nombre de sujets et si les sujets avec
        lesquels il est fortement connecté sont importants~:
        \begin{align}
          S(n_i) &= (1 - \lambda) + \lambda \times \sum_{n_j \in A(n_i)} \frac{\text{poids}(n_j, n_i) \times S(n_j)}{\mathlarger{\sum}_{n_k \in A(n_j)} \text{poids}(n_i, n_j)}
        \end{align}
        où $A(n_i)$ est l'ensemble des sujets\footnote{$A(n_i) = \{n_j\ |\
        \forall{n_j \in N}, n_j \neq n_i\}$, car $G$ est un graphe complet.}
        connectés au sujet $n_i$ et où $\lambda$ est un facteur d'atténuation.
        Défini entre 0 et 1, ce dernier peut être considéré comme la probabilité
        pour que le sujet $n_i$ soit utilisé par recommandation. Nous suivons
        \newcite{brin1998pagerank} et fixons $\lambda$ à 0,85.

      \subsubsection{Sélection des termes-clés}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-topicrank-keyphrase_selection}
        % De quoi s'agit-il ?
        La sélection des termes-clés est la dernière étape de TopicRank. Elle
        consiste à chercher les termes-clés candidats qui représentent le mieux
        les sujets importants. Dans le but de ne pas extraire de termes-clés
        redondants, un seul candidat est sélectionné par sujet.
        % Quel en est le but ?
        Ainsi, pour $k$ sujets, $k$ termes-clés non redondants couvrant
        exactement $k$ sujets sont extraits.

        % Quelles sont les différentes stratégies envisageable ?
        La difficulté de ce principe de sélection réside dans la capacité à
        trouver parmi plusieurs termes-clés candidats d'un même sujet celui qui
        le représente le mieux. Nous proposons trois stratégies de sélection
        pouvant répondre à ce problème~:
        \begin{itemize}
          \item{position~: en supposant qu'un sujet est tout d'abord
                introduit par sa forme la plus appropriée, le terme-clé
                candidat sélectionné pour un sujet est celui qui apparaît en
                premier dans le document~;}
          \item{fréquence~: en supposant que la forme la plus représentative
                d'un sujet est sa forme la plus fréquente, le terme-clé candidat
                sélectionné pour un sujet est celui qui est le plus fréquent
                dans le document~;}
          \item{centroïde~: le terme-clé candidat sélectionné pour un sujet
                est celui qui est le plus similaire aux autres candidats du
                sujet (voir l'équation~\ref{equa:jaccard}).}
        \end{itemize}
        % Laquelle des trois stratégies semble être la mieux ?
        Parmi ces trois stratégies, celle qui semble la plus appropriée est la
        stratégie position. Sélectionner les candidats les plus fréquents risque
        de ne pas être une solution stable selon les genres de documents, en
        particulier selon leur taille~; sélectionner les centroïdes risque de ne
        pas fournir les termes-clés les plus précis (informatif), car celui-ci
        représente le tronc commun à la majorité des candidats.

      \subsubsection{Exemple}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-topicrank-example}
        \TODO{exemple}

    \subsection{Évaluation}
    \label{subsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation}
      Pour valider notre approche, nous réalisons deux série d'expériences. Une
      première série pour déterminer les paramètres optimaux de TopicRank et
      une seconde séries pour le comparer aux travaux précédents, ainsi
      que pour analyser l'impact de chacune de nos contributions.
      
      \subsubsection{Méthodes de référence}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation-baselines}
        % Comment les baselines sont-elles choisies ?
        Dans nos expériences, nous comparons TopicRank à trois autres
        méthodes non supervisées d'extraction automatique de termes-clés. Nous
        choisissons TextRank et SingleRank, les deux méthodes qui sont la
        fondation des méthodes à base de graphe, et la méthode TF-IDF.

      \subsubsection{Collections de données}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation-evaluation_data}
        Pour évaluer ce travail, nous utilisons quatre des cinq collections dont
        nous disposons\footnote{La collection de données Termith est utilisée
        pour évaluer manuellement nos travaux. Les évaluations manuelles sont
        présentées dans le chapitre \TODO{6} (page \TODO{}).}. Nous utilisons
        les deux collections d'articles journalistiques \textsc{Duc} (anglais)
        et Wikinews (français), ainsi que les deux collections d'articles
        scientifiques SemEval (anglais) et \textsc{Deft} (français).
        Le tableau~\ref{tab:topicrank:corpora_recap} rappelle les
        caractéristiques des collections de données que nous utilisons pour
        évaluer TopicRank.
        \begin{table}[!h]
          \centering
          \resizebox{\linewidth}{!}{
            \begin{tabular}{l@{~}l|c@{~~}c@{~~}c@{~~}c|c@{~~}c@{~~}c@{~~}c}
              \toprule
              \multicolumn{2}{l|}{\multirow{2}{*}{\textbf{Corpus}}} & \multicolumn{4}{c|}{\textbf{Documents}} & \multicolumn{4}{c}{\textbf{Termes-clés}}\\
              \cline{3-10}
              & & Langue & Genre & Quantité & Mots moy. & Annotateur & Quantité moy. & \og{}À assigner\fg{} & Mots moy.\\
              \hline
              \multicolumn{2}{l|}{\textsc{Deft}} & & & & & & & &\\
              $\drsh$ & Appr. & Français & Scientifique & 141 & 7~276,7 & Auteur & 5,4 & 18,2~\% & 1,7\\
              $\drsh$ & Test & \ditto & \ditto & 93 & 6~839,4 & \ditto & 5,2 & 21,1~\% & 1,6\\
              \multicolumn{2}{l|}{Wikinews} & \ditto & Journalistique & 100 & 308,5 & Lecteur & 9,6 & 7,6~\% & 1,7\\
              \multicolumn{2}{l|}{Semeval} & & & & & & & &\\
              $\drsh$ & Appr. & Anglais & Scientifique & 144 & 5~134,6 & Auteur~/~Lecteur & 15,4 & 13,5~\% & 2,1\\
              $\drsh$ & Test & \ditto & \ditto & 100 & 5~177,7 & \ditto & 14,7 & 22,1~\% & 2,1\\
              \multicolumn{2}{l|}{\textsc{Duc}} & \ditto & Journalistique & 308 & 900,7 & Lecteur & 8,1 & 3,5~\% & 2,1\\
              \bottomrule
            \end{tabular}
          }

          \caption{Rappel des corpus utilisés pour l'évaluation de TopicRank
                   \label{tab:topicrank:corpora_recap}}
        \end{table}

      \subsubsection{Prétraitement}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation-preprocessing}
        Les documents de chaque collection de données subissent les mêmes
        prétraitements. Ils sont tout d'abord segmentées en phrases, les phrases
        sont segmentés en mots, puis les mots sont étiquetés grammaticalement.
        La segmentation en phrases est réalisée avec la classe
        \texttt{PunktSentenceTokenizer} du module Python
        \textsc{Nltk}~\cite[\textit{Natural Language ToolKit}]{bird2009nltk}~;
        La segmentation des phrases en mots est mise en \oe{}uvre avec la classe
        \texttt{TreeBankWordTokenizer} de \textsc{Nltk} pour l'anglais et avec
        l'outil Bonsai, du \textit{Bonsai \textsc{Pcfg-La}
        parser}\footnote{\url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}},
        pour le français~; L'étiquetage grammatical, il est effectué avec le
        \textit{Stanford \textsc{Pos}
        tagger}~\cite{toutanova2003stanfordpostagger} pour l'anglais et avec
        l'outil MElt~\cite{denis2009melt} pour le français. Tous ces outils sont
        utilisés avec leur configuration par défaut.

      \subsubsection{Mesures d'évaluation}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation-evaluation_measures}
        Les performances des méthodes d'extraction de termes-clés sont exprimées
        en termes de précision (P), rappel (R) et f-mesure (f1-mesure, F). En
        accord avec l'évaluation menée dans les travaux précédents, nous
        considérons correcte l'extraction d'une variante flexionnelle d'un
        terme-clé de référence~\cite{kim2010semeval}. Les opérations de
        comparaison entre les termes-clés de référence et les termes-clés
        extraits sont donc effectuées à partir de la racine des mots qui les
        composent, selon la méthode de \newcite{porter1980suffixstripping}.

      \subsubsection{Analyse empirique de TopicRank}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation-empirical_analysis_of_topicrank}
        Dans cette section, nous effectuons une première série d'expériences
        afin de déterminer quelle est la configuration optimale de TopicRank. En
        utilisant les ensembles d'entraînement de SemEval et de \textsc{Deft},
        nous réalisons deux expériences durant lesquelles nous faisons varier,
        dans un premier temps, le seuil de similarité ($\zeta$) et la stratégie
        de groupement (simple, complète et moyenne), puis dans un second temps,
        la stratégie de sélection du terme-clé candidat le plus représentatif de
        chacun des sujets les plus importants.
        \begin{figure}
          \centering
          \subfigure[SemEval]{
            \begin{tikzpicture}
              \pgfkeys{/pgf/number format/.cd, use comma, fixed}
              \begin{axis}[axis lines=middle,
                           x=0.37\linewidth,
                           xtick={0.0, 0.2, ..., 1.2},
                           xmin=0.0,
                           xmax=1.05,
                           xlabel=$\zeta$,
                           x label style={anchor=west},
                           y=0.012\textheight,
                           ytick={0, 5, 10, 15},
                           ymin=0,
                           ymax=18,
                           ylabel=F,
                           y label style={anchor=south}]
                % simple
                \addplot[green!66, mark=x] coordinates{
                  (0.05, 4.2)
                  (0.10, 4.2)
                  (0.15, 4.4)
                  (0.20, 4.7)
                  (0.25, 4.9)
                  (0.30, 5.3)
                  (0.35, 7.4)
                  (0.40, 7.4)
                  (0.45, 7.3)
                  (0.50, 7.3)
                  (0.55, 7.5)
                  (0.60, 7.5)
                  (0.65, 7.5)
                  (0.70, 8.0)
                  (0.75, 8.0)
                  (0.80, 8.0)
                  (0.85, 8.0)
                  (0.90, 8.0)
                  (0.95, 8.0)
                  (1.00, 8.0)
                };
                % complet
                \addplot[cyan!66, mark=+] coordinates{
                  (0.05, 11.5)
                  (0.10, 11.5)
                  (0.15, 10.9)
                  (0.20, 10.3)
                  (0.25, 9.3)
                  (0.30, 7.8)
                  (0.35, 7.6)
                  (0.40, 7.6)
                  (0.45, 7.5)
                  (0.50, 7.5)
                  (0.55, 7.5)
                  (0.60, 7.5)
                  (0.65, 7.6)
                  (0.70, 8.0)
                  (0.75, 8.0)
                  (0.80, 8.0)
                  (0.85, 8.0)
                  (0.90, 8.0)
                  (0.95, 8.0)
                  (1.00, 8.0)
                };
                % moyen
                \addplot[red!66, mark=o] coordinates{
                  (0.05, 12.2)
                  (0.10, 11.7)
                  (0.15, 11.2)
                  (0.20, 11.4)
                  (0.25, 11.1)
                  (0.30, 10.2)
                  (0.35, 8.7)
                  (0.40, 7.7)
                  (0.45, 7.7)
                  (0.50, 7.5)
                  (0.55, 7.6)
                  (0.60, 7.6)
                  (0.65, 7.5)
                  (0.70, 8.0)
                  (0.75, 8.0)
                  (0.80, 8.0)
                  (0.85, 8.0)
                  (0.90, 8.0)
                  (0.95, 8.0)
                  (1.00, 8.0)
                };
                \draw[thick] ({axis cs:0.05,0}|-{rel axis cs:0,1}) -- ({axis cs:0.05,0}|-{rel axis cs:0,0}) [color=red!66];
                \draw[densely dashed] ({axis cs:0.20,0}|-{rel axis cs:0,1}) -- ({axis cs:0.20,0}|-{rel axis cs:0,0}) [color=black!66];
                \node at (axis cs:0.05,17.5) [color=red!66, anchor=west] {\tiny{0,05}};
                \node at (axis cs:0.20,17.5) [color=black!66, anchor=west] {\tiny{0,20}};
                \legend{Simple, Complète, Moyenne}
              \end{axis}
            \end{tikzpicture}
          }
          \subfigure[\textsc{Deft}]{
            \begin{tikzpicture}
              \pgfkeys{/pgf/number format/.cd, use comma, fixed}
              \begin{axis}[axis lines=middle,
                           x=0.37\linewidth,
                           xtick={0.0, 0.2, ..., 1.2},
                           xmin=0.0,
                           xmax=1.05,
                           xlabel=$\zeta$,
                           x label style={anchor=west},
                           y=0.012\textheight,
                           ytick={0, 5, 10, 15},
                           ymin=0,
                           ymax=18,
                           ylabel=F,
                           y label style={anchor=south}]
                % simple
                \addplot[green!66, mark=x] coordinates{
                  (0.05, 8.6)
                  (0.10, 8.6)
                  (0.15, 8.6)
                  (0.20, 8.6)
                  (0.25, 8.6)
                  (0.30, 8.9)
                  (0.35, 11.1)
                  (0.40, 11.1)
                  (0.45, 11.2)
                  (0.50, 11.2)
                  (0.55, 15.3)
                  (0.60, 15.3)
                  (0.65, 15.3)
                  (0.70, 15.6)
                  (0.75, 15.6)
                  (0.80, 15.6)
                  (0.85, 15.6)
                  (0.90, 15.6)
                  (0.95, 15.6)
                  (1.00, 15.6)
                };
                % complet
                \addplot[cyan!66, mark=+] coordinates{
                  (0.05, 15.8)
                  (0.10, 15.8)
                  (0.15, 15.8)
                  (0.20, 15.8)
                  (0.25, 15.9)
                  (0.30, 15.5)
                  (0.35, 16.1)
                  (0.40, 16.1)
                  (0.45, 16.1)
                  (0.50, 16.1)
                  (0.55, 15.6)
                  (0.60, 15.6)
                  (0.65, 15.6)
                  (0.70, 15.6)
                  (0.75, 15.6)
                  (0.80, 15.6)
                  (0.85, 15.6)
                  (0.90, 15.6)
                  (0.95, 15.6)
                  (1.00, 15.6)
                };
                % moyen
                \addplot[red!66, mark=o] coordinates{
                  (0.05, 13.8)
                  (0.10, 13.9)
                  (0.15, 14.9)
                  (0.20, 15.4)
                  (0.25, 15.2)
                  (0.30, 15.3)
                  (0.35, 15.3)
                  (0.40, 15.5)
                  (0.45, 15.8)
                  (0.50, 15.9)
                  (0.55, 15.4)
                  (0.60, 15.5)
                  (0.65, 15.6)
                  (0.70, 15.6)
                  (0.75, 15.6)
                  (0.80, 15.6)
                  (0.85, 15.6)
                  (0.90, 15.6)
                  (0.95, 15.6)
                  (1.00, 15.6)
                };
                \draw[thick] ({axis cs:0.50,0}|-{rel axis cs:0,1}) -- ({axis cs:0.50,0}|-{rel axis cs:0,0}) [color=red!66];
                \draw[densely dashed] ({axis cs:0.20,0}|-{rel axis cs:0,1}) -- ({axis cs:0.20,0}|-{rel axis cs:0,0}) [color=black!66];
                \node at (axis cs:0.50,17.5) [color=red!66, anchor=west] {\tiny{0,50}};
                \node at (axis cs:0.20,17.5) [color=black!66, anchor=west] {\tiny{0,20}};
              \end{axis}
            \end{tikzpicture}
          }
          \caption[Résultats de l'extraction de dix termes-clés avec TopicRank,
                   en fonction de la stratégie de regroupement et de la valeur
                   du seuil de similarité $\zeta$]{
            Résultats de l'extraction de dix termes-clés avec TopicRank, en
            fonction de la stratégie de regroupement et de la valeur du seuil
            de similarité $\zeta$, sur les ensembles d'entraînement de
            SemEval et de \textsc{Deft}
            \label{fig:variation_du_seuil_de_similarite}
          }
        \end{figure}

        % Variation du seuil de similarité et de la stratégie de groupement
        La figure~\ref{fig:variation_du_seuil_de_similarite} présente les
        résultats de TopicRank lorsque nous faisons varier le seuil~$\zeta$ avec
        un pas de 0,05 pour toutes les stratégies de groupement\footnote{La
        stratégie de sélection du terme-clé le plus représentatif par sujet
        utilisée dans cette expérience est la stratégie position.}.
        % Quelle analyse peut-on faire à partir des courbes ?
        Globalement, chaque stratégie de groupement a un comportement qui lui
        est propre jusqu'à un certain point de convergence lorsque $\zeta$ vaut
        0,70, ce point de convergence correspondant à la valeur du seuil $\zeta$
        pour laquelle les sujets créés sont les mêmes quelle que soit la
        stratégie. Avec la stratégie simple, les résultats s'améliorent lorsque
        $\zeta$ augmente. Du fait qu'elle ne prend en compte que la similarité
        maximale entre deux candidats de deux groupes, cette stratégie à
        tendance à trop grouper et donc à créer des groupes contenant en réalité
        plusieurs sujets. L'augmentation du seuil $\zeta$ a pour effet de
        restreindre cette tendance et la qualité du groupement s'améliore. En
        opposition, la stratégie complète, qui a le fonctionnement inverse, voit
        ses résultats se dégrader lorsque $\zeta$ augmente. Enfin, la stratégie
        moyenne agit en compromis. Pour SemEval, son comportement est le même
        que celui de la stratégie complète, mais ses résultats sont supérieurs
        jusqu'au point de convergence. Pour \textsc{Deft}, son comportement est
        le même que celui de la stratégie simple, mais ses résultats sont très
        supérieurs jusqu'au point de convergence.
        % Quels sont les paramètres utilisés ?
        Après observation des résultats de cette expérience, nous décidons
        d'utiliser la stratégie de groupement moyenne avec un seuil $\zeta$ de
        0,20 pour toutes les expériences suivantes.

        La figure~\ref{fig:variation_de_la_selection_des_candidats} présente les
        résultats obtenus avec TopicRank et les différentes stratégies de
        sélection d'un terme-clé candidat par sujet. Les résultats confirment
        notre hypothèse qui est que le choix des candidats apparaissant en
        premier dans le document fournit de meilleurs termes-clés que le choix
        des candidats centroïdes ou des candidats les plus fréquents. La
        stratégie centroïde donne de très faibles résultats et la
        stratégie fréquence n'est pas stable comparée à la stratégie position.
        Enfin, bien que la stratégie position donne les résultats les plus
        satisfaisants, nous remarquons qu'il existe encore une marge de
        progression importante. Les valeurs indiquées par la borne haute
        représentent les résultats qui pourraient être obtenus avec un oracle.
        Pour chacun des sujets les plus importants, l'oracle sélectionne
        toujours un candidat positif, s'il y en a un. La marge de progression de
        14,8 points de f-mesure pour SemEval et de 5,4 points de f-mesure pour
        \textsc{Deft} est encourageante pour des travaux futurs.
        \begin{figure}
          \centering
          \begin{tikzpicture}
            \pgfkeys{/pgf/number format/.cd, use comma, fixed}
            \begin{axis}[axis lines=left,
                         symbolic x coords={SemEval, DEFT},
                         xtick=data,
                         xticklabels={SemEval, \textsc{Deft}},
                         enlarge x limits=0.5,
                         x=.3\linewidth,
                         nodes near coords,
                         nodes near coords align={vertical},
                         every node near coord/.append style={font=\tiny},
                         y=0.004\textheight,
                         ytick={0, 10, ..., 50},
                         ymin=0,
                         ymax=52.5,
                         ybar=8pt,
                         ylabel=F,
                         ylabel style={at={(ticklabel* cs:1)},
                                       anchor=south,
                                       rotate=270}]%,
                         %legend style={at={(0.5,-0.15)},
                         %              anchor=north,
                         %              legend columns=-1}]
              % centroïde
              \addplot[green!66,
                       pattern=north east lines,
                       pattern color=green!40] coordinates{
                (SemEval,   2.6)
                (DEFT,      4.7)
              };
              % fréquence
              \addplot[cyan!66,
                       pattern=north west lines,
                       pattern color=cyan!40] coordinates{
                (SemEval,   7.5)
                (DEFT,      14.2)
              };
              % position
              \addplot[black!66,
                       pattern=horizontal lines,
                       pattern color=black!40] coordinates{
                (SemEval,   11.4)
                (DEFT,      15.4)
              };
              % borne haute
              \addplot[red!66,fill=red!40] coordinates{
                (SemEval,   26.2)
                (DEFT,      20.8)
              };

              \legend{Centroïde, Fréquence, Position, Borne haute}
            \end{axis}
          \end{tikzpicture}
          \caption{Résultats de l'extraction de dix termes-clés, avec TopicRank,
                   en fonction des différentes stratégies de sélections d'un
                   terme-clé candidats par sujet
                   \label{fig:variation_de_la_selection_des_candidats}}
        \end{figure}

      \subsubsection{Paramétrage empirique de SingleRank}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation-empirical_setting_of_singlerank}
        Contrairement aux autres méthodes de référence, SingleRank possède un
        paramètre qui est définit arbitrairement~: la fenêtre de cooccurrences
        fixée à dix par \newcite{wan2008expandrank}. De même que pour TopicRank,
        nous utilisons les ensembles d'entrainement de SemEval et de
        \textsc{Deft} pour déterminer qu'elle est la valeur optimale de la
        fenêtre de cooccurrences pour SingleRank\footnote{Nous ne répétons pas
        cette expérience pour TextRank, car le critère d'adjacence (fenêtre de
        valeur 2) est un critère fort dans la méthode TextRank.}. 

        La figure~\ref{fig:variation_de_la_fenetre} présente les résultats de
        SingleRank lorsque nous faisons varier la fenêtre de cooccurrences de
        deux à vingt mots, avec un pas de un. Globalement, nous observons une
        stabilité des performances de SingleRank quelle que soit la valeur
        utilisée pour la fenêtre de cooccurrences, avec des résultats optimaux
        obtenus lorsque celle-ci vaut 12. Dans les expériences suivantes, nous
        fixons donc la valeur de la fenêtre de cooccurrences à 12.
        \begin{figure}
          \centering
          \begin{tikzpicture}
            \begin{axis}[axis lines=middle,
                         x=0.025\linewidth,
                         xtick={2, 4, 6, 8, 10, 12, 14, 16, 18, 20},
                         xmin=1,
                         xmax=21,
                         xlabel=Fenêtre,
                         x label style={anchor=west},
                         y=0.012\textheight,
                         ytick={0, 5, 10, 15},
                         ymin=0,
                         ymax=18,
                         ylabel=F,
                         y label style={anchor=south}]
              % semeval
              \addplot[cyan!66, mark=+] coordinates{
                (2, 5.0)
                (3, 5.2)
                (4, 4.8)
                (5, 5.1)
                (6, 5.0)
                (7, 4.9)
                (8, 4.9)
                (9, 4.9)
                (10, 5.1)
                (11, 5.1)
                (12, 5.2)
                (13, 5.2)
                (14, 5.2)
                (15, 5.2)
                (16, 5.2)
                (17, 5.2)
                (18, 5.1)
                (19, 5.2)
                (20, 5.2)
              };
              % deft
              \addplot[red!66, mark=o] coordinates{
                (2, 3.5)
                (3, 6.1)
                (4, 6.3)
                (5, 6.4)
                (6, 6.8)
                (7, 6.7)
                (8, 6.9)
                (9, 6.9)
                (10, 7.1)
                (11, 7.2)
                (12, 7.1)
                (13, 7.0)
                (14, 7.0)
                (15, 7.0)
                (16, 7.0)
                (17, 7.3)
                (18, 7.3)
                (19, 7.3)
                (20, 7.1)
              };
              \draw[densely dashed] ({axis cs:12,0}|-{rel axis cs:0,1}) -- ({axis cs:12,0}|-{rel axis cs:0,0}) [color=black!66];
              \node at (axis cs:12,17.5) [color=black!66, anchor=west] {\tiny{12}};
              \legend{SemEval, \textsc{Deft}}
            \end{axis}
          \end{tikzpicture}
          \caption{Résultats de l'extraction de dix termes-clés, avec
                   SingleRank, en fonction de la fenêtre de cooccurrences
                   \label{fig:variation_de_la_fenetre}}
        \end{figure}

      \subsubsection{Comparaison de TopicRank avec l'existant}
      \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation-comparison}
        % Que représente le tableau ?
        Le tableau~\ref{tab:resultats_globaux} montre les performances de
        TopicRank comparées à celles des trois méthodes de référence. De manière
        générale, les performances des méthodes d'extraction de termes-clés sont
        basses. De plus, il est avéré que les documents de grande taille, tels
        que ceux de SemEval et de \textsc{Deft}, sont plus difficiles à traiter que les
        autres documents. \newcite{hasan2014state_of_the_art} explique qu'un
        grand nombre de termes-clés candidats sont sélectionnés dans ces
        documents (ils sont en moyenne 647 pour SemEval et 915 pour
        \textsc{Deft}), ce qui augmente la difficulté de l'extraction de
        termes-clés.

        % Que peut-on dire globalement ?
        Globalement, TopicRank donne de meilleurs résultats que les méthodes de
        référence utilisées.
        % Que peut-on dire de plus ? (analyse plus approfondie)
        Comparée à la méthode TF-IDF, TopicRank donne de meilleurs résultats pour
        SemEval, Wikinews et \textsc{Deft}. Cette supériorité vis-à-vis de TF-IDF est
        importante à noter, car cette méthode obtient de bons résultats en
        tirant parti de statistiques extraites de documents supplémentaires,
        alors que TopicRank n'utilise que le document à analyser. Comparé aux
        autres méthodes à base de graphe, TopicRank donne des résultats
        significativement meilleurs pour SemEval, Wikinews et \textsc{Deft}. Ceci
        confirme donc que le groupement des candidats permet de rassembler des
        informations pour améliorer la précision de l'ordonnancement. En ce qui
        concerne \textsc{Duc}, notre méthode est aussi significativement meilleure que
        TextRank, mais elle ne l'est pas vis-à-vis de SingleRank. D'après la
        borne haute, l'une des raisons à la plus faible performance de TopicRank
        pour \textsc{Duc} est que la stratégie de sélection des candidats les plus
        représentatifs des sujets est moins adaptée. En effet, la différence
        avec la borne haute est de 12,9 points de f-mesure. Une analyse plus
        approfondie des différents apports de TopicRank peut aussi aider à
        comprendre les raisons de ses moins bons résultats.
        \begin{table}
          \centering
          \begin{tabular}{l|c@{~~}c@{~~}c@{~}|c@{~~}c@{~~}c@{~}|c@{~~}c@{~~}c@{~}|c@{~~}c@{~~}c@{~}}
            \toprule
            \multirow{2}{*}[-2pt]{\textbf{Méthode}} & \multicolumn{3}{c|}{\textbf{\textsc{Duc}}} & \multicolumn{3}{c|}{\textbf{SemEval}} & \multicolumn{3}{c|}{\textbf{Wikinews}} & \multicolumn{3}{c}{\textbf{\textsc{Deft}}}\\
            \cline{2-4}\cline{5-7}\cline{8-10}\cline{11-13}
            & P & R & F & P & R & F & P & R & F & P & R & F\\
            \hline
            TF-IDF & \textbf{23,8} & \textbf{30,7} & \textbf{26,4}$^{~}$ & 13,2 & $~~$8,9 & 10,5$^{~}$ & 33,9 & 35,9 & 34,3$^{~}$ & 10,3 & 19,1 & 13,2$^{~}$\\
            TextRank & $~~$4,9 & $~~$5,4 & $~~$5,0$^{~}$ & $~~$7,9 & $~~$4,5 & $~~$5,6$^{~}$ & $~~$9,3 & $~~$8,3 & $~~$8,6$^{~}$ & $~~$4,9 & $~~$7,1 & $~~$5,7$^{~}$\\
            SingleRank & 22,6 & 28,8 & 25,0$^{~}$ & $~~$4,8 & $~~$3,3 & $~~$3,9$^{~}$ & 19,2 & 20,4 & 19,5$^{~}$ & $~~$4,7 & $~~$9,4 & $~~$6,2$^{~}$\\
            TopicRank & 18,2 & 23,2 & 20,1 & \textbf{15,1}$^{~}$ & \textbf{10,6} & \textbf{12,3}$^\dagger$ & \textbf{34,8} & \textbf{37,3} & \textbf{35,4}$^\dagger$ & \textbf{11,3} & \textbf{21,0} & \textbf{14,5}$^\dagger$\\
            \hline
            \textbf{Borne haute} & \textbf{31,6} & \textbf{35,3} & \textbf{33,0}$^{~}$ & \textbf{33,8} & \textbf{23,3} & \textbf{27,3}$^{~}$ & \textbf{41,7} & \textbf{44,1} & \textbf{42,2}$^{~}$ & \textbf{14,5} & \textbf{27,0} & \textbf{18,7}$^{~}$\\
            \bottomrule
          \end{tabular}
          \caption[Résultats de l'extraction de dix termes-clés avec TF-IDF,
                   TextRank, SingleRank et TopicRank]{
            Résultats de l'extraction de dix termes-clés avec TF-IDF, TextRank,
            SingleRank et TopicRank. $\dagger$ indique une amélioration
            significative de TopicRank vis-à-vis de TextRank et SingleRank, à
            0,001 pour le t-test de Student.
            \label{tab:resultats_globaux}
          }
        \end{table}

        \begin{table}
          \centering
          \begin{tabular}{l|c@{~~}c@{~~}c@{~}|c@{~~}c@{~~}c@{~}|c@{~~}c@{~~}c@{~}|c@{~~}c@{~~}c@{~}}
            \toprule
            \multirow{2}{*}[-2pt]{\textbf{Méthode}} & \multicolumn{3}{c|}{\textbf{\textsc{Duc}}} & \multicolumn{3}{c|}{\textbf{SemEval}} & \multicolumn{3}{c|}{\textbf{Wikinews}} & \multicolumn{3}{c}{\textbf{\textsc{Deft}}}\\
            \cline{2-4}\cline{5-7}\cline{8-10}\cline{11-13}
            & P & R & F & P & R & F & P & R & F & P & R & F\\
            \hline
            SingleRank & \textbf{22,6} & \textbf{28,8} & \textbf{25,0}$^{~}$ & $~~$4,8 & $~~$3,3 & $~~$3,9$^{~}$ & 19,2 & 20,4 & 19,5$^{~}$ & $~~$4,7 & $~~$9,4 & $~~$6,2$^{~}$\\
            + complet & 22,2 & 28,1 & 24,5$^{~}$ & $~~$5,5 & $~~$3,8 & $~~$4,4$^{~}$ & 20,0 & 21,4 & 20,3${~}$ & $~~$4,4 & $~~$9,0 & $~~$5,8$^{~}$\\
            + candidats & 10,4 & 13,5 & 11,6$^{~}$ & $~~$9,4 & $~~$6,8 & $~~$7,8$^\dagger$ & 28,5 & 30,0 & 28,8$^\dagger$ & 10,3 & 19,2 & 13,2$^\dagger$\\
            + sujets & 18,9 & 24,2 & 21,0$^{~}$ & 14,2 & $~~$9,9 & 11,6$^\dagger$ & 30,7 & 32,6 & 31,1$^\dagger$ & 11,1 & 20,4 & 14,2$^\dagger$\\
            TopicRank & 18,2 & 23,2 & 20,1$^{~}$ & \textbf{15,1} & \textbf{10,6} & \textbf{12,3}$^\dagger$ & \textbf{34,8} & \textbf{37,3} & \textbf{35,4}$^\dagger$ & \textbf{11,3} & \textbf{21,0} & \textbf{14,5}$^\dagger$\\
            \bottomrule
          \end{tabular}
          \caption[Résultats de l'extraction de dix termes-clés avec chacune des
                   contributions de TopicRank, appliquées séparément à
                   SingleRank]{
            Résultats de l'extraction de dix termes-clés avec chacune des
            contributions de TopicRank, appliquées séparément à SingleRank.
            $\dagger$ indique une amélioration significative vis-à-vis de
            SingleRank, à 0,001 pour le t-test de Student.
            \label{tab:evaluation_individuelle_des_ameliorations}
          }
        \end{table}

        Dans le but de confirmer la pertinence de tous les apports de TopicRank,
        nous réalisons une expérience supplémentaire dans laquelle nous
        appliquons individuellement à SingleRank toutes les modifications
        successives permettant d'obtenir la méthode TopicRank depuis la méthode
        SingleRank~: l'usage d'un graphe complet (+ complet), la projection des
        termes-clés candidats dans le graphe (+ candidats) et la projection des
        sujets dans le graphe (+ sujets). Les résultats de ces trois variantes
        de SingleRank sont présentés dans le
        tableau~\ref{tab:evaluation_individuelle_des_ameliorations}.
        Globalement, l'usage des termes-clés candidats, ou sujets, induit une
        amélioration significative des performances de SingleRank, avec une
        amélioration plus importante en utilisant les sujets. Cela confirme la
        pertinence d'ordonner directement les candidats, plutôt que les mots,
        ainsi que la pertinence de grouper les candidats représentant le même
        sujet pour mutualiser les relations qu'ils entretiennent avec les
        candidats représentant d'autres sujets. L'usage d'un graphe complet,
        quant à lui, n'améliore pas significativement les résultats de
        SingleRank. Ceux-ci sont compétitifs vis-à-vis de ceux obtenus en
        construisant un graphe de cooccurrences. Toutefois, nous pensons que
        l'usage du graphe complet est à privilégier afin d'éviter d'avoir à
        fixer le paramètre de la fenêtre de cooccurrences.
        
        En ce qui concerne la collection \textsc{Duc}, le
        tableau~\ref{tab:evaluation_individuelle_des_ameliorations} montre une
        perte de performance induite par la construction du graphe avec les
        termes-clés candidats. Cette perte de performance s'explique par le fait
        qu'il y a, dans les documents de \textsc{Duc}, peu de répétition des
        candidats, notamment ceux de plus d'un mot. Le graphe créé contient
        alors moins de relations de cooccurrences que lorsque les n\oe{}uds sont
        les mots du document et est donc moins précis pour l'ordonnancement.

      \subsection{Analyse d'erreurs}
      \label{subsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-error_analysis}
        Dans cette section, nous proposons d'analyser les erreurs de TopicRank.
        Dans un premier temps, nous analysons les sujets que détecte TopicRank,
        puis dans un second temps, nous analysons les termes-clés de référence
        qui ne sont pas extraits par Topic\-Rank.

        \subsubsection{Analyse des sujets détectés}
        \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-error_analysis-detected_topics}
          Dans cette section, nous analysons les groupements en sujets effectués
          par Topic\-Rank afin de déterminer quelles sont les principales causes
          d'erreurs.

          \TODO{peut-être revoir les exemples qui suivent}

          Nous observons des erreurs liées à la sélection des termes-clés
          candidats. Lors de cette étape, certaines unités textuelles sont
          sélectionnées comme candidats à cause d'erreurs commises lors de
          l'étiquetage grammatical. Ces erreurs concernent principalement la
          détection des participes. Par exemple, dans la phrase \og{}[\dots]
          elles ne cessent de se développer à travers le monde et
          particulièrement dans les pays dits ``du
          sud''~[\dots]\fg{}\footnote{Exemple issu de l'article d'anthropologie
          \textit{Le marché parallèle du médicament en milieu rural au Sénégal}
          (\url{http://id.erudit.org/iderudit/014935ar}) de la collection
          \textsc{Deft}.}, \og{}dits\fg{} est un adjectif selon l'outils MElt, ce qui
          entraîne la sélection erronée du terme-clé candidat \og{}pays
          dits\fg{}.

          Nous observons également de nombreuses erreurs lorsque les groupements
          sont déclenchés par un adjectif. Ce sont particulièrement les
          expansions nominales s'effectuant à gauche qui en sont la cause (par
          exemple \og{}même langue\fg{} groupé avec \og{}même
          représentation\fg{}). Parmi les expansions nominales s'effectuant à
          droite, les adjectifs relationnels sont moins sujets aux erreurs que
          les autres adjectifs. Notons tout de même que lorsque ces adjectifs
          sont liés au contexte général du document, ils sont très fréquemment
          utilisés et beaucoup de candidats les contenant sont groupés par
          erreur (par exemple \og{}forces économiques\fg{} peut être groupé
          avec \og{}délabrement économique\fg{} dans un document d'économie).
          Outres ces groupements erronés, nous observons aussi de mauvais
          groupements lorsque les candidats ne contiennent que très peu de mots.
          Pour les candidats de deux mots, il ne suffit que d'un seul mot en
          commun pour les grouper. Ces candidats étant très fréquents, ils sont
          la cause de nombreuses erreurs.

        \subsubsection{Analyse des faux négatifs}
        \label{subsubsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-error_analysis-false_negatives}
          Dans cette section, nous analysons les termes-clés de référence qui
          n'ont pas été extraits par TopicRank. Plus particulièrement, nous nous
          intéressons à ceux qui sont présents dans les dix sujets jugés les
          plus importants de chaque document, mais qui n'ont pas été
          sélectionnés pour les représenter. Nous observons deux sources
          d'erreurs.

          La première source d'erreurs est le groupement en sujets. Lorsqu'un
          sujet détecté contient en réalité des termes-clés candidats
          représentant des sujets différents, la stratégie de sélection du
          meilleur terme-clé dans le sujet parvient à sélectionner le terme-clé
          correct dans certains cas, mais elle échoue parfois.

          \TODO{peut-être revoir les exemples qui suivent}

          La seconde source d'erreurs est la spécialisation des termes-clés de
          référence. Nous observons deux problèmes de sous et sur-spécialisation
          de certains termes-clés extraits vis-à-vis des termes-clés de
          référence. Dans le cas de la sous-spécialisation, nous pouvons citer,
          par exemple, \og{}papillons\fg{} qui est extrait à la place de
          \og{}papillons mutants\fg{}\footnote{Exemple issue de l'article
          journalistique \textit{Fukushima fait muter les papillons}
          (\url{http://fr.wikinews.org/w/index.php?oldid=432477}) de la
          collection Wikinews.}. Bien que ce problème de sous-spécialisation
          soit identifié, l'existance du problème inverse le rend plus difficile
          à résoudre. Dans le cas de la sur-spécialisation, nous pouvons citer,
          par exemple, \og{}député Antoni Pastor\fg{} qui est extrait à la place
          de \og{}Antoni Pastor\fg{}\footnote{Exemple issu de l'article
          journalistique \textit{Îles Baléares : le Parti populaire exclut le
          député Antoni Pastor pour avoir défendu la langue catalane}
          (\url{http://fr.wikinews.org/w/index.php?oldid=479948}) de la
          collection Wikinews.}. La raison principale de ce problème est
          l'aspect libre et ambigu de l'annotation manuelle des termes-clés.
%          Toutefois, privilégier les modifications adjectivales (par exemple
%          \og{}mutants\fg{}) et, au contraire, éviter les modifications
%          nominales (par exemple \og{}député\fg{}) semblent être une hypothèse à
%          vérifier.

      \subsection{Bilan}
      \label{subsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-bilan}
        Avec TopicRank, nous proposons une méthode à base de graphe pour
        l'extraction non supervisée de termes-clés. Elle groupe les termes-clés
        candidats en sujets, détermine quels sont ceux les plus importants, puis
        extrait le terme-clé candidat qui représente le mieux chacun des sujets
        les plus importants. Cette nouvelle méthode offre plusieurs avantages
        vis-à-vis des précédentes méthodes à base de graphe. Le groupement des
        termes-clés potentiels en sujets distincts permet de rassembler des
        informations relatives au même sujets et le choix d'un seul terme-clé
        pour représenter un sujet important permet d'extraire un ensemble de
        termes-clés non redondants (pour $k$ termes-clés extraits, exactement
        $k$ sujets sont couverts). Enfin, le graphe est complet et ne requiert
        plus le paramétrage d'une fenêtre de cooccurrences.

        Les bons résultats de notre méthode montrent la pertinence d'un
        groupement des candidats en sujets et d'un ordonnancement de ces sujets,
        plutôt que des mots. Les expériences montrent aussi qu'il est encore
        possible d'améliorer notre méthode en proposant une nouvelle stratégie
        de sélection du terme-clé candidat le plus représentatif d'un sujet.
        Dans un premier temps, nous souhaitons explorer plus encore l'usage du
        graphe et l'étendre à l'assignement de termes-clés.

  %-----------------------------------------------------------------------------

  \section{Indexation automatique par termes-clés}
  \label{sec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_extraction}
    Dans cet section, nous présentons TopicCoRank, une méthode permettant
    d'effectuer simultanément extraction et assignement de termes-clés, soit la
    première méthode à considérer les deux aspects de la tâche d'indexation par
    termes-clés. Ce travail se fonde sur celui de TopicRank, que nous présentons
    dans la section précédente. Il s'agit d'une extension supervisée de
    TopicRank.

    La tâche d'assignement de termes-clés est la tâche la moins étudiée pour
    l'indexation automatique par termes-clés. Elle est plus contraignante que
    l'extraction de termes-clés, car elle requiert un vocabulaire contrôlé
    coûteux à produire. Elle est aussi plus difficile, car elle doit tisser des
    liens entre le contenu du document et les entrées du vocabulaire contrôlé
    alors que ces dernières n'apparaissent pas nécessairement dans le document.
    Si nous prenons l'exemple de l'indexation par termes-clés de référence
    réalisée pour les collections de données Termith, la majorité des
    termes-clés sont obtenus par assignement. Les indexeurs professionnels qui
    sont intervenus sur ces données avaient pour consignes de fournir des
    termes-clés généraux au domaine (majoritairement par assignement), mais
    aussi  des termes-clés précis, vis-à-vis du contenu des documents, ou des
    concepts nouveaux (par extraction). Il est donc important de considérer la
    tâche d'assignement de termes-clés. Cependant, nous pensons que celle-ci
    doit être réalisée conjointement à la tâche d'extraction, non séparément.

    Avec TopicCoRank, nous proposons une nouvelle méthode d'indexation par
    termes-clés qui unifie un graphe représentatif du document et un graphe
    représentatif du domaine, c'est-à-dire de ses termes-clés de référence et de
    leurs relations. De cette manière, nous sommes capable d'extraire et
    d'assigner des termes-clés indifféremment. Concrètement, TopicCoRank apporte
    trois contributions~: une extension applicable à toute méthode d'extraction
    de termes-clés à base de graphe, une alternative à la construction manuelle
    d'un vocabulaire contrôlé et une approche purement automatique qui n'utilise
    pas d'heuristique pour combiner les deux catégories d'indexation par
    termes-clés.
        
    Le travail le plus proche de celui que nous présentons est celui de
    \newcite{figueroa2014collaborativeranking}. Tout comme nous avec TopicRank,
    \newcite{figueroa2014collaborativeranking} tentent d'améliorer l'approche à
    base de graphe TextRank en mettant à profit des informations extraites des
    données d'entraînement. Leur travail diffère cependant du notre, car ils
    utilisent la méthode supervisée d'extraction de terme-clé KEA, non pas
    d'assignement de termes-clés, et combinent sa sortie à celle de TextRank
    avec un algorithme de fusion de listes ordonnées. Notre travail exploite les
    données d'entraînement pour ajouter la capacité à assigner des termes-clés.
    De plus, TopicCoRank ne fusionne pas les sorties de deux méthodes, il met en
    phase le document et son domaine pour extraire des termes-clés du document
    et en assigner depuis le domaine lorsque c'est nécessaire.

    \subsection{TopicCoRank}
    \label{subsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-topiccorank}
      TopicCoRank est une méthode supervisée à base de graphe qui réalise
      simultanément extraction et assignement de termes-clés. Il s'agit d'une
      extension de TopicRank. Elle étend les étapes suivantes de ce dernier~:
      construction du graphe, ordonnancement et sélection des termes-clés.

      \subsubsection{Construction du graphe}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-graph_construction}
        Afin de réaliser simultanément extraction et assignement de termes-clés,
        TopicCoRank unifie deux graphes représentant le document (graphe de
        sujets) et son domaine (graphe de termes-clés de référence). Ce dernier
        graphe est construit à partir des termes-clés de référence des documents
        d'entraînement fournis avec une collection de données. Comme
        \newcite{chaimongkol2013technicaltermextraction} l'ont fait avant nous
        pour l'extraction de termes techniques, nous faisons l'hypothèse que les
        termes-clés de référence des documents d'entraînement constituent la
        terminologie du domaine et nous les utilisons comme substitut au
        vocabulaire contrôlé. Contrairement au termes-clés candidats
        sélectionnés dans le document, les termes-clés de référence ne sont pas
        jugés redondants. TopicCoRank ne les groupe pas avant de construire le
        graphe du domaine.

        Soit le graphe unifié $G = (N, A = A_{\textnormal{\textit{interne}}}
        \cup A_{\textnormal{\textit{externe}}})$ non orienté. $N$ dénote
        indifféremment les sujets et les termes-clés de référence. $A$ regroupe
        les arêtes $A_{\textnormal{\textit{interne}}}$, qui connectent deux
        sujets ou deux termes-clés de référence, et les arêtes
        $A_{\textnormal{\textit{externe}}}$, qui connectent un sujet à un
        terme-clé de référence (cf. figure~\ref{fig:topiccorank_graph}). Le
        graphe de sujets et le graphe du domaine sont unifiés grâce aux arêtes
        $A_{\textnormal{\textit{externe}}}$. Une arête
        $A_{\textnormal{\textit{externe}}}$ est ajouté pour connecter un sujet
        et un terme-clé si, et seulement si, le terme-clé fait partie des
        termes-clés candidats qui composent le sujet. En d'autres termes,
        TopicCoRank considère le domaine comme une carte conceptuelle et
        connecte le document au domaine par l'intermédiaire des concepts qu'ils
        partagent. Pour favoriser les connections, et à la manière du groupement
        en sujets, la méthode de racinisation de
        \newcite{porter1980suffixstripping} est utilisée en amont des
        comparaisons.
        \begin{figure}
          \newcommand{\xslant}{0.25}
          \newcommand{\yslant}{0}

          \centering
          \begin{tikzpicture}[transform shape, scale=.75]
            % frame
            \node [draw,
                   rectangle,
                   minimum width=.7\linewidth,
                   minimum height=8em,
                   xslant=\xslant,
                   yslant=\yslant] (domain_graph) {};
            \node [above=of domain_graph,
                   xshift=.36\linewidth,
                   yshift=8em,
                   anchor=south east] (domain_graph_label) {termes-clés de référence};

            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=.3\linewidth,
                 yshift=5em] (domain_node1) {$V_1$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=-.3\linewidth,
                   yshift=5em] (domain_node2) {$V_2$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   yshift=5em] (domain_node3) {$V_3$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=.15\linewidth,
                   yshift=.75em] (domain_node4) {$V_4$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=-.15\linewidth,
                   yshift=.75em] (domain_node5) {$V_5$};

            \draw (domain_node1) -- (domain_node3);
            \draw (domain_node2) -- (domain_node3);
            \draw (domain_node2) -- (domain_node4);
            \draw (domain_node4) -- (domain_node5);
            \draw (domain_node4) -- (domain_node3);

            % document
            \node [draw,
                   rectangle,
                   minimum width=.7\linewidth,
                   minimum height=8em,
                   xslant=\xslant,
                   yslant=\yslant,
                   above=of domain_graph,
                   xshift=-2em] (document_graph) {};
            \node [below=of document_graph,
                   xshift=-.36\linewidth,
                   yshift=-8em,
                   anchor=north west] (document_graph_label) {sujets du document};

            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                   xshift=.3\linewidth,
                   yshift=-5em] (document_node1) {$V_6$};
            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                   xshift=-.3\linewidth,
                   yshift=-5em] (document_node2) {$V_7$};
            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                 yshift=-5em] (document_node3) {$V_8$};
            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                   xshift=.15\linewidth,
                   yshift=-.75em] (document_node4) {$V_9$};

            \draw (document_node2) -- (document_node3);
            \draw (document_node3) -- (document_node1);
            \draw (document_node1) -- (document_node4);
            \draw (document_node3) -- (document_node4);

            % extra link
            \draw [dashed] (document_node2) -- (domain_node2);
            \draw [dashed] (document_node3) -- (domain_node3);
            \draw [dashed] (document_node4) -- (domain_node1);
            \draw [dashed] (document_node3) -- (domain_node4);

            % legend
            \node [right=of document_graph, xshift=2em, yshift=-9.25em] (legend_title) {\underline{Légende~:}};
            \node [below=of legend_title, xshift=-1em, yshift=2em] (begin_inner) {};
            \node [right=of begin_inner] (end_inner) {: $A_\textnormal{\textit{interne}}$};
            \node [below=of begin_inner, yshift=1.5em] (begin_outer) {};
            \node [right=of begin_outer] (end_outer) {: $A_\textnormal{\textit{externe}}$};

            \draw (legend_title.north  -| end_outer.east) rectangle (end_outer.south -| legend_title.west);

            \draw (begin_inner) -- (end_inner);
            \draw [dashed] (begin_outer) -- (end_outer);
          \end{tikzpicture}
          \caption{Illustration du graphe unifié utilisé par TopicCoRank
                   \label{fig:topiccorank_graph}}
        \end{figure}

        Pour permettre un ordonnancement conjoint des sujets et des termes-clés
        de référence, le schéma de connexion entre deux sujets et entre deux
        termes-clés de référence (arêtes $A_\textnormal{\textit{interne}}$) est
        homogénéisé. En effet, si les conditions de connexion et si la
        pondération des arêtes ne sont pas, respectivement, sémantiquement
        équivalentes et du même ordre de grandeur, alors l'impact du domaine sur
        le document, et inversement, est marginal et inexploitable. TopicCoRank
        connecte deux sujets ou deux termes-clés de référence $n_i$ et $n_j$
        lorsqu'il apparaissent dans le même contexte et pondère leur arête par
        le nombre de fois que cela se produit ($\textnormal{poids}(n_i, n_j)$),
        normalisé par le nombre de contextes explorés. Lorsqu'il s'agit des
        sujets, le contexte est une phrase du document~; lorsqu'il s'agit des
        termes-clés de référence, le contexte est l'ensemble des termes-clés de
        référence d'un document d'entraînement. Ne pouvant pas créer un graphe
        complet du domaine à partir de ce schéma de connexion, nous n'utilisons
        pas un graphe complet pour le graphe de sujets. Afin d'éviter d'utiliser
        une valeur pour la fenêtre de cooccurrences, nous difinissons la phrase
        comme fenêtre.

      \subsubsection{Ordonnancement conjoint des sujets et des termes-clés de référence}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-co_ranking}
        L'ordonnancement conjoint des sujets et des termes-clés de référence
        établit l'ordre d'importance des sujets du document et des termes-clés
        de référence du domaine vis-à-vis du contenu du document. Le score
        d'importance attribué aux sujets et aux termes-clés de référence est
        obtenu avec le même algorithme et la même instance de l'algorithme.
        L'ordonnancement fournit donc une seule liste ordonnée, mêlant sujets et
        termes-clés de référence.

        Dans TopicCoRank, le principe de la recommandation  de TopicRank est
        repris et adapté au problème d'ordonnancement conjoint. Les premières
        hypothèses de recommandation sont donc les mêmes que celle de
        TopicRank~:
        \begin{itemize}
          \item{un sujet est d'autant plus important s'il est fortement connecté
                à un grand nombre de sujets et si les sujets avec lesquels il
                est fortement connecté sont importants~;}
          \item{un terme-clé de référence est d'autant plus important s'il est
                fortement connecté à un grand nombre de termes-clés de référence
                et si les termes-clés de référence avec lesquels il est connecté
                sont importants.}
        \end{itemize}
        Ces hypothèses de recommandation, que nous qualifions d'internes,
        permettent d'établir l'importance des sujets les uns par rapport aux
        autres et l'importance des termes-clés de référence les uns par rapport
        aux autres. Cependant, elles ne permettent pas de tirer profit des
        termes-clés de référence importants pour renforcer l'importance des
        sujets qui y sont connectés et, inversement, elles ne permettent pas de
        tirer profit des sujets importants pour renforcer l'importance des
        termes-clés de référence auxquels ils sont connectés. De plus,
        l'importance des termes-clés de référence est indépendante du document
        et, pour la même collection de données, l'importance des termes-clés de
        référence est toujours la même. Nous ajoutons donc deux nouvelles
        hypothèses de recommandation, que nous qualifions d'externes~:
        \begin{itemize}
          \item{un sujet est d'autant plus important s'il contient des
                termes-clés de référence importants~;}
          \item{un terme-clé de référence est d'autant plus important
                \underline{vis-à-vis du contenu du document} s'il est connecté à
                l'un de ses sujets importants.}
        \end{itemize}
        Sujets et termes-clés de référence sont ainsi évalués d'après leur usage
        dans le document et leur usage (global) dans le domaine.
        L'ordonnancement des uns joue un rôle sur celui des autres et permet
        ainsi d'effectuer extraction et assignement conjointement.

        Mathématiquement, la formule d'ordonnancement de TopicRank est adaptée
        en préservant la recommandation interne
        ($R_{\textnormal{\textit{interne}}}$) et en remplaçant l'aléa, exprimé
        avec le terme $(1 - \lambda)$, par la recommandation externe
        ($R_{\textnormal{\textit{externe}}}$)~:
        \begin{align}
          S(n_i) &= (1 - \lambda)\ R_{\textnormal{\textit{externe}}}(n_i) + \lambda\ R_{\textnormal{\textit{interne}}}(n_i)\label{math:topiccorank}\\
          R_{\textnormal{\textit{interne}}}(n_i) &= \sum_{n_j \in A_{\textnormal{\textit{interne}}}(n_i)}{\frac{\textnormal{poids}(n_j, n_i) \times S(n_j)}{\mathlarger\sum_{n_k \in A_{\textnormal{\textit{interne}}}(n_j)}{{\textnormal{poids}(n_j, n_k)}}}}\label{math:rin}\\
          R_{\textnormal{\textit{externe}}}(n_i) &= \sum_{n_j \in A_{\textnormal{\textit{externe}}}(n_i)}{\frac{S(n_j)}{|A_{\textnormal{\textit{externe}}}(n_j)|}}\label{math:rout}
        \end{align}
        où $A_{\textnormal{\textit{interne}}}(n_i)$ représente l'ensemble de
        tous les n\oe{}uds connectés au n\oe{}ud $n_i$ par une arête
        $A_\textnormal{\textit{interne}}$, où
        $A_{\textnormal{\textit{externe}}}(n_i)$ représente l'ensemble de tous
        les n\oe{}uds connectés au n\oe{}ud $n_i$ par une arête
        $A_\textnormal{\textit{externe}}$ et où le facteur $\lambda$ permet
        désormais de définir la recommandation la plus influente entre
        $R_{\textnormal{\textit{interne}}}$ et
        $R_{\textnormal{\textit{externe}}}$. Par défaut, nous définissons
        $\lambda=0,5$ afin de donner autant de poids aux deux recommandations.

      \subsubsection{Sélection des termes-clés}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-keyphrase_selection}
        Pour terminer l'indexation par termes-clés, TopicCoRank utilise l'ordre
        d'importance $S$ des sujets et termes-clés de référence pour déterminer
        les termes-clés du document. Les $k$ n\oe{}uds du graphe unifié ayant
        obtenu les meilleurs scores sont retenus, qu'ils soient des sujets ou
        des termes-clés de référence.

        Avant la troncature aux $k$ meilleurs sujets et termes-clés de
        référence, certains termes-clés de référence sont retirés si leur
        ordonnancement n'a pas été influencé par le document. En effet, le
        graphe du domaine peut, dans certain cas, être constitué de composantes
        connexes, c'est-à-dire composé de sous-graphes qui ne sont connectés par
        aucune arête (\TODO{exemple}), soit dans notre cas des groupes de
        termes-clés de référence qui ne partagent aucun contexte. Si aucun
        terme-clé de référence d'une composante connexe n'est connecté à un
        sujet du document, alors l'ordonnancement au sein de cette composante
        connexe n'est pas influencé par le document. Il n'est donc pas pertinent
        d'en tenir compte.

        Lorsque un n\oe{}ud retenu représente un sujet, C'est la même stratégie
        que celle de TopicRank qui est appliquée. Pour un sujet donné, le
        terme-clé extrait est son terme-clé candidat qui apparaît en premier
        dans le document.

      \subsubsection{Exemple}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-exemple}
        \TODO{\dots}

    \subsection{Évaluation}
    \label{subsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation}
      Pour valider notre approche, nous réalisons deux séries d'expériences. Une
      première série pour comparer TopicCoRank à deux méthodes de référence et
      une seconde série pour analyser sont comportement.

      \subsubsection{Méthodes de référence}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-baselines}
        Dans nos expériences, nous comparons TopicCoRank à TopicRank et à
        une méthode simple d'assignement (\textit{Assignement}). Pour cette
        dernière, nous utilisons le même vocabulaire contrôlé que TopicCoRank et
        ordonnons les entrées de celui-ci d'après leur fréquence d'apparition
        dans le document. \TODO{remplacer \textit{Assignement} par KEA++}

        Nous comparons aussi TopicCoRank à deux de ces variantes possibles. La
        première, TopicCoRank$_\textnormal{\textit{extr.}}$, ne réalise que
        l'extraction de termes-clés~; la seconde,
        TopicCoRank$_\textnormal{\textit{assign.}}$, n'effectue que
        l'assignement.

      \subsubsection{Collections de données}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-evaluation_data}
        Pour évaluer TopicCoRank, nous utilisons la collection de documents de
        linguistique de la ressource Termith et les collections SemEval et
        \textsc{Duc}.
        
        Pour la collection de Termith, nous construisons le graphe du domaine à
        partir des documents d'entraînement~; pour SemEval, nous construisons
        quatre graphes de domaine à partir des documents d'entraînement des
        quatre catégories \textsc{Acm} (C2.4, H3.3, I2.11 et J4) et utilisons
        l'un ou l'autre de ces graphes selon la catégorie du document traité par
        TopicCoRank~; pour \textsc{Duc}, nous n'utilisons pas sa répartition
        entraînement--test proposée dans la
        section~\ref{sec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection}
        (page~\pageref{sec:main-automatic_keyphrase_annotation-keyphrase_candidate_selection}),
        mais nous construisons un graphe de domaine unique pour chaque document.
        Comme chaque document de \textsc{Duc} appartient à l'un des 30 sujets
        d'actualités couverts, nous utilisons leur sujet d'actualité pour
        construire leur graphe de domaine à partir des autres documents traitant
        du même sujet. Le tableau~\ref{tab:topiccorank:corpora_recap} rappelle
        les caractéristiques des collections de données que nous utilisons pour
        évaluer TopicCoRank.
        \begin{table}[!h]
          \centering
          \resizebox{\linewidth}{!}{
            \begin{tabular}{l@{~}l@{~}l|c@{~~}c@{~~}c@{~~}c|c@{~~}c@{~~}c@{~~}c}
              \toprule
              \multicolumn{3}{l|}{\multirow{2}{*}{\textbf{Corpus}}} & \multicolumn{4}{c|}{\textbf{Documents}} & \multicolumn{4}{c}{\textbf{Termes-clés}}\\
              \cline{4-11}
              & & & Langue & Genre & Quantité & Mots moy. & Annotateur & Quantité moy. & \og{}À assigner\fg{} & Mots moy.\\
              \hline
              \multicolumn{3}{l|}{Termith} & & & & & & & &\\
              $\drsh$ & \multicolumn{2}{@{}l|}{Linguistique} & & & & & & & &\\
              & $\drsh$ & Appr. & Français & Scientifique (résumé) & 515 & 160,5 & Professionnel & 8,6 & 60,6~\% & 1,7\\
              & $\drsh$ & Test & \ditto & \ditto & 200 & 147,0 & \ditto & 8,9 & 62,8~\% & 1,8\\
%              $\drsh$ & \multicolumn{2}{@{}l|}{Sciences de l'info.} & & & & & & &\\
%              & $\drsh$ & Appr. & Français & Scientifique (résumé) & 506 & 105,0 & Professionnel & $~~$7,8 & 67,9~\% & 1,8\\
%              & $\drsh$ & Test & \ditto & \ditto & 200 & 157,0 & \ditto & 10,2 & 66,9~\% & 1,7\\
%              $\drsh$ & \multicolumn{2}{@{}l|}{Archéologie} & & & & & & &\\
%              & $\drsh$ & Appr. & Français & Scientifique (résumé) & 518 & 221,1 & Professionnel & 16,9 & 37,0~\% & 1,3\\
%              & $\drsh$ & Test & \ditto & \ditto & 200 & 213,9 & \ditto & 15,6 & 37,4~\% & 1,3\\
%              $\drsh$ & \multicolumn{2}{@{}l|}{Chimie} & & & & & & &\\
%              & $\drsh$ & Appr. & Français & Scientifique (résumé) & 582 & 105,7 &  Professionnel & 12,2 & 75,2~\% & 2,2\\
%              & $\drsh$ & Test & \ditto & \ditto & 200 & 103,9 & \ditto & 14,6 & 78,8~\% & 2,4\\
              \multicolumn{3}{l|}{Semeval} & & & & & & & &\\
              $\drsh$ & \multicolumn{2}{@{}l|}{Appr.} & Anglais & Scientifique & 144 & 5~134,6 & Auteur~/~Lecteur & 15,4 & 13,5~\% & 2,1\\
              $\drsh$ & \multicolumn{2}{@{}l|}{Test} & \ditto & \ditto & 100 & 5~177,7 & \ditto & 14,7 & 22,1~\% & 2,1\\
              \multicolumn{3}{l|}{\textsc{Duc}} & \ditto & Journalistique & 308 & 900,7 & Lecteur & $~~$8,1 & $~~$3,5~\% & 2,1\\
              \bottomrule
            \end{tabular}
          }

          \caption{Rappel des corpus utilisés pour l'évaluation de TopicCoRank
                   \label{tab:topiccorank:corpora_recap}}
        \end{table}

      \subsubsection{Prétraitements}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-preprocessing}
        Pour chaque collection de données, les documents subissent les mêmes
        prétraitements~: segmentation en phrases, segmentation des phrases en
        mots et étiquetage grammatical des mots. Quelque soit la langue, la
        segmentation en phrases est mise en \oe{}uvre avec la classe
        \texttt{PunktSentenceTokenizer} de \textsc{Nltk}~\cite{bird2009nltk}. En
        anglais, la segmentation des phrases en mots est réalisée avec la classe
        \texttt{TreeBankWordTokenizer} de \textsc{Nltk} et l'étiquetage
        grammaticale est effectué avec le \textit{Stanford \textsc{Pos}
        tagger}~\cite{toutanova2003stanfordpostagger}. En français, la
        segmentation des phrases en mots est mise en \oe{}uvre avec le
        segmenteur Bonsai, du \textit{Bonsai \textsc{Pcfg-La}
        parser}\footnote{\url{http://alpage.inria.fr/statgram/frdep/fr_stat_dep_parsing.html}},
        dont se fonde l'outil MElt~\cite{denis2009melt} utilisé pour
        l'étiquetage grammatical. Tous ces outils sont appliqués avec leur
        configuration par défaut.
      
      \subsubsection{Mesures d'évaluation}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-evaluation_measures}
        Les performances des méthodes d'extraction de termes-clés sont exprimées
        en termes de précision (P), rappel (R) et f-mesure (f1-mesure, F). En
        accord avec l'évaluation menée dans les travaux précédents, nous
        considérons correcte l'extraction d'une variante flexionnelle d'un
        terme-clé de référence~\cite{kim2010semeval}. Les opérations de
        comparaison entre les termes-clés de référence et les termes-clés
        extraits sont donc effectuées à partir de la racine des mots qui les
        composent, d'après la méthode de \newcite{porter1980suffixstripping}.

        Nous représentons aussi les résultats sous la forme de courbes de
        rappel--précision. Celles-ci permettent d'observer si une méthode domine
        les autres pour les critère de rappel et de précision. En optimisation
        multi-critères, nous parlons de front de Pareto optimal, c'est à dire de
        la méthode pour laquelle aucune autre méthode n'obtient de meilleures
        performances. Pour générer ces courbes, nous calculons la précision et
        le rappel lorsque  le nombre de termes-clés extraits/assignés varie de
        un jusqu'au nombre total de termes-clés
        candidats~\cite{hassan2010conundrums}.
      
      \subsubsection{Comparaison de TopicRank++ avec l'existant}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-comparison}
        Le tableau~\ref{tab:topiccorank-comparison_results} montre les
        performances de TopicCoRank comparées à celles des méthodes de
        référence. Dans un premier temps, nous observons que l'assignement de
        termes-clés (avec \textit{Assignement} et
        TopicCoRank$_\textnormal{\textit{assign.}}$) est plus aisé que
        l'extraction de termes-clés (avec TopicRank et
        TopicCoRank$_\textnormal{\textit{extr.}}$) pour les collections Termith
        et \textsc{Duc}, et qu'elle est plus difficile pour la collection
        SemEval. Les résultats montrent toutefois qu'il est bénéfique
        d'effectuer les deux simultanément. En effet, quelque soit la collection
        TopicCoRank obtient de meilleures performances que TopicRank,
        \textit{Assignement} et TopicCoRank$_\textnormal{\textit{extr.}}$. Dans
        le cas de la collection Termith, TopicCoRank donne la seconde meilleure
        performance après TopicCoRank$_\textnormal{\textit{assign.}}$. Dans ce
        cas de figure, la proportion de termes-clés \og{}à assigner\fg{} est
        tellement importante qu'effectuer uniquement de l'assignement est plus
        fructueux. Toutefois, la pertinence de TopicCoRank n'est en aucun cas
        remise en cause, car TopicCoRank$_\textnormal{\textit{assign.}}$ à une
        performance significativement meilleure à celle de la méthode de
        référence \textit{Assignement}. Globalement, les résultats montrent
        l'apport d'ordonner conjointement les sujets du document et les
        termes-clés de référence du domaine pour extraire et assigner des
        termes-clés.
        \begin{table}[h!]
          \centering
          %\resizebox{\linewidth}{!}{
            \begin{tabular}{l|ccc@{}|ccc@{}|ccc@{~}}
              \toprule
              \multirow{2}{*}{\textbf{Method}} &
         \multicolumn{3}{c|}{\textbf{Termith}} & \multicolumn{3}{c|}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}}\\
              \cline{2-10}
              & P & R & F$^{~~~~}$ & P & R & F$^{~~~~}$ & P & R & F$^{~~}$\\
              \hline
              TopicRank & 11.3 & 13.1 & 12.0$^{~~~~}$ & 17.8 & 22.7 & 19.7$^{~~~~}$ & 14.6 & 10.1 & 11.8$^{\ddagger}$\\
              \textit{Assignement} & 18.0 & 21.1 & 19.1$^{\dagger~~}$ & 24.3 & 31.2 & 27.0$^{\dagger~~}$ & ~~8.5 & ~~6.3 & ~~7.2$^{~~}$\\
              \hline
              TopicCoRank$_\textnormal{\textit{extr}}$ & 14.6 & 16.8 & 15.4$^{\dagger~~}$ & 25.5 & 32.4 & 28.1$^{\dagger~~}$ & 15.2 & 10.6 & 12.4$^{\ddagger}$\\
              TopicCoRank$_\textnormal{\textit{assign}}$ & \textbf{24.9} & \textbf{28.9} & \textbf{26.4$^{\dagger\ddagger}$} & 25.9 & 33.3 & 28.8$^{\dagger~~}$ & 11.6 & ~~8.3 & ~~9.5$^{~~}$\\
              \hline
              TopicCoRank & 19.0 & 22.0 & 20.1$^{\dagger~~}$ & \textbf{28.4} & \textbf{36.6} & \textbf{31.5$^{\dagger\ddagger}$} & \textbf{16.4} & \textbf{11.6} & \textbf{13.4$^{\ddagger}$}\\
              \bottomrule
            \end{tabular}
          %}
        \caption[
          Résultat de l'extraction de dix termes-clés avec TopicRank,
          \textit{Assignement}, TopicCoRank$_\textnormal{\textit{extr.}}$,
          TopicCoRank$_\textnormal{\textit{assign.}}$ et TopicCoRank
        ]{
          Résultat de l'extraction de dix termes-clés avec TopicRank,
          \textit{Assignement}, TopicCoRank$_\textnormal{\textit{extr.}}$,
          TopicCoRank$_\textnormal{\textit{assign.}}$ et TopicCoRank. $\dagger$
          et $\ddagger$ indiquent une amélioration significative par rapport à
          TopicRank et \textit{Assignement}, respectivement.
          \label{tab:topiccorank-comparison_results}}
        \end{table}
        
        La figure~\ref{fig:topiccorank-pr_curves} compare les comportements
        respectifs de TopicRank, \textit{Assignement} et TopicCoRank à l'aide de
        courbes rappel--précision. Ces dernières confirment la domination de
        TopicCoRank par rapport aux deux autres méthodes (front de Pareto
        optimal). Elles montrent aussi un phénomène intéressant~: TopicCoRank à
        un comportement similaire à celui de TopicRank, boosté par sa capacité à
        assigner. Le processus d'ordonnancement initial de TopicRank n'est pas
        dénaturé et l'ajout du graphe du domaine agit effectivement comme une
        extension.
        \begin{figure}[h!]
            \centering
            \subfigure[Termith]{
              \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
                \pgfkeys{/pgf/number format/.cd, fixed}
                \begin{axis}[x=0.00855\linewidth,
                             xtick={0, 20, 40, ..., 100},
                             xmin=0,
                             xmax=40,
                             xlabel=rappel (\%),
                             x label style={yshift=.34em, font=\small},
                             y=0.00855\linewidth,
                             ytick={0, 10, 20, ..., 100},
                             ymin=0,
                             ymax=40,
                             ylabel=précision (\%),
                             y label style={yshift=-1.1em, font=\small},
                             legend style={font=\footnotesize}]
                  \addplot [red, mark=+] file {input/data/inist_topicrank.csv};
                  \addplot [green, mark=-] file {input/data/inist_assignment.csv};
                  \addplot [blue, mark=x] file {input/data/inist_topiccorank.csv};
                  \addplot [dotted, domain=20:40] {(30 * x) / ((2 * x) - 30)};
                  \addplot [dotted, domain=10:40] {(20 * x) / ((2 * x) - 20)};
                  \addplot [dotted, domain=5:40] {(10 * x) / ((2 * x) - 10)};
                  \legend{TopicRank, \textit{Assignement}, TopicCoRank};
                \end{axis}
                \node at (4.85,3.1) [anchor=east] {\footnotesize{F=0.30}};
                \node at (4.85,1.8) [anchor=east] {\footnotesize{F=0.20}};
                \node at (4.85,0.9) [anchor=east] {\footnotesize{F=0.10}};
              \end{tikzpicture}
            }
            \subfigure[\textsc{Duc}]{
              \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
                \pgfkeys{/pgf/number format/.cd, fixed}
                \begin{axis}[x=0.0057\linewidth,
                             xtick={0, 20, 40, ..., 100},
                             xmin=0,
                             xmax=60,
                             xlabel=rappel (\%),
                             x label style={yshift=.34em, font=\small},
                             y=0.0057\linewidth,
                             ytick={0, 15, 30, ..., 100},
                             ymin=0,
                             ymax=60,
                             ylabel=précision (\%),
                             y label style={yshift=-1.1em, font=\small},
                             legend style={font=\footnotesize}]
                  \addplot [red, mark=+] file {input/data/duc_topicrank.csv};
                  \addplot [green, mark=-] file {input/data/duc_assignment.csv};
                  \addplot [blue, mark=x] file {input/data/duc_topiccorank.csv};
                  \addplot [dotted, domain=30:60] {(40 * x) / ((2 * x) - 40)};
                  \addplot [dotted, domain=20:60] {(30 * x) / ((2 * x) - 30)};
                  \addplot [dotted, domain=10:60] {(20 * x) / ((2 * x) - 20)};
                  \addplot [dotted, domain=5:60] {(10 * x) / ((2 * x) - 10)};
                  \legend{TopicRank, \textit{Assignement}, TopicCoRank};
                \end{axis}
                \node at (4.85,2.6) [anchor=east] {\footnotesize{F=0.40}};
                \node at (4.85,1.8) [anchor=east] {\footnotesize{F=0.30}};
                \node at (4.85,1.15) [anchor=east] {\footnotesize{F=0.20}};
                \node at (4.85,0.6) [anchor=east] {\footnotesize{F=0.10}};
              \end{tikzpicture}
            }
            \subfigure[SemEval]{
              \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
                \pgfkeys{/pgf/number format/.cd, fixed}
                \begin{axis}[x=0.00855\linewidth,
                             xtick={0, 20, 40, ..., 100},
                             xmin=0,
                             xmax=40,
                             xlabel=rappel (\%),
                             x label style={yshift=.34em, font=\small},
                             y=0.00855\linewidth,
                             ytick={0, 10, 20, ..., 100},
                             ymin=0,
                             ymax=40,
                             ylabel=précision (\%),
                             y label style={yshift=-1.1em, font=\small},
                             legend style={font=\footnotesize}]
                  \addplot [red, mark=+] file {input/data/semeval_topicrank.csv};
                  \addplot [green, mark=-] file {input/data/semeval_assignment.csv};
                  \addplot [blue, mark=x] file {input/data/semeval_topiccorank.csv};
                  \addplot [dotted, domain=20:40] {(30 * x) / ((2 * x) - 30)};
                  \addplot [dotted, domain=10:40] {(20 * x) / ((2 * x) - 20)};
                  \addplot [dotted, domain=5:40] {(10 * x) / ((2 * x) - 10)};
                  \legend{TopicRank, \textit{Assignement}, TopicCoRank};
                \end{axis}
                \node at (4.85,3.1) [anchor=east] {\footnotesize{F=0.30}};
                \node at (4.85,1.8) [anchor=east] {\footnotesize{F=0.20}};
                \node at (4.85,0.9) [anchor=east] {\footnotesize{F=0.10}};
              \end{tikzpicture}
            }
            \caption{Courbes de rappel-précision de TopicRank
                     \textit{Assignement} et TopicCoRank
                     \label{fig:topiccorank-pr_curves}}
        \end{figure}
      
      \subsubsection{Analyse du comportement de TopicCoRank}
      \label{subsubsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-topiccorank-analysis}
        Bien qu'ils montrent de meilleures performances pour TopicCoRank, les
        résultats précédents ne permettent pas de se faire une idée précise de
        la place que prend l'assignement sur l'extraction. Le
        tableau~\ref{tab:assignment_ratio} montre la proportion de termes-clés
        extraits et assignés parmi les dix termes-clés que fournit TopicCoRank.
        Globalement, nous observons un équilibre entre extraction et
        assignement, avec tout de même une plus forte tendance à l'extraction.
        Cet équilibre et les variations observées selon les collections de
        données montrent que l'algorithme d'ordonnancement conjoint des sujets
        du document et des termes-clés de référence du domaine est effectivement
        capable de mêler sujets et termes-clés de références.
        \begin{table}[!h]
          \centering
          \begin{tabular}{l|cc}
              \toprule
              & Extraction (\%) & Assignement (\%)\\
              \hline
              Inist & 61,3 & 38,7\\
              DUC & 41,4 & 58,6\\
              SemEval & 60,0 & 40,0\\
              \bottomrule
          \end{tabular}
          \caption{Taux d'extraction et d'assignement de TopicCoRank pour dix
                   termes-clés
                   \label{tab:assignment_ratio}}
        \end{table}

        Enfin, la figure~\ref{fig:lambda_variations} montre le comportement de
        TopicCoRank lorsque nous faisons varier la valeur de $\lambda$ avec un
        pas de 0,1. Tout d'abord, nous observons que la moins bonne performance
        pouvant être obtenue est bornée par la performance de TopicRank. Quelque
        soit son degré d'influence, le graphe du domaine contribue positivement
        à l'indexation par termes-clés. Nous observons aussi deux comportements
        différents selon les collections de données. Dans le cas de Termith et
        de \textsc{Duc} les meilleures performances sont obtenues avec une
        influence plus importante de la recommandation externe
        R$_\textnormal{\textit{externe}}$, tandis que dans le cas de SemEval la
        recommandation interne R$_\textnormal{\textit{interne}}$ doit avoir une
        plus forte influence. Nous observons cependant un compromis lorsque
        $\lambda = 0,5$ (valeur que nous avons défini par défaut), soit quand
        les deux recommandations R$_\textnormal{\textit{interne}}$ et
        R$_\textnormal{\textit{externe}}$ ont la même influence sur
        l'ordonnancement.
        \begin{figure}[h!]
          \centering
          \subfigure[Inist]{
            \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
              \pgfkeys{/pgf/number format/.cd, fixed}
              \begin{axis}[x=0.393\linewidth,
                           xtick={0.1, 0.3, ..., 0.9},
                           xmin=0.05,
                           xmax=0.95,
                           xlabel=$\lambda$,
                           x label style={yshift=.34em, font=\small},
                           y=0.01875\textheight,
                           ytick={0, 2, 4, ..., 34},
                           ymin=14,
                           ymax=25,
                           ylabel=f-score (\%),
                           y label style={yshift=-1.1em, font=\small}]
                \addplot[blue, mark=x] coordinates{
                  (0.1, 20.04)
                  (0.2, 20.22)
                  (0.3, 20.37)
                  (0.4, 20.14)
                  (0.5, 20.09)
                  (0.6, 18.46)
                  (0.7, 16.69)
                  (0.8, 15.98)
                  (0.9, 16.43)
                };
              \end{axis}
            \end{tikzpicture}
          }
          \subfigure[\textsc{Duc}]{
            \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
              \pgfkeys{/pgf/number format/.cd, fixed}
              \begin{axis}[x=0.393\linewidth,
                           xtick={0.1, 0.3, ..., 0.9},
                           xmin=0.05,
                           xmax=0.95,
                           xlabel=$\lambda$,
                           x label style={yshift=.34em, font=\small},
                           y=0.01875\textheight,
                           ytick={0, 2, 4, ..., 34},
                           ymin=24,
                           ymax=35,
                           ylabel=f-score (\%),
                           y label style={yshift=-1.1em, font=\small}]
                \addplot[blue, mark=x] coordinates{
                  (0.10, 31.96)
                  (0.20, 32.24)
                  (0.30, 32.40)
                  (0.40, 31.92)
                  (0.50, 31.53)
                  (0.60, 30.50)
                  (0.70, 28.23)
                  (0.80, 26.61)
                  (0.90, 24.63)
                };
              \end{axis}
            \end{tikzpicture}
          }
          \subfigure[SemEval]{
            \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
              \pgfkeys{/pgf/number format/.cd, fixed}
              \begin{axis}[x=0.393\linewidth,
                           xtick={0.1, 0.3, ..., 0.9},
                           xmin=0.05,
                           xmax=0.95,
                           xlabel=$\lambda$,
                           x label style={yshift=.34em, font=\small},
                           y=0.01875\textheight,
                           ytick={0, 2, 4, ..., 34},
                           ymin=8,
                           ymax=19,
                           ylabel=f-score (\%),
                           y label style={yshift=-1.1em, font=\small}]
                \addplot[blue, mark=x] coordinates{
                  (0.10, 11.24)
                  (0.20, 11.33)
                  (0.30, 11.39)
                  (0.40, 12.07)
                  (0.50, 13.43)
                  (0.60, 14.49)
                  (0.70, 14.09)
                  (0.80, 13.96)
                  (0.90, 13.28)
                };
              \end{axis}
            \end{tikzpicture}
          }
          \caption{Comportement de TopicCoRank en fonction de la valeur de
                   $\lambda$
                   \label{fig:lambda_variations}}
        \end{figure} 

    \subsection{Analyse d'erreurs}
    \label{subsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-error_analysis}
      Dans cette section, nous analysons les erreurs d'extraction et
      d'assignement, soit les faux positifs. \TODO{\dots}

    \subsection{Bilan}
    \label{subsec:main-automatic_keyphrase_annotation-supervised_automatic_keyphrase_annotation-conclusion}
      Avec TopicCoRank, nous proposons une extension de la méthode TopicRank,
      que nous présentons dans la
      section~\ref{sec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction}.
      Cette extension apporte à TopicRank la capacité à assigner des
      termes-clés, soit à réaliser la tâche d'indexation par termes-clés dans sa
      globalité. Pour ce faire, TopicCoRank utilise les termes-clés de
      références des documents d'entraînement comme vocabulaire contrôlé, crée
      un graphe dont chaque n\oe{}uds est une entrée du vocabulaire connecté aux
      autres lorsqu'ils sont termes-clés d'un même document, puis unifie se
      graphe au graphe de sujets de TopicRank.

  %-----------------------------------------------------------------------------

  \section{Conclusion}
  \label{sec:main-automatic_keyphrase_annotation-conclusion}
    Dans ce chapitre, nous présentons nos trois contributions à l'indexation
    automatique par termes-clés.
    
    Dans un premier temps, nous analysons les propriétés linguistique des
    termes-clés et proposons une méthode de sélection plus fine des termes-clés
    candidats en nous fondant sur cette analyse. Dans ce travail, nous nous
    intéressons principalement aux adjectifs que contiennent les termes-clés et
    montrons que les adjectifs relationnels sont de meilleurs modificateurs au
    sein des termes-clés.
    
    Dans un second temps, nous proposons une méthode non supervisé à base de
    graphe pour l'extraction automatique de termes-clés. Cette méthode,
    TopicRank, à fait l'objet d'une communication dans une conférence
    internationale~\cite{bougouin2013topicrank} et d'une publication dans un
    journal~\cite{bougouin2014topicrank}. Elle fonctionne en quatres étapes~:
    les sujets abordés dans le document sont identifiés grâce aux termes-clés
    candidats qui les véhicules, les relations entre les sujets sont
    représentées par un graphe, le graphe est utilisé pour déterminer
    l'importance de chaque sujet et un terme-clé est extrait pour chacun des $k$
    sujets les plus importants. En plus de montrer que TopicRank obtient de
    meilleures performance que les méthodes de références, nos évaluation
    mettent en évidence des résultats théoriques pouvant doubler si la stratégie
    pour choisir le terme-clé le plus représentatif de chaque sujet, parmi les
    termes-clés candidats qui le véhicule, était optimale.

    Dans un dernier temps, nous étendons TopicRank afin de lui permettre
    d'aborder la tâche d'indexation par termes-clés dans son ensemble,
    c'est-à-dire d'extraire et d'assigner des termes-clés. Cette méthode,
    TopicCoRank, tire profit des documents dont nous connaissons déjà les
    termes-clés pour simuler un vocabulaire contrôlé, le projeter dans un graphe
    et ordonner conjointement les entrées du vocabulaire et les sujets du
    documents. Nos évaluation montrent que TopicCoRank est plus performante
    sur chacune des deux tâches et qu'elle l'est d'autant plus quand elle
    effectue les deux simultanément.
    
