\chapter{Indexation par termes-clés en domaines de spécialités}
\label{chap:main-domain_specific_keyphrase_annotation}
  \chaptercite{
    [\dots] the essential challenge of keyphrase extraction is the vocabulary
    gap between documents and keyphrases.
  }{
    \newcite{liu2011vocabularygap}
  }

  %-----------------------------------------------------------------------------

  \section{Introduction}
  \label{sec:main:domain_specific_keyphrase_annotation-introduction}
    Dans ce chapitre, nous nous interessons à l'application de l'indexation par
    termes-clés en domaines de spécialités. Dans la littérature, l'indexation
    par termes-clés se divise en deux catégories~: l'extraction de termes-clés,
    qui fournit des termes-clés apparaissant dans le contenu du document, et
    l'assignement de termes-clés, qui fournit des termes-clés appartenant à un
    vocabulaire contrôlé et n'apparaissant pas nécessairement dans le document.
    Alors que l'indexation par termes-clés est principalement réalisée au seul
    moyen de l'extraction de termes-clés, nous montrons que l'assignement de
    termes-clés joue un rôle important en domaines de spécialités.

    Dans un premier temps, nous décrivons le comportement des indexeurs
    professionnels d'une bibliothèque numérique (Inist), puis, dans un second
    temps, nous proposons une automatisation de ce comportement. Les indexeurs
    professionnels assignent à chaque document des termes-clés du domaine,
    répertoriés par un vocabulaire contrôlé, et extraient des termes-clés hors
    du vocabulaire contrôlé lorsqu'ils sont très spécifiques au document ou
    lorsqu'il s'agit de nouveaux concepts. Pour reproduire ce comportement, nous
    étendons nos travaux précédents par l'intégration, dans le graphe, des
    termes-clés du domaine, c'est-à-dire du vocabulaire contrôlé. Ce dernier
    n'étant cependant pas disponible dans tous les cas de figure, nous en
    construisons une approximation à partir des termes-clés de référence des
    données d'entraînement.

    Enfin, nous validons l'application de nos travaux en domaines de spécialités
    grâce à une série d'évaluations manuelles réalisées par des indexeurs
    professionnels. Pour cette série d'évaluations, nous proposons un protocole
    et des metriques permettant d'évaluer deux aspects. Le premier aspect permet
    d'évaluer la performance d'une méthode d'après la pertinence des termes-clés
    qu'elle extrait et/ou assigne. Le second aspect, quant à lui, permet
    d'évaluer la performance d'une méthode en terme de quantité d'information
    importante capturée par ces termes-clés.\\

  %-----------------------------------------------------------------------------

  \section[Indexation manuelle dans le contexte d'une bibliothèque scientifique]{Indexation manuelle dans le contexte d'une bibliothèque\\scientifique}
  \label{sec:main-domain_specific_keyphrase_annotation-manual_keyphrase_annotation}
    Dans cette section, nous présentons la tâche d'indexation par termes-clés
    réalisée manuellement au sein d'un organisme gestionnaire d'une bibliothèque
    scientifique. Pour cela, nous nous fondons sur les propos recueillis des
    indexeurs professionnels de l'Inist, dont les données Termith sont issues.
    La bibliothèque numérique scientifique de l'Inist est indexée à l'aide de
    termes-clés majoritairement issus de ressources terminologique définies et
    maintenues par les indexeurs de l'Inist.

    \subsection{Principes généraux}
    \label{subsec:main-domain_specific_keyphrase_annotation-manual_keyphrase_annotation-principles}
      L'indexation manuelle à l'Inist est régie par cinq pilliers~:
      \begin{enumerate}
        \item{Conformité~: les termes-clés doivent être conformes au contenu du
              document et au language documentaire utilisé dans son domaine~;}
        \item{Exhaustivité~: les termes-clés doivent représenter tous les
              aspects importants du document, même lorsque ceux-ci sont
              implicites~;}
        \item{Homogénéité~: les termes-clés des documents d'un même domaine
              doivent être cohérents et identiques lorsqu'ils représentent le
              même concept~;}
        \item{Spécificité~: les termes-clés doivent décrire le contenu d'un
              document au niveau le plus spécifique du document et peuvent
              parfois être accompagnés de termes-clés plus génériques afin de
              restituer le contenu du document dans son domaine~;}
        \item{Impartialité~: les termes-clés ne doivent pas être représentatif
              d'un jugement apporté par l'indexeur.}
      \end{enumerate}
      Ces cinq pilliers font écho à des principes que nous retrouvons aussi bien
      en extraction et en assignement automatique de termes-clés. Les trois
      premiers pilliers impliquent une indexation majoritairement contrôlée,
      comme en assignement automatique de termes-clés, et le quatrième pillier
      implique une indexation en partie libre, comme en extraction de
      termes-clés. Le deuxième pillier est celui le plus difficile à respecter
      de manière automatique. Une seule méthode, à notre connaissance, est
      actuellement capable d'extraire des termes-clés représentant des aspects
      implicites d'un documents~\cite{liu2011vocabularygap}, mais elle n'a
      jamais été appliquée pour effectuer de l'indexation contrôlée.

      Les principes généraux de l'indexation manuelle par termes-clés remettent
      en cause la séparation entre extraction et assignement automatique de
      termes-clés dans le contexte de l'indexation automatique en domaines de
      spécialités. Une indexation par termes-clés à un niveau professionnel se
      doit de respecter le language de spécialité employé dans le domaine des
      documents indexés, mais elle se doit aussi d'être exhaustive et donc de
      fournir des termes-clés très spécifiques (libres), voir de nouveaux
      concepts.

    \subsection{Ressources}
    \label{subsec:main-domain_specific_keyphrase_annotation-manual_keyphrase_annotation-resources}
      L'indexation par termes-clés réalisée par les indexeurs professionnels de
      l'Inist s'appuie sur plusieurs ressources. Ces dernières sont
      représentatives d'une expertise de terrain dans chaque domaine de
      spécialité (grille d'indexation), d'une expertise terminologique
      (vocabulaire contrôlé) et d'une expertise documentaire (règles de
      pré-indexation).

      \subsubsection{Grille d'indexation}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-manual_keyphrase_annotation-resources-indexing_guidelines}
        La grille d'indexation est un guide qui indique les notions qui peuvent
        être pertinentes à indexer selon le domaine de spécialité et la
        problématique du document. Elle peut-être assimilée à un formulaire à
        compléter pour chaque document. En linguistique, par exemple, les
        termes-clés fournis doivent répondre aux questions suivantes~:
        \TODO{faire un tableau avec une colonne "champs" et une colonne
        "exemple" ou nous montrons ou se répartissent les termes-clés de la
        notice de linguistique utilisée en exemple dans le chapitre Ressources}
        \begin{enumerate}
          \item{Quel est le domaine d'investigation~?}
          \begin{itemize}
            \item{biologie du language}
            \item{psychologie du language}
            \item{sociologie du language}
            \item{etc.}
          \end{itemize}
          \item{Quel est l'objet d'étude~?}
          \begin{itemize}
            \item{langue~: laquelle~?}
            \item{formalisme~: lequel~?}
            \item{\oe{}uvre~: laquelle~?}
            \item{etc.}
          \end{itemize}
          \item{Quel est l'environnement d'étude~?}
          \begin{itemize}
            \item{historique~: date~? siècle~? période~? etc.}
            \item{géographique~: pays~? région~? ville~?}
            \item{culturel~: quel peuple~?}
            \item{etc.}
          \end{itemize}
        \end{enumerate}
        La grille d'indexation est un canevas donné à titre indicatif aux
        indexeurs. Ces derniers sont les seuls juges pour décider si elle est
        adaptée ou non pour indexer un document.

      \subsubsection{Vocabulaire contrôlé}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-manual_keyphrase_annotation-resources-controlled_vocabulary}
        Le vocabulaire contrôlé est un lexique qui recense et organise les
        termes d'un domaine de spécialité. Dans le contexte de l'indexation par
        termes-clés, il définit le language documentaire à utiliser pour les
        termes-clés. C'est la ressource indispensable pour assurer la conformité
        et l'homogénéité de l'indexation. Il doit toutefois être mis à jour
        régulièrement, soit par une veille terminologique, soit au fur et à
        mesure des indexations manuelles.

      \subsubsection{Règles de pré-indexation}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-manual_keyphrase_annotation-resources-preindexing_rules}
        Les règles de pré-indexation sont des règles qui définissent les
        termes-clés à assigner en fonction d'unités textuelles données. Deux
        types de règles sont distingués~: celles qui assignent un terme-clé du
        vocabulaire contrôlé lorsqu'une unité textuelle donnée occure dans le
        document (\TODO{exemple}) et celles qui assignent un terme-clé générique
        du vocabulaire contrôlé lorsqu'un terme-clé donné est assigné au
        document (\TODO{exemple}). Fortement couplées avec le vocabulaire
        contrôlé, les règles de préindexation permettent d'assurer la conformité
        et l'homogénéité de l'indexation. Elles contribuent aussi a
        l'exhaustivité, en permettant l'assignement d'aspects implicites dans le
        document, et à la spécificité, en restituant le contenu du document dans
        son domaine grâce au termes-clés génériques.

        Les règles de pré-indexation sont compilées par sous-domaines et
        interprétées, à la demande d'un indexeur, par un système automatique qui
        fournit le résultat à l'indexeur pour modifications et validations.
        Selon le niveau d'expertise de l'indexeur, ces règles ne sont pas
        nécessairement utilisées. Nous qualifions donc l'indexation réalisée à
        l'Inist d'indexation manuelle, mais il est plus juste de la qualifier de
        semi-automatique dans certains cas.

    \subsection{Bilan}
    \label{subsec:main-domain_specific_keyphrase_annotation-manual_keyphrase_annotation-conclusion}
      L'indexation manuelle en domaines de spécialités que nous avons présenté
      suit des principes d'indexation que nous retrouvons soit en extraction
      automatique de termes-clés, soit en assignement automatique de
      termes-clés. Alors qu'en indexation automatique par termes-clés, les
      méthodes d'extractions prédominent sur les méthodes d'assignement, notre
      étude de l'indexation réalisée par des indexeurs professionnels montre que
      l'assignement est la pratique préférée, car elle assure conformité,
      exhaustivité et homogénéité, et que l'extraction doit servir à la
      compléter.

  %-----------------------------------------------------------------------------

  \section{Indexation automatique en domaines de spécialités}
  \label{sec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_extraction}
    \TODO{\dots}

    \subsection{TopicCoRank}
    \label{subsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-topiccorank}
      TopicCoRank est une méthode supervisée à base de graphe qui réalise
      simultanément extraction et assignement de termes-clés. Il s'agit d'une
      extension de TopicRank. Elle étend les étapes suivantes de ce dernier~:
      construction du graphe, ordonnancement et sélection des termes-clés.

      \subsubsection{Construction du graphe}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-graph_construction}
        Afin de réaliser simultanément extraction et assignement de termes-clés,
        TopicCoRank unifie deux graphes représentant le document (graphe de
        sujets) et son domaine (graphe de termes-clés de référence). Ce dernier
        graphe est construit à partir des termes-clés de référence des documents
        d'entraînement fournis avec une collection de données. Comme
        \newcite{chaimongkol2013technicaltermextraction} l'ont fait avant nous
        pour l'extraction de termes techniques, nous faisons l'hypothèse que les
        termes-clés de référence des documents d'entraînement constituent la
        terminologie du domaine et nous les utilisons comme substitut au
        vocabulaire contrôlé. Contrairement au termes-clés candidats
        sélectionnés dans le document, les termes-clés de référence ne sont pas
        jugés redondants. TopicCoRank ne les groupe pas avant de construire le
        graphe du domaine.

        Soit le graphe unifié $G = (N, A = A_{\textnormal{\textit{interne}}}
        \cup A_{\textnormal{\textit{externe}}})$ non orienté. $N$ dénote
        indifféremment les sujets et les termes-clés de référence. $A$ regroupe
        les arêtes $A_{\textnormal{\textit{interne}}}$, qui connectent deux
        sujets ou deux termes-clés de référence, et les arêtes
        $A_{\textnormal{\textit{externe}}}$, qui connectent un sujet à un
        terme-clé de référence (cf. figure~\ref{fig:topiccorank_graph}). Le
        graphe de sujets et le graphe du domaine sont unifiés grâce aux arêtes
        $A_{\textnormal{\textit{externe}}}$. Une arête
        $A_{\textnormal{\textit{externe}}}$ est ajouté pour connecter un sujet
        et un terme-clé si, et seulement si, le terme-clé fait partie des
        termes-clés candidats qui composent le sujet. En d'autres termes,
        TopicCoRank considère le domaine comme une carte conceptuelle et
        connecte le document au domaine par l'intermédiaire des concepts qu'ils
        partagent. Pour favoriser les connections, et à la manière du groupement
        en sujets, la méthode de racinisation de
        \newcite{porter1980suffixstripping} est utilisée en amont des
        comparaisons.
        \begin{figure}
          \newcommand{\xslant}{0.25}
          \newcommand{\yslant}{0}

          \centering
          \begin{tikzpicture}[transform shape, scale=.75]
            % frame
            \node [draw,
                   rectangle,
                   minimum width=.7\linewidth,
                   minimum height=8em,
                   xslant=\xslant,
                   yslant=\yslant] (domain_graph) {};
            \node [above=of domain_graph,
                   xshift=.36\linewidth,
                   yshift=8em,
                   anchor=south east] (domain_graph_label) {termes-clés de référence};

            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=.3\linewidth,
                 yshift=5em] (domain_node1) {$V_1$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=-.3\linewidth,
                   yshift=5em] (domain_node2) {$V_2$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   yshift=5em] (domain_node3) {$V_3$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=.15\linewidth,
                   yshift=.75em] (domain_node4) {$V_4$};
            \node [draw,
                   circle,
                   above=of domain_graph,
                   xshift=-.15\linewidth,
                   yshift=.75em] (domain_node5) {$V_5$};

            \draw (domain_node1) -- (domain_node3);
            \draw (domain_node2) -- (domain_node3);
            \draw (domain_node2) -- (domain_node4);
            \draw (domain_node4) -- (domain_node5);
            \draw (domain_node4) -- (domain_node3);

            % document
            \node [draw,
                   rectangle,
                   minimum width=.7\linewidth,
                   minimum height=8em,
                   xslant=\xslant,
                   yslant=\yslant,
                   above=of domain_graph,
                   xshift=-2em] (document_graph) {};
            \node [below=of document_graph,
                   xshift=-.36\linewidth,
                   yshift=-8em,
                   anchor=north west] (document_graph_label) {sujets du document};

            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                   xshift=.3\linewidth,
                   yshift=-5em] (document_node1) {$V_6$};
            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                   xshift=-.3\linewidth,
                   yshift=-5em] (document_node2) {$V_7$};
            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                 yshift=-5em] (document_node3) {$V_8$};
            \node [draw,
                   circle,
                   regular polygon sides=8,
                   below=of document_graph,
                   xshift=.15\linewidth,
                   yshift=-.75em] (document_node4) {$V_9$};

            \draw (document_node2) -- (document_node3);
            \draw (document_node3) -- (document_node1);
            \draw (document_node1) -- (document_node4);
            \draw (document_node3) -- (document_node4);

            % extra link
            \draw [dashed] (document_node2) -- (domain_node2);
            \draw [dashed] (document_node3) -- (domain_node3);
            \draw [dashed] (document_node4) -- (domain_node1);
            \draw [dashed] (document_node3) -- (domain_node4);

            % legend
            \node [right=of document_graph, xshift=2em, yshift=-9.25em] (legend_title) {\underline{Légende~:}};
            \node [below=of legend_title, xshift=-1em, yshift=2em] (begin_inner) {};
            \node [right=of begin_inner] (end_inner) {: $A_\textnormal{\textit{interne}}$};
            \node [below=of begin_inner, yshift=1.5em] (begin_outer) {};
            \node [right=of begin_outer] (end_outer) {: $A_\textnormal{\textit{externe}}$};

            \draw (legend_title.north  -| end_outer.east) rectangle (end_outer.south -| legend_title.west);

            \draw (begin_inner) -- (end_inner);
            \draw [dashed] (begin_outer) -- (end_outer);
          \end{tikzpicture}
          \caption{Illustration du graphe unifié utilisé par TopicCoRank
                   \label{fig:topiccorank_graph}}
        \end{figure}

        Pour permettre un ordonnancement conjoint des sujets et des termes-clés
        de référence, le schéma de connexion entre deux sujets et entre deux
        termes-clés de référence (arêtes $A_\textnormal{\textit{interne}}$) est
        homogénéisé. En effet, si les conditions de connexion et si la
        pondération des arêtes ne sont pas, respectivement, sémantiquement
        équivalentes et du même ordre de grandeur, alors l'impact du domaine sur
        le document, et inversement, est marginal et inexploitable. TopicCoRank
        connecte deux sujets ou deux termes-clés de référence $n_i$ et $n_j$
        lorsqu'il apparaissent dans le même contexte et pondère leur arête par
        le nombre de fois que cela se produit ($\textnormal{poids}(n_i, n_j)$),
        normalisé par le nombre de contextes explorés. Lorsqu'il s'agit des
        sujets, le contexte est une phrase du document~; lorsqu'il s'agit des
        termes-clés de référence, le contexte est l'ensemble des termes-clés de
        référence d'un document d'entraînement. Ne pouvant pas créer un graphe
        complet du domaine à partir de ce schéma de connexion, nous n'utilisons
        pas un graphe complet pour le graphe de sujets. Afin d'éviter d'utiliser
        une valeur pour la fenêtre de cooccurrences, nous difinissons la phrase
        comme fenêtre.

      \subsubsection{Ordonnancement conjoint des sujets et des termes-clés de référence}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-co_ranking}
        L'ordonnancement conjoint des sujets et des termes-clés de référence
        établit l'ordre d'importance des sujets du document et des termes-clés
        de référence du domaine vis-à-vis du contenu du document. Le score
        d'importance attribué aux sujets et aux termes-clés de référence est
        obtenu avec le même algorithme et la même instance de l'algorithme.
        L'ordonnancement fournit donc une seule liste ordonnée, mêlant sujets et
        termes-clés de référence.

        Dans TopicCoRank, le principe de la recommandation  de TopicRank est
        repris et adapté au problème d'ordonnancement conjoint. Les premières
        hypothèses de recommandation sont donc les mêmes que celle de
        TopicRank~:
        \begin{itemize}
          \item{un sujet est d'autant plus important s'il est fortement connecté
                à un grand nombre de sujets et si les sujets avec lesquels il
                est fortement connecté sont importants~;}
          \item{un terme-clé de référence est d'autant plus important s'il est
                fortement connecté à un grand nombre de termes-clés de référence
                et si les termes-clés de référence avec lesquels il est connecté
                sont importants.}
        \end{itemize}
        Ces hypothèses de recommandation, que nous qualifions d'internes,
        permettent d'établir l'importance des sujets les uns par rapport aux
        autres et l'importance des termes-clés de référence les uns par rapport
        aux autres. Cependant, elles ne permettent pas de tirer profit des
        termes-clés de référence importants pour renforcer l'importance des
        sujets qui y sont connectés et, inversement, elles ne permettent pas de
        tirer profit des sujets importants pour renforcer l'importance des
        termes-clés de référence auxquels ils sont connectés. De plus,
        l'importance des termes-clés de référence est indépendante du document
        et, pour la même collection de données, l'importance des termes-clés de
        référence est toujours la même. Nous ajoutons donc deux nouvelles
        hypothèses de recommandation, que nous qualifions d'externes~:
        \begin{itemize}
          \item{un sujet est d'autant plus important s'il contient des
                termes-clés de référence importants~;}
          \item{un terme-clé de référence est d'autant plus important
                \underline{vis-à-vis du contenu du document} s'il est connecté à
                l'un de ses sujets importants.}
        \end{itemize}
        Sujets et termes-clés de référence sont ainsi évalués d'après leur usage
        dans le document et leur usage (global) dans le domaine.
        L'ordonnancement des uns joue un rôle sur celui des autres et permet
        ainsi d'effectuer extraction et assignement conjointement.

        Mathématiquement, la formule d'ordonnancement de TopicRank est adaptée
        en préservant la recommandation interne
        ($R_{\textnormal{\textit{interne}}}$) et en remplaçant l'aléa, exprimé
        avec le terme $(1 - \lambda)$, par la recommandation externe
        ($R_{\textnormal{\textit{externe}}}$)~:
        \begin{align}
          S(n_i) &= (1 - \lambda)\ R_{\textnormal{\textit{externe}}}(n_i) + \lambda\ R_{\textnormal{\textit{interne}}}(n_i)\label{math:topiccorank}\\
          R_{\textnormal{\textit{interne}}}(n_i) &= \sum_{n_j \in A_{\textnormal{\textit{interne}}}(n_i)}{\frac{\textnormal{poids}(n_j, n_i) \times S(n_j)}{\mathlarger\sum_{n_k \in A_{\textnormal{\textit{interne}}}(n_j)}{{\textnormal{poids}(n_j, n_k)}}}}\label{math:rin}\\
          R_{\textnormal{\textit{externe}}}(n_i) &= \sum_{n_j \in A_{\textnormal{\textit{externe}}}(n_i)}{\frac{S(n_j)}{|A_{\textnormal{\textit{externe}}}(n_j)|}}\label{math:rout}
        \end{align}
        où $A_{\textnormal{\textit{interne}}}(n_i)$ représente l'ensemble de
        tous les n\oe{}uds connectés au n\oe{}ud $n_i$ par une arête
        $A_\textnormal{\textit{interne}}$, où
        $A_{\textnormal{\textit{externe}}}(n_i)$ représente l'ensemble de tous
        les n\oe{}uds connectés au n\oe{}ud $n_i$ par une arête
        $A_\textnormal{\textit{externe}}$ et où le facteur $\lambda$ permet
        désormais de définir la recommandation la plus influente entre
        $R_{\textnormal{\textit{interne}}}$ et
        $R_{\textnormal{\textit{externe}}}$. Par défaut, nous définissons
        $\lambda=0,5$ afin de donner autant de poids aux deux recommandations.

      \subsubsection{Sélection des termes-clés}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-keyphrase_selection}
        Pour terminer l'indexation par termes-clés, TopicCoRank utilise l'ordre
        d'importance $S$ des sujets et termes-clés de référence pour déterminer
        les termes-clés du document. Les $k$ n\oe{}uds du graphe unifié ayant
        obtenu les meilleurs scores sont retenus, qu'ils soient des sujets ou
        des termes-clés de référence.

        Avant la troncature aux $k$ meilleurs sujets et termes-clés de
        référence, certains termes-clés de référence sont retirés si leur
        ordonnancement n'a pas été influencé par le document. En effet, le
        graphe du domaine peut, dans certain cas, être constitué de composantes
        connexes, c'est-à-dire composé de sous-graphes qui ne sont connectés par
        aucune arête (\TODO{exemple}), soit dans notre cas des groupes de
        termes-clés de référence qui ne partagent aucun contexte. Si aucun
        terme-clé de référence d'une composante connexe n'est connecté à un
        sujet du document, alors l'ordonnancement au sein de cette composante
        connexe n'est pas influencé par le document. Il n'est donc pas pertinent
        d'en tenir compte.

        Lorsque un n\oe{}ud retenu représente un sujet, C'est la même stratégie
        que celle de TopicRank qui est appliquée. Pour un sujet donné, le
        terme-clé extrait est son terme-clé candidat qui apparaît en premier
        dans le document.

      \subsubsection{Exemple}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_extraction-topiccorank-exemple}
        \TODO{\dots}

    \subsection{Évaluation}
    \label{subsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation}
      Pour valider notre approche, nous réalisons deux séries d'expériences. Une
      première série pour comparer TopicCoRank à deux méthodes de référence et
      une seconde série pour analyser sont comportement.

      \subsubsection{Méthodes de référence}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-baselines}
        Dans nos expériences, nous comparons TopicCoRank à TopicRank et à
        une méthode simple d'assignement (\textit{Assignement}). Pour cette
        dernière, nous utilisons le même vocabulaire contrôlé que TopicCoRank et
        ordonnons les entrées de celui-ci d'après leur fréquence d'apparition
        dans le document. \TODO{remplacer \textit{Assignement} par KEA++}

        Nous comparons aussi TopicCoRank à deux de ces variantes possibles. La
        première, TopicCoRank$_\textnormal{\textit{extr.}}$, ne réalise que
        l'extraction de termes-clés~; la seconde,
        TopicCoRank$_\textnormal{\textit{assign.}}$, n'effectue que
        l'assignement.

      \subsubsection{Collections de données}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-evaluation_data}
        Pour évaluer TopicCoRank, nous utilisons la collection de documents de
        linguistique de la ressource Termith et les collections SemEval et
        \textsc{Duc}.
        
        Pour la collection de Termith, nous construisons le graphe du domaine à
        partir des documents d'entraînement~; pour SemEval, nous construisons
        quatre graphes de domaine à partir des documents d'entraînement des
        quatre catégories \textsc{Acm} (C2.4, H3.3, I2.11 et J4) et utilisons
        l'un ou l'autre de ces graphes selon la catégorie du document traité par
        TopicCoRank~; pour \textsc{Duc}, nous n'utilisons pas sa répartition
        entraînement--test proposée dans la
        section~\ref{sec:main-domain_specific_keyphrase_annotation-keyphrase_candidate_selection}
        (page~\pageref{sec:main-domain_specific_keyphrase_annotation-keyphrase_candidate_selection}),
        mais nous construisons un graphe de domaine unique pour chaque document.
        Comme chaque document de \textsc{Duc} appartient à l'un des 30 sujets
        d'actualités couverts, nous utilisons leur sujet d'actualité pour
        construire leur graphe de domaine à partir des autres documents traitant
        du même sujet.
      
      \subsubsection{Mesures d'évaluation}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-evaluation_measures}
        Les performances des méthodes d'extraction de termes-clés sont exprimées
        en termes de précision (P), rappel (R) et f-mesure (f1-mesure, F). En
        accord avec l'évaluation menée dans les travaux précédents, nous
        considérons correcte l'extraction d'une variante flexionnelle d'un
        terme-clé de référence~\cite{kim2010semeval}. Les opérations de
        comparaison entre les termes-clés de référence et les termes-clés
        extraits sont donc effectuées à partir de la racine des mots qui les
        composent, d'après la méthode de \newcite{porter1980suffixstripping}.

        Nous représentons aussi les résultats sous la forme de courbes de
        rappel--précision. Celles-ci permettent d'observer si une méthode domine
        les autres pour les critère de rappel et de précision. En optimisation
        multi-critères, nous parlons de front de Pareto optimal, c'est à dire de
        la méthode pour laquelle aucune autre méthode n'obtient de meilleures
        performances. Pour générer ces courbes, nous calculons la précision et
        le rappel lorsque  le nombre de termes-clés extraits/assignés varie de
        un jusqu'au nombre total de termes-clés
        candidats~\cite{hassan2010conundrums}.
      
      \subsubsection{Comparaison de TopicRank++ avec l'existant}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-comparison}
        Le tableau~\ref{tab:topiccorank-comparison_results} montre les
        performances de TopicCoRank comparées à celles des méthodes de
        référence. Dans un premier temps, nous observons que l'assignement de
        termes-clés (avec \textit{Assignement} et
        TopicCoRank$_\textnormal{\textit{assign.}}$) est plus aisé que
        l'extraction de termes-clés (avec TopicRank et
        TopicCoRank$_\textnormal{\textit{extr.}}$) pour les collections Termith
        et \textsc{Duc}, et qu'elle est plus difficile pour la collection
        SemEval. Les résultats montrent toutefois qu'il est bénéfique
        d'effectuer les deux simultanément. En effet, quelque soit la collection
        TopicCoRank obtient de meilleures performances que TopicRank,
        \textit{Assignement} et TopicCoRank$_\textnormal{\textit{extr.}}$. Dans
        le cas de la collection Termith, TopicCoRank donne la seconde meilleure
        performance après TopicCoRank$_\textnormal{\textit{assign.}}$. Dans ce
        cas de figure, la proportion de termes-clés \og{}à assigner\fg{} est
        tellement importante qu'effectuer uniquement de l'assignement est plus
        fructueux. Toutefois, la pertinence de TopicCoRank n'est en aucun cas
        remise en cause, car TopicCoRank$_\textnormal{\textit{assign.}}$ à une
        performance significativement meilleure à celle de la méthode de
        référence \textit{Assignement}. Globalement, les résultats montrent
        l'apport d'ordonner conjointement les sujets du document et les
        termes-clés de référence du domaine pour extraire et assigner des
        termes-clés.
        \begin{table}[h!]
          \centering
          \resizebox{\linewidth}{!}{
            \begin{tabular}{l|ccc|ccc|ccc|ccc}
              \toprule
              \multirow{2}{*}{\textbf{Méthode}} & \multicolumn{3}{c|}{\textbf{Linguistique}} & \multicolumn{3}{c|}{\textbf{Sciences de l'info.}} & \multicolumn{3}{c|}{\textbf{Archéologie}} & \multicolumn{3}{c}{\textbf{Chimie}}\\
              \cline{2-13}
              & P & R & F & P & R & F & P & R & F & P & R & F\\
              \hline
              TopicRank & 11.3 & 13.1 & 12.0 & 12.2 & 12.8 & 12.1 & 28.1 & 19.1 & 22.2 & 14.0 & 11.3 & 11.8\\
              KEA++ & 11.6 & 13.0 & 12.1 & $~~$9.5 & 10.2 & $~~$9.6 & 23.5 & 16.2 & 18.8 & 11.4 & $~~$8.5 & $~~$9.2\\
              \hline
              TopicCoRank$_\textnormal{extr.}$ & 14.6 & 16.8 & 15.4 & 15.5 & 15.9 & 15.3 & 36.4 & 24.6 & 28.7 & 15.4 & 12.0 & 12.9\\
              TopicCoRank$_\textnormal{assign.}$ & \textbf{24.9} & \textbf{28.9} & \textbf{26.4} & \textbf{19.8} & \textbf{20.0} & \textbf{19.3} & \textbf{49.1} & \textbf{33.3} & \textbf{38.8} & \textbf{19.8} & \textbf{14.8} & \textbf{16.2}\\
              \hline
              TopicCoRank & 19.0 & 22.0 & 20.1 & 17.1 & 17.5 & 16.8 & 39.5 & 26.6 & 31.1 & 16.7 & 13.1 & 14.0\\
              \bottomrule
            \end{tabular}
          }
        \caption[
          Résultat de l'extraction de dix termes-clés avec TopicRank,
          \textsc{Kea++}, TopicCoRank$_\textnormal{\textit{extr.}}$,
          TopicCoRank$_\textnormal{\textit{assign.}}$ et TopicCoRank
        ]{
          Résultat de l'extraction de dix termes-clés avec TopicRank,
          \textsc{Kea++}, TopicCoRank$_\textnormal{\textit{extr.}}$,
          TopicCoRank$_\textnormal{\textit{assign.}}$ et TopicCoRank. $\dagger$
          et $\ddagger$ indiquent une amélioration significative par rapport à
          TopicRank et \textit{Assignement}, respectivement.
          \label{tab:topiccorank-comparison_results}}
        \end{table}
        \begin{table}[h!]
          \centering
          %\resizebox{\linewidth}{!}{
            \begin{tabular}{l|ccc|ccc|ccc}
              \toprule
              \multirow{2}{*}{\textbf{Méthode}} & \multicolumn{3}{c|}{\textbf{\textsc{De}ft}} & \multicolumn{3}{c|}{\textbf{DUC}} & \multicolumn{3}{c}{\textbf{SemEval}}\\
              \cline{2-10}
              & P & R & F$^{~~}$ & P & R & F$^{~~}$ & P & R & F$^{~~}$\\
              \hline
              TopicRank & \textbf{11,7} & \textbf{21,7} & \textbf{15,1$^{~~}$} & 17,8 & 22,7 & 19,7$^{~~}$ & 14,6 & 10,1 & 11,8\\
              KEA++ & --- & --- & ---$^{~~}$ & --- & --- & ---$^{~~}$ & --- & --- & ---\\
              \hline
              TopicCoRank$_\textnormal{\textit{extr.}}$ & $~~$8,9 & 16,6 & 11,4$^{~~}$ & 25,5 & 32,4 & 28,1$^{\dagger}$ & 15,2 & 10,6 & 12,4\\
              TopicCoRank$_\textnormal{\textit{assign.}}$ & 7,0 & 13,1 & $~~$9,0$^{~~}$ & 25,9 & 33,3 & 28,8$^{\dagger}$ & 11,6 & ~~8,3 & ~~9,5\\
              \hline
              TopicCoRank & $~~$8,9 & 16,6 & 11,5$^{~~}$ & \textbf{28,4} & \textbf{36,6} & \textbf{31,5$^{\dagger}$} & \textbf{16,4} & \textbf{11,6} & \textbf{13,4}\\
              \bottomrule
            \end{tabular}
          %}
        \caption[
          Résultat de l'extraction de dix termes-clés avec TopicRank,
          \textsc{Kea++}, TopicCoRank$_\textnormal{\textit{extr.}}$,
          TopicCoRank$_\textnormal{\textit{assign.}}$ et TopicCoRank
        ]{
          Résultat de l'extraction de dix termes-clés avec TopicRank,
          \textsc{Kea++}, TopicCoRank$_\textnormal{\textit{extr.}}$,
          TopicCoRank$_\textnormal{\textit{assign.}}$ et TopicCoRank. $\dagger$
          et $\ddagger$ indiquent une amélioration significative par rapport à
          TopicRank et \textit{Assignement}, respectivement.
          \label{tab:topiccorank-comparison_results}}
        \end{table}
        
        La figure~\ref{fig:topiccorank-pr_curves} compare les comportements
        respectifs de TopicRank, \textit{Assignement} et TopicCoRank à l'aide de
        courbes rappel--précision. Ces dernières confirment la domination de
        TopicCoRank par rapport aux deux autres méthodes (front de Pareto
        optimal). Elles montrent aussi un phénomène intéressant~: TopicCoRank à
        un comportement similaire à celui de TopicRank, boosté par sa capacité à
        assigner. Le processus d'ordonnancement initial de TopicRank n'est pas
        dénaturé et l'ajout du graphe du domaine agit effectivement comme une
        extension.
        \begin{figure}[h!]
            \centering
            \subfigure[Termith]{
              \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
                \pgfkeys{/pgf/number format/.cd, fixed}
                \begin{axis}[x=0.00855\linewidth,
                             xtick={0, 20, 40, ..., 100},
                             xmin=0,
                             xmax=40,
                             xlabel=rappel (\%),
                             x label style={yshift=.34em, font=\small},
                             y=0.00855\linewidth,
                             ytick={0, 10, 20, ..., 100},
                             ymin=0,
                             ymax=40,
                             ylabel=précision (\%),
                             y label style={yshift=-1.1em, font=\small},
                             legend style={font=\footnotesize}]
                  \addplot [red, mark=+] file {input/data/inist_topicrank.csv};
                  \addplot [green, mark=-] file {input/data/inist_assignment.csv};
                  \addplot [blue, mark=x] file {input/data/inist_topiccorank.csv};
                  \addplot [dotted, domain=20:40] {(30 * x) / ((2 * x) - 30)};
                  \addplot [dotted, domain=10:40] {(20 * x) / ((2 * x) - 20)};
                  \addplot [dotted, domain=5:40] {(10 * x) / ((2 * x) - 10)};
                  \legend{TopicRank, \textit{Assignement}, TopicCoRank};
                \end{axis}
                \node at (4.85,3.1) [anchor=east] {\footnotesize{F=0.30}};
                \node at (4.85,1.8) [anchor=east] {\footnotesize{F=0.20}};
                \node at (4.85,0.9) [anchor=east] {\footnotesize{F=0.10}};
              \end{tikzpicture}
            }
            \subfigure[\textsc{Duc}]{
              \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
                \pgfkeys{/pgf/number format/.cd, fixed}
                \begin{axis}[x=0.0057\linewidth,
                             xtick={0, 20, 40, ..., 100},
                             xmin=0,
                             xmax=60,
                             xlabel=rappel (\%),
                             x label style={yshift=.34em, font=\small},
                             y=0.0057\linewidth,
                             ytick={0, 15, 30, ..., 100},
                             ymin=0,
                             ymax=60,
                             ylabel=précision (\%),
                             y label style={yshift=-1.1em, font=\small},
                             legend style={font=\footnotesize}]
                  \addplot [red, mark=+] file {input/data/duc_topicrank.csv};
                  \addplot [green, mark=-] file {input/data/duc_assignment.csv};
                  \addplot [blue, mark=x] file {input/data/duc_topiccorank.csv};
                  \addplot [dotted, domain=30:60] {(40 * x) / ((2 * x) - 40)};
                  \addplot [dotted, domain=20:60] {(30 * x) / ((2 * x) - 30)};
                  \addplot [dotted, domain=10:60] {(20 * x) / ((2 * x) - 20)};
                  \addplot [dotted, domain=5:60] {(10 * x) / ((2 * x) - 10)};
                  \legend{TopicRank, \textit{Assignement}, TopicCoRank};
                \end{axis}
                \node at (4.85,2.6) [anchor=east] {\footnotesize{F=0.40}};
                \node at (4.85,1.8) [anchor=east] {\footnotesize{F=0.30}};
                \node at (4.85,1.15) [anchor=east] {\footnotesize{F=0.20}};
                \node at (4.85,0.6) [anchor=east] {\footnotesize{F=0.10}};
              \end{tikzpicture}
            }
            \subfigure[SemEval]{
              \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
                \pgfkeys{/pgf/number format/.cd, fixed}
                \begin{axis}[x=0.00855\linewidth,
                             xtick={0, 20, 40, ..., 100},
                             xmin=0,
                             xmax=40,
                             xlabel=rappel (\%),
                             x label style={yshift=.34em, font=\small},
                             y=0.00855\linewidth,
                             ytick={0, 10, 20, ..., 100},
                             ymin=0,
                             ymax=40,
                             ylabel=précision (\%),
                             y label style={yshift=-1.1em, font=\small},
                             legend style={font=\footnotesize}]
                  \addplot [red, mark=+] file {input/data/semeval_topicrank.csv};
                  \addplot [green, mark=-] file {input/data/semeval_assignment.csv};
                  \addplot [blue, mark=x] file {input/data/semeval_topiccorank.csv};
                  \addplot [dotted, domain=20:40] {(30 * x) / ((2 * x) - 30)};
                  \addplot [dotted, domain=10:40] {(20 * x) / ((2 * x) - 20)};
                  \addplot [dotted, domain=5:40] {(10 * x) / ((2 * x) - 10)};
                  \legend{TopicRank, \textit{Assignement}, TopicCoRank};
                \end{axis}
                \node at (4.85,3.1) [anchor=east] {\footnotesize{F=0.30}};
                \node at (4.85,1.8) [anchor=east] {\footnotesize{F=0.20}};
                \node at (4.85,0.9) [anchor=east] {\footnotesize{F=0.10}};
              \end{tikzpicture}
            }
            \caption{Courbes de rappel-précision de TopicRank
                     \textit{Assignement} et TopicCoRank
                     \label{fig:topiccorank-pr_curves}}
        \end{figure}
      
      \subsubsection{Analyse du comportement de TopicCoRank}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-evaluation-topiccorank-analysis}
        Bien qu'ils montrent de meilleures performances pour TopicCoRank, les
        résultats précédents ne permettent pas de se faire une idée précise de
        la place que prend l'assignement sur l'extraction. Le
        tableau~\ref{tab:assignment_ratio} montre la proportion de termes-clés
        extraits et assignés parmi les dix termes-clés que fournit TopicCoRank.
        Globalement, nous observons un équilibre entre extraction et
        assignement, avec tout de même une plus forte tendance à l'extraction.
        Cet équilibre et les variations observées selon les collections de
        données montrent que l'algorithme d'ordonnancement conjoint des sujets
        du document et des termes-clés de référence du domaine est effectivement
        capable de mêler sujets et termes-clés de références.
        \begin{table}[!h]
          \centering
          \begin{tabular}{l|cc}
              \toprule
              & Extraction (\%) & Assignement (\%)\\
              \hline
              Inist & 61,3 & 38,7\\
              DUC & 41,4 & 58,6\\
              SemEval & 60,0 & 40,0\\
              \bottomrule
          \end{tabular}
          \caption{Taux d'extraction et d'assignement de TopicCoRank pour dix
                   termes-clés
                   \label{tab:assignment_ratio}}
        \end{table}

        Enfin, la figure~\ref{fig:lambda_variations} montre le comportement de
        TopicCoRank lorsque nous faisons varier la valeur de $\lambda$ avec un
        pas de 0,1. Tout d'abord, nous observons que la moins bonne performance
        pouvant être obtenue est bornée par la performance de TopicRank. Quelque
        soit son degré d'influence, le graphe du domaine contribue positivement
        à l'indexation par termes-clés. Nous observons aussi deux comportements
        différents selon les collections de données. Dans le cas de Termith et
        de \textsc{Duc} les meilleures performances sont obtenues avec une
        influence plus importante de la recommandation externe
        R$_\textnormal{\textit{externe}}$, tandis que dans le cas de SemEval la
        recommandation interne R$_\textnormal{\textit{interne}}$ doit avoir une
        plus forte influence. Nous observons cependant un compromis lorsque
        $\lambda = 0,5$ (valeur que nous avons défini par défaut), soit quand
        les deux recommandations R$_\textnormal{\textit{interne}}$ et
        R$_\textnormal{\textit{externe}}$ ont la même influence sur
        l'ordonnancement.
        \begin{figure}[h!]
          \centering
          \subfigure[Inist]{
            \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
              \pgfkeys{/pgf/number format/.cd, fixed}
              \begin{axis}[x=0.393\linewidth,
                           xtick={0.1, 0.3, ..., 0.9},
                           xmin=0.05,
                           xmax=0.95,
                           xlabel=$\lambda$,
                           x label style={yshift=.34em, font=\small},
                           y=0.01875\textheight,
                           ytick={0, 2, 4, ..., 34},
                           ymin=14,
                           ymax=25,
                           ylabel=f-score (\%),
                           y label style={yshift=-1.1em, font=\small}]
                \addplot[blue, mark=x] coordinates{
                  (0.1, 20.04)
                  (0.2, 20.22)
                  (0.3, 20.37)
                  (0.4, 20.14)
                  (0.5, 20.09)
                  (0.6, 18.46)
                  (0.7, 16.69)
                  (0.8, 15.98)
                  (0.9, 16.43)
                };
              \end{axis}
            \end{tikzpicture}
          }
          \subfigure[\textsc{Duc}]{
            \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
              \pgfkeys{/pgf/number format/.cd, fixed}
              \begin{axis}[x=0.393\linewidth,
                           xtick={0.1, 0.3, ..., 0.9},
                           xmin=0.05,
                           xmax=0.95,
                           xlabel=$\lambda$,
                           x label style={yshift=.34em, font=\small},
                           y=0.01875\textheight,
                           ytick={0, 2, 4, ..., 34},
                           ymin=24,
                           ymax=35,
                           ylabel=f-score (\%),
                           y label style={yshift=-1.1em, font=\small}]
                \addplot[blue, mark=x] coordinates{
                  (0.10, 31.96)
                  (0.20, 32.24)
                  (0.30, 32.40)
                  (0.40, 31.92)
                  (0.50, 31.53)
                  (0.60, 30.50)
                  (0.70, 28.23)
                  (0.80, 26.61)
                  (0.90, 24.63)
                };
              \end{axis}
            \end{tikzpicture}
          }
          \subfigure[SemEval]{
            \begin{tikzpicture}[every axis/.append style={font=\footnotesize}]
              \pgfkeys{/pgf/number format/.cd, fixed}
              \begin{axis}[x=0.393\linewidth,
                           xtick={0.1, 0.3, ..., 0.9},
                           xmin=0.05,
                           xmax=0.95,
                           xlabel=$\lambda$,
                           x label style={yshift=.34em, font=\small},
                           y=0.01875\textheight,
                           ytick={0, 2, 4, ..., 34},
                           ymin=8,
                           ymax=19,
                           ylabel=f-score (\%),
                           y label style={yshift=-1.1em, font=\small}]
                \addplot[blue, mark=x] coordinates{
                  (0.10, 11.24)
                  (0.20, 11.33)
                  (0.30, 11.39)
                  (0.40, 12.07)
                  (0.50, 13.43)
                  (0.60, 14.49)
                  (0.70, 14.09)
                  (0.80, 13.96)
                  (0.90, 13.28)
                };
              \end{axis}
            \end{tikzpicture}
          }
          \caption{Comportement de TopicCoRank en fonction de la valeur de
                   $\lambda$
                   \label{fig:lambda_variations}}
        \end{figure} 

    \subsection{Analyse d'erreurs}
    \label{subsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-error_analysis}
      Dans cette section, nous nous intéressons aux termes-clés issus du graphe
      représentatif du domaine. Nous analysons les termes-clés corrects et
      incorrects obtenus par assignement.\TODO{\dots}

      \subsubsection{Analyse des vrais positifs}
      \label{subsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-error_analysis-true_positives}

      \subsubsection{Analyse des faux positifs}
      \label{subsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-error_analysis-false_positives}

    \subsection{Bilan}
    \label{subsec:main-domain_specific_keyphrase_annotation-supervised_automatic_keyphrase_annotation-conclusion}
      Avec TopicCoRank, nous proposons une extension de la méthode TopicRank,
      que nous présentons dans la
      section~\ref{sec:main-domain_specific_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction}.
      Cette extension apporte à TopicRank la capacité à assigner des
      termes-clés, soit à réaliser la tâche d'indexation par termes-clés dans sa
      globalité. Pour ce faire, TopicCoRank utilise les termes-clés de
      références des documents d'entraînement comme vocabulaire contrôlé, crée
      un graphe dont chaque n\oe{}uds est une entrée du vocabulaire connecté aux
      autres lorsqu'ils sont termes-clés d'un même document, puis unifie se
      graphe au graphe de sujets de TopicRank.

  %-----------------------------------------------------------------------------

  \section{Évaluation manuelle en domaines de spécialités}
  \label{sec:main-domain_specific_keyphrase_annotation-manual_evaluation}
    Pour évaluer les performances d'une méthode d'indexation automatique par
    termes-clés et la comparer aux autres méthodes, il est courant d'utiliser un
    système d'évaluation automatique. Un tel système utilise un jugement de
    référence qu'il compare aux sorties de la méthode
    automatique~\cite{voorhees2002philosophy}. Dans le cas de l'indexation par
    termes-clés, si un terme-clé donné par la méthode fait partie des
    termes-clés de référence (jugement de référence), alors celui-ci est jugé
    correct, sinon il est jugé incorrect. Alternative viable et plus accessible
    que l'évaluation manuelle, l'évaluation automatique possède toutefois un
    inconvénient majeur~: la condition stricte d'appartenance au jugement de
    référence n'est pas adaptée à une tâche subjective telle que celle de
    l'indexation par termes-clés et elle rend donc pessimiste l'évaluation de
    cette dernière. En effet, un même sujet peut êrte représenté par plusieurs
    expressions synonymiques, mais le jugement de référence n'en accepte qu'une
    seule alors que les autres peuvent aussi
    convenir~\cite{hasan2014state_of_the_art}. Certains travaux tentent de
    résoudre ce problème en acceptant des variantes des termes-clés de
    référence~\cite{zesch2009rprecision,kim2010rprecision}. Cependant, aucun ne
    quantifie la divergence sémantique entre un terme-clé de référence et sa
    supposée variante. De cette ménière, l'évaluation pert certes en pessimisme,
    mais aussi en exactitude.
    
    Pour compléter les évaluations automatiques que nous réalisons pour évaluer
    nos travaux, le projet Termith et l'Inist mettent à notre disposition des
    indexeurs professionnels pour évaluer manuellement les termes-clés produits
    par nos méthodes, pour les domaines de la linguistique, des sciences de
    l'information, de l'archéologie et de la chimie (ressource Termith).

    \subsection{Protocole d'évaluation manuelle}
    \label{subsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-evaluation_protocol}
      L'évaluation manuelle concerne dix termes-clés extraits et/ou assignés par
      chaque méthode d'indexation par termes-clés. Le protocole que nous
      proposons permet d'évaluer deux aspects de l'indexation automatique par
      termes-clés~:
      \begin{enumerate}
        \item{Pertinence~: chaque terme-clé fourni par la méthode d'indexation
              automatique par termes-clés est-il important pour la compréhension
              du contenu principal du document~?}
        \item{Silence~: quel est le degré d'importance des informations perdues
              entre les termes-clés de référence et les termes-clés fournis par
              la méthode d'indexation automatique par termes-clés~?}
      \end{enumerate}
      L'évaluation de la pertinence traite le même aspect que l'évaluation
      automatique~: le nombre de termes-clés corrects doit être maximisé pour
      obtenir la meilleure performance. L'évaluation du silence traite un aspect
      qui n'est pas traité par l'évaluation automatique. Elle a une dimension
      plus sémantique~: les termes-clés corrects dont l'information est la plus
      capitale à la compréhension du contenu principal du document doivent être
      priorisés pour obtenir la meilleure performance.

      Afin de minimiser les problèmes d'ambiguïté et de subjectivité de certains
      cas de figure, la pertinence et le silence sont évalués sur une échelle à
      trois valeurs~: une valeur représentant l'échec, une autre représentant le
      succès et une dernière valeur représentant un cas intermédiaire.

      \subsubsection{Évaluation de la pertinence}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-evaluation_protocol-relevancy}
        \TODO{expliquer que l'indexeur ne peut pas voir les termes-clés inist
        lors de cette évaluation}
        Pour évaluer la pertinence d'un terme-clé fourni par une méthode
        d'indexation par termes-clés, l'évaluateur doit lui attribuer un score
        sur une échelle de 0 à 2. Ce score distingue les termes-clés incorrects
        (0), les termes-clés corrects (2) et les variantes de ces derniers (1).

        Pour permettre une étude précise de cette évaluation, les indexeurs
        professionnels doivent indiquer la forme préférée des termes-clés
        auquels ils donnent un score de 1 (variantes). Une variante peut faire
        référence à deux catégories de formes préférées, qui induisent deux
        raisonnements différents~:
        \begin{itemize}
          \item{variante d'un terme-clé déjà fourni (score de
                2)~$\Rightarrow$~la méthode d'indexation par termes-clés fourni
                des termes-clés redondants~;}
          \item{variante d'un terme-clé non fourni mais présent dans le
              texte~$\Rightarrow$~la méthode d'indexation par termes-clés
                identifie correctement les sujets importants du document, mais
                peine à trouver leur forme la plus appropriée pour les
                représenter.}
        \end{itemize}

        Lorsque la forme préférée n'est pas présente dans le document, nous
        estimons que la méthode d'indexation a fourni un terme-clé correct,
        auquel cas il se voit attribuer le score de 2. Les formes variantes
        résultant d'un accord en nombre (pluriel) obtiennent aussi un score de
        2, lorsque la forme normalisée (singulier) ne se trouve pas parmi les
        termes-clés fournis.

      \subsubsection{Évaluation du silence}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-evaluation_protocol-silence}
        \TODO{explique que l'évaluateur effectue cette évaluation après la
        pertinence et qu'il ne peut pas revenir à la pertinence car ça
        introduirait un biais}
        Pour évaluer le silence, l'évaluateur doit attribuer à chaque terme-clé
        de référence un score indiquant le degré d'importance de l'information
        qu'il véhicule et qui n'est pas capturée par les termes-clés fournis par
        une méthode d'indexation par termes-clés. Sur une échelle de 0 à 2, ce
        score permet d'indiquer s'il n'y pas de perte d'information (0), si
        l'information perdue est capitale (2) ou si elle est secondaire (1).
        Lorsqu'un terme-clé de référence obtient un score de 0, cela signifie
        soit qu'il fait partie des termes-clés fournis par la méthode
        d'indexation par termes-clés, soit que l'indexeur juge qu'il ne devrait
        pas être un terme-clé de référence, c'est-à-dire que c'est une erreur
        parmi les termes-clés de référence.

        Une perte d'information est jugée secondaire (score de 1) dans deux
        cas de figure différents~:
        \begin{itemize}
          \item{terme-clé de référence secondaire~: le terme-clé de référence
                n'apporte pas l'information la plus importante~;}
          \item{terme-clé de référence générique~: le terme-clé de référence
                n'est pas suffisamment spécifique au contenu du document, il a
                un usage classificatoire~;}
        \end{itemize}
        Afin de minimiser les pertes d'informations dues à des termes-clés de
        référence qui ne sont pas présents dans le document, les évaluateurs
        leur attribuent un score de 1.

    \subsection{Évaluation manuelle des méthodes proposées}
    \label{subsec:main-domain_specific_keyphrase_annotation-manual_evaluation-analysis}
      Dans cette section, nous analysons l'évaluation manuelle de TopicRank,
      TopicCoRank, d'une méthode de référence non supervisée, \textsc{Tf-Idf},
      et d'une méthode de référence supervisée, \textsc{Kea}. L'évaluation est
      effectuée par un indexeur professionnel par document.

      Dans un premier temps, nous analysons l'évaluation manuelle de TopicRank
      et la comparons à celle de \textsc{Tf-Idf}, puis, dans un second temps,
      nous analysons l'évaluation manuelle de TopicCoRank et la comparons à
      celle de \textsc{Kea}.

      \subsubsection{Analyse de TopicRank}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-manual_evaluation-analysis-topicrank}
        \TODO{\dots}
      
        ~\\Le
        tableau~\ref{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-pertinence_score_ratio}
        dresse le bilan des scores de pertinence attribués en moyenne par
        méthode. Pour le score de 1, qui indique qu'un terme-clé est un forme
        variante, nous distinguons le cas où la variante est redondante du cas
        où la variante n'est pas redondante. Globalement, nous observons que
        TopicRank est meilleur que \textsc{Tf-Idf}. TopicRank fournit plus de
        termes-clés pertinents que \textsc{Tf-Idf}, mais fait aussi plus
        d'erreurs. Les termes-clés ayant un score de 1 donnent un explication
        intéressante à cette contradiction. En effet, \textsc{Tf-Idf} à une
        forte tendence à extraire des termes-clés redondant, soit des
        termes-clés variantes de termes-clés déjà extrait. En revanche,
        TopicRank remplit presque son objectif de ne pas extraire de termes-clés
        redondant, avec seulement 0,9~\% de termes-clés redondants. Comme nous
        l'avons aussi observé lors de l'évaluation automatique de TopicRank (cf
        section~\ref{subsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation}
        page~\ref{subsec:main-automatic_keyphrase_annotation-unsupervised_automatic_keyphrase_extraction-evaluation}),
        celui-ci extrait cependant plus de termes-clés variantes non redondants,
        c'est-à-dire que la strategie de TopicRank pour sélectionner le meilleur
        terme-clé pour un sujet n'est pas optimale.
        \begin{table}[h!]
          \centering
          \begin{tabular}{l|c|c|c|c}
            \toprule
            \multirow{2}{*}{\textbf{Méthode}} & \multirow{2}{*}{\textbf{0}} & \multicolumn{2}{c|}{\textbf{1}} & \multirow{2}{*}{\textbf{2}}\\
            \cline{3-4}
            & & \multicolumn{1}{p{.175\linewidth}|}{\centering{}redondant} & \multicolumn{1}{p{.175\linewidth}|}{\centering{}non redondant} &\\
            \hline
            \textsc{Tf-Idf} & \textbf{53,8~\%} & 6,8~\% & 4,2~\% & 35,3~\%\\
            TopicRank & 56,3~\% & \textbf{0,9~\%} & \textbf{5,7~\%} & \textbf{37,1~\%}\\
            \bottomrule
          \end{tabular}
          \caption{Taux de termes-clés avec un score de 0, de 1 ou de 2 pour
                   l'évaluation de la pertinence de \textsc{Tf-Idf} et de
                   TopicRank
                   \label{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-pertinence_score_ratio}}
        \end{table}

        Le
        tableau~\ref{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-prf}
        présente les performances de \textsc{Tf-Idf} et de TopicRank, en termes
        de précision, de rappel et de f-mesure, et les compare à celles
        observées par notre système d'évaluation automatique. Pour calculer ces
        performances, les termes-clés ayant un score de 2 sont considérés
        corrects, de même que ceux ayant un score de 1 non redondants. La
        difficulté d'évaluer automatiquement la tâche d'indexation par
        termes-clés se confirme. Les conclusions ne sont pas les mêmes, puisque
        de manière automatique TopicRank est moins performant que
        \textsc{Tf-Idf} alors qu'il est plus permformant selon l'évaluation
        manuelle. Nous observons aussi des différences d'environ 30 points entre
        les mesures obtenues manuellement et automatiquement. Les résultats
        montrent ici que la tâche d'indexation par termes-clés est effectivement
        subjective et que l'évaluation manuelle permet de réduire ce problème.
        \begin{table}[h!]
          \centering
          \begin{tabular}{l|ccc|ccc}
            \toprule
            \multirow{2}{*}{\textbf{Méthode}} & \multicolumn{3}{c|}{\textbf{Manuel}} & \multicolumn{3}{c}{\textbf{Automatique}}\\
            \cline{2-7}
            & P & R & F & P & R & F\\
            \hline
            \textsc{Tf-Idf} & 39,5 & 29,7 & 33,5 & \textbf{13,0} & \textbf{15,4} & \textbf{13,9}\\
            TopicRank & \textbf{42,8} & \textbf{32,2} & \textbf{36,2} & 11,2 & 13,1 & 11,9\\
            \bottomrule
          \end{tabular}
          \caption[
            Performances de \textsc{Tf-Idf} et de TopicRank en termes de
            précision, de rappel et de f-mesure
          ]{
            Performances de \textsc{Tf-Idf} et de TopicRank en termes de
            précision (P), de rappel (R) et de f-mesure (F)
            \label{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-prf}}
        \end{table}
      
        ~\\Le
        tableau~\ref{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-silence_score_ratio}
        dresse le bilan des scores de silence attribués en moyenne par méthode.
        D'après la description donnée pour chacun des scores, la méthode qui
        capture le plus d'informations est celle qui maximise le nombre de
        termes-clés de référence ayant un score de silence 0 et qui minimise
        ceux ayant un score de 1 et de 2. De ce fait, nous observons que
        TopicRank couvre mieux le contenu principal des documents que
        \textsc{Tf-Idf}.
        \begin{table}[h!]
          \centering
          \begin{tabular}{l|c|c|c}
            \toprule
            \textbf{Méthode} & \textbf{0} & \textbf{1} & \textbf{2}\\
            \hline
            \textsc{Tf-Idf} & 31,4~\% & 48,5~\% & 20,1~\%\\
            TopicRank & \textbf{35,0~\%} & \textbf{48,3~\%} & \textbf{16,8~\%}\\
            \bottomrule
          \end{tabular}
          \caption{Taux de termes-clés de référence avec un score de 0, de 1 ou
                   de 2 pour l'évaluation du silence de \textsc{Tf-Idf} et de
                   TopicRank
                   \label{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-silence_score_ratio}}
        \end{table}

      \subsubsection{Analyse de TopicCoRank}
      \label{subsubsec:main-domain_specific_keyphrase_annotation-manual_evaluation-analysis-topiccorank}
        \TODO{\dots}

    \subsection{Bilan}
    \label{subsec:main-domain_specific_keyphrase_annotation-manual_evaluation-conclusion}
      \TODO{mettre en évidence les points forts et les faiblesses}

      \TODO{L'évaluation manuelle de TopicRank, et sa comparaison avec
      \textsc{Tf-Idf}, montre l'apport de TopicRank vis-à-vis de l'état de
      l'art. Les résultats montrent aussi que TopicRank remplit effectivement
      l'objectif d'éviter l'extraction de termes-clés redondants.}

  %-----------------------------------------------------------------------------

  \section{Conclusion}
  \label{sec:main-domain_specific_keyphrase_annotation-conclusion}
    \TODO{\dots}

