\chapter{Évaluation manuelle}
\label{chap:main-manuelle_evaluation_of_keyphrase_annotation}
  \chaptercite{
    The performance of most keyphrase extraction algorithms is [automatically]
    evaluated by comparing whether the extracted keyphrases exactly match the
    human assigned gold standard keyphrases. However, this is known to
    underestimate performance.
  }{
    \newcite{zesch2009rprecision}
  }

  \section{Introduction}
  \label{sec:main-automatic_evaluation_of_keyphrase_annotation-introduction}
    Pour évaluer les performances d'une méthode d'indexation par termes-clés et
    la comparée aux autres méthodes, il est courant d'utiliser un système
    d'évaluation automatique. Un tel système utilise un jugement de référence
    (termes-clés de référence) qu'il compare aux sorties de la
    méthode~\cite{voorhees2002philosophy}. Si un terme-clé donné par la méthode
    fait partie des termes-clés de référence, alors celui-ci est jugé correct,
    sinon il est jugé incorrect. Alternative viable et plus accessible que
    l'évaluation manuelle, l'évaluation automatique possède toutefois un
    inconvénient majeur~: la condition stricte d'appartenance au jugement de
    référence n'est pas adaptée à une tâche subjective telle que celle de
    l'indexation par termes-clés et rend pessimiste l'évaluation de cette
    dernière~\cite{hasan2014state_of_the_art}. Bien que certains travaux tentent
    de résoudre ce problème en acceptant des supposées variantes des termes-clés
    de référence~\cite{zesch2009rprecision,kim2010rprecision}, aucun ne
    quantifie la divergence sémantique entre le terme-clé de référence et sa
    variante. Dans ce dernier cas, l'évaluation pert certes en pessimisme, mais
    elle pert aussi en exactitude.
    
    Pour compléter les évaluations automatiques que nous utilisons pour évaluer
    nos travaux présentés dans le
    chapitre~\ref{chap:main-automatic_keyphrase_annotation}, le projet Termith
    et l'Inist mettent à notre disposition des indexeurs professionnels pour
    évaluer manuellement les termes-clés produits par nos méthodes. Ce travail,
    réalisé conjointement avec l'Inist et l'Inria Saclay donne lieu à la
    formalisation d'un protocole d'évaluation et à la spécification d'un format
    d'échange permettant de distribuer les données indexées par nos méthodes
    et leur évaluation. Additionnellement, rendre public ces données poura
    permettre l'étude de nouvelles méthodes d'évaluation automatique, notamment
    leur corrélation avec une évaluation réalisée par des humains.

  %-----------------------------------------------------------------------------

  \section{Méthodologie}
  \label{section:main-automatic_evaluation_of_keyphrase_annotation-methodology}
    Dans cette section, nous décrivons le protocole mis en place pour
    l'évaluation manuelle des méthodes d'indexation par termes-clés et
    présentons le format utilisé pour pérenniser les différentes étapes
    (indexation automatique et évaluation manuelle selon divers aspects).

    \subsection{Protocole d'évaluation}
    \label{subsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-evaluation_protocol}
      \TODO{que veut-on évaluer ? $\rightarrow$ pertinence et silence}
      \TODO{Comment ? $\rightarrow$ c'est ça le protocole biatch}

    \subsection{Format des données}
    \label{subsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-data_format}
      \TODO{expliquer que nous ajoutons des annotations (et sous annotations) au
      document}
      \TODO{présenter les format TEI et TBX}
      \TODO{spécification de nos shéma d'annotation}

  %-----------------------------------------------------------------------------

  \section{Resultats}
  \label{sec:main-automatic_evaluation_of_keyphrase_annotation-results}
    \TODO{intro $\rightarrow$ évaluation en ? phrases}
    
    \TODO{phase 1~: évaluation de la pertinence + évaluation du silence}
    \TODO{ne pas oublier de rappeler les données et les méthodes}

    \TODO{phase 2~: évaluation de la pertinence + évaluation du silence}
    \TODO{ne pas oublier de rappeler les données et les méthodes}

    \TODO{\dots}

  %-----------------------------------------------------------------------------

  \section{Conclusion}
  \label{sec:main-automatic_evaluation_of_keyphrase_annotation-Conclusion}

