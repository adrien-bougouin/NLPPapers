\chapter{Évaluations manuelles}
\label{chap:main-manuelle_evaluation_of_keyphrase_annotation}
  \chaptercite{
    The performance of most keyphrase extraction algorithms is [automatically]
    evaluated by comparing whether the extracted keyphrases exactly match the
    human assigned gold standard keyphrases. However, this is known to
    underestimate performance.
  }{
    \newcite{zesch2009rprecision}
  }

  \section{Introduction}
  \label{sec:main-automatic_evaluation_of_keyphrase_annotation-introduction}
    Pour évaluer les performances d'une méthode d'indexation par termes-clés et
    la comparée aux autres méthodes, il est courant d'utiliser un système
    d'évaluation automatique. Un tel système utilise un jugement de référence
    qu'il compare aux sorties de la méthode~\cite{voorhees2002philosophy}. Dans
    le cas de l'indexation par termes-clés, si un terme-clé donné par la méthode
    fait partie des termes-clés de référence (jugement de référence), alors
    celui-ci est jugé correct, sinon il est jugé incorrect. Alternative viable
    et plus accessible que l'évaluation manuelle, l'évaluation automatique
    possède toutefois un inconvénient majeur~: la condition stricte
    d'appartenance au jugement de référence n'est pas adaptée à une tâche
    subjective telle que celle de l'indexation par termes-clés et elle rend donc
    pessimiste l'évaluation de cette dernière. En effet, un même sujet peut êrte
    représenté par plusieurs expressions variantes, le jugement de référence
    n'en contient qu'une seule alors que les autres peuvent aussi
    convenier~\cite{hasan2014state_of_the_art}. Bien que certains travaux
    tentent de résoudre ce problème en acceptant des supposées variantes des
    termes-clés de référence~\cite{zesch2009rprecision,kim2010rprecision}, aucun
    ne quantifie la divergence sémantique entre un terme-clé de référence et sa
    supposée variante. L'évaluation pert certes en pessimisme, mais aussi en
    exactitude.
    
    Pour compléter les évaluations automatiques que nous utilisons pour évaluer
    nos travaux présentés dans le
    chapitre~\ref{chap:main-automatic_keyphrase_annotation}, le projet Termith
    et l'Inist mettent à notre disposition des indexeurs professionnels pour
    évaluer manuellement les termes-clés produits par nos méthodes. Ce travail,
    réalisé conjointement avec l'Inist et l'Inria Saclay donne lieu à la
    formalisation d'un protocole d'évaluation et à la spécification d'un format
    d'échange permettant de distribuer les données indexées par nos méthodes
    ainsi que leur évaluation. Additionnellement, rendre public ces données
    pourra permettre l'étude de nouvelles méthodes d'évaluation automatique,
    notamment leur corrélation avec les évaluations manuelles de plusieurs
    méthodes.

  %-----------------------------------------------------------------------------

  \section{Méthodologie}
  \label{section:main-automatic_evaluation_of_keyphrase_annotation-methodology}
    Dans cette section, nous décrivons le protocole mis en place pour
    l'évaluation manuelle des méthodes d'indexation par termes-clés et
    présentons le format utilisé pour pérenniser les différentes étapes
    (indexation automatique et évaluation manuelle selon divers aspects) et
    ainsi les rendre disponibles pour la communauté scientifique.

    \subsection{Protocole d'évaluation manuelle}
    \label{subsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-evaluation_protocol}
      Le protocole d'évaluation manuelle que nous proposons permet d'évaluer
      deux aspects de l'indexation automatique par termes-clés~:
      \begin{enumerate}
        \item{Pertinence~: chaque terme-clé fourni par la méthode d'indexation
              automatique par termes-clés est-il important pour la compréhension
              du contenu principal du document~?}
        \item{Silence~: quel est le degré d'importance des informations perdues
              entre les termes-clés de référence et les termes-clés fournis par
              la méthode d'indexation automatique par termes-clés~?}
      \end{enumerate}
      L'évaluation de la pertinence traite le même aspect que l'évaluation
      automatique~: le nombre de termes-clés correctes doit être maximisé pour
      obtenir la meilleure performance. L'évaluation du silence traite un aspect
      qui n'est pas traité par l'évaluation automatique. Elle a une dimension
      plus sémantique~: les termes-clés correctes dont l'information est la plus
      capitale à la compréhension du contenu principal du document doivent être
      priorisés pour obtenir la meilleure performance.

      Afin de minimiser les problèmes d'ambiguïté et de subjectivité de certains
      cas de figure, la pertinence et le silence sont évalués sur une échelle à
      trois valeurs~: une valeur représentant l'échec, une autre représentant le
      succès et une dernière valeur représentant un cas intermédiaire.

      \subsubsection{Évaluation de la pertinence}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-evaluation_protocol-relevancy}
        Pour évaluer la pertinence d'un terme-clé fourni par une méthode
        d'indexation par termes-clés, l'évaluateur doit lui attribuer un score
        sur une échelle de 0 à 2. Ce score distingue les termes-clés incorrects
        (0), les termes-clés corrects (2) et les variantes de ces derniers (1).

        Pour permettre une étude précise de cette évaluation, les indexeurs
        professionnels doivent indiquer la forme préférée des termes-clés
        auquels ils donnent un score de 1 (variantes). Une variante peut faire
        référence à deux catégories de formes préférées, qui induisent deux
        raisonnements différents~:
        \begin{itemize}
          \item{variante d'un terme-clé déjà fourni (score de
                2)~$\Rightarrow$~la méthode d'indexation par termes-clés fourni
                des termes-clés redondants~;}
          \item{variante d'un terme-clé non fourni mais présent dans le
              texte~$\Rightarrow$~la méthode d'indexation par termes-clés
                identifie correctement les sujets importants du document, mais
                peine à trouver leur forme la plus appropriée pour les
                représenter.}
        \end{itemize}

        Lorsque la forme préférée n'est pas présente dans le document, nous
        estimons que la méthode d'indexation a fourni un terme-clé correct,
        auquel cas il se voit attribuer le score de 2. Les formes variantes
        résultant d'un accord en nombre obtiennent aussi un score de 2, lorsque
        la forme normalisée (singulier) ne se trouve pas parmi les termes-clés
        fournis.

        \TODO{commentaires des indexeurs~???}

        \TODO{exemples}

      \subsubsection{Évaluation du silence}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-evaluation_protocol-silence}
        Pour évaluer le silence, l'évaluateur doit attribuer à chaque terme-clé
        de référence un score indiquant le degré d'importance de l'information
        qu'il véhicule et qui n'est pas capturée par un terme-clé fourni par une
        méthode d'indexation par termes-clés. Sur une échelle de 0 à 2, ce score
        permet d'indiquer s'il n'y pas de perte d'information (0), si
        l'information perdue est capitale (2) ou si elle est secondaire (1).
        Lorsqu'un terme-clé de référence obtient un score de 0, cela signifie
        soit qu'il fait partie des termes-clés fournis par la méthode
        d'indexation par termes-clés, soit que l'indexeur juge qu'il ne devrait
        pas être un terme-clé de référence, que c'est une erreur parmi les
        termes-clés de référence.

        Une perte d'information est jugée secondaire (score de 1) dans deux
        cas de figure différents~:
        \begin{itemize}
          \item{terme-clé de référence secondaire~: le terme-clé de référence
                n'apporte pas l'information la plus importante~;}
          \item{terme-clé de référence générique~: le terme-clé de référence
                n'est pas suffisamment spécifique au contenu du document, il a
                un usage classificatoire~;}
        \end{itemize}
        Afin de minimiser les pertes d'informations dues à des termes-clés de
        référence qui ne sont pas présents dans le document, les évaluateurs
        attribuent un score de 1.

        \TODO{commentaires des indexeurs~???}

        \TODO{exemples}

    \subsection{Format des données}
    \label{subsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-data_format}
      Les documents distribués à l'issue de l'évaluation manuelle se présentent
      sous la forme de données structurées comprenant leurs informations
      factuelles (titre, auteurs, affiliation des auteurs, etc.), leur contenu
      textuel et leurs indexations par termes-clés effectuées par différentes
      méthodes, elles même annotées par un évaluateur. Les données sont
      structurées au format \textsc{Xml} (\textit{eXtensible Markup Language}),
      d'après les standards \textsc{Tei} (\textit{Text Encoding Initiative}) et
      \textsc{Tbx} (\textit{TermBase eXchange}).

      \subsubsection{Format \textsc{Xml}}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-data_format-xml}
        \textsc{Xml} est un language pour encoder des documents de sorte qu'ils
        soient interprétables aussi bien par un humain que par une machine. Un
        document \textsc{Xml} se présente sous la forme d'un arbre. Chaque
        élément de l'arbre représente un champ du document (par exemple, un
        titre), délimitée par une balise ouvrante et une balise fermante (par
        exemple, \texttt{<titre>} et \texttt{</titre>}).

        La figure~\ref{fig:xml_example} donne un exemple de représentation
        \textsc{Xml} d'une notice Termith. Il s'agit de la notice donnée en
        exemple dans la figure~\ref{fig:example_inist}
        (page~\ref{fig:example_inist}) du
        chapitre~\ref{chap:main-data_description}. Dans le format \textsc{Xml},
        la nature de chaque élément de la notice est clairement identifiée par
        les balises \textsc{Xml}.
        \begin{figure}[h!]
          \lstset{language=XML}
          \lstset{morekeywords={notice, titre, resume}}
          \lstset{literate={'}{{'}}1 {é}{{\'e}}1 {è}{{\`e}}1 {ê}{{\^e}}1 {à}{{\`a}}1 {ô}{{\^o}}1}
          \lstset{frame=single}
          \lstinputlisting{input/data/linguistics_xml_example.xml}
          \caption{Exemple de notice de linguistique au format \textsc{Xml}
                   \label{fig:xml_example}}
        \end{figure}

      \subsubsection{Standard \textsc{Tei}}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-data_format-tei}
        Le standard \textsc{Tei} propose un schéma de codage normalisé et
        structuré pour décrire toute sorte de documents numériques. Bien plus
        qu'une simple spécification de format, il s'agit d'un cadre permettant
        de créer des spécifications \textsc{Xml} personnalisées. Une
        spécification \textsc{Xml} \textsc{Tei} est organisée en trois niveaux~:
        \begin{enumerate}
          \item{Le c\oe{}ur~: spécification des éléments structurels communs à
                tout document \textsc{Tei} (en-tête, corps du text, paragraphe,
                etc.)~;}
          \item{Un jeu d'éléments structurels spécifiques~: spécification des
                éléments spécifiques à certains genres de document (théâtre,
                discours, dictionnaire, etc.)~;}
          \item{Des modules additionnels.}
        \end{enumerate}
        La figure~\ref{fig:tei_example} donne un exemple de représentation
        \textsc{Xml} \textsc{Tei} de la notice Termith représentée avec un
        format \textsc{Xml} simple dans la figure~\ref{fig:xml_example}. 
        \begin{figure}[h!]
          \lstset{language=XML}
          \lstset{morekeywords={encoding, TEI, xmlns, xmlns:tei, teiHeader, fileDesc, sourceDesc, biblStruct, type, analytic, title, author, persName, ana, forename, surname, profileDesc, abstract, p}}
          \lstset{literate={'}{{'}}1 {é}{{\'e}}1 {è}{{\`e}}1 {ê}{{\^e}}1 {à}{{\`a}}1 {ô}{{\^o}}1}
          \lstset{frame=single}
          \lstinputlisting{input/data/linguistics_tei_example.tei}
          \caption{Exemple de notice de linguistique au format \textsc{Tei}
                   \label{fig:tei_example}}
        \end{figure}

      \subsubsection{Standard \textsc{Tbx}}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-data_format-tbx}
        Le standard \textsc{Tbx} décrit un format de représentation de bases
        terminologiques. Les éléments qui composent la base de données
        terminologique sont organisés d'après le standard \textsc{Tmf}
        (\textit{Terminological Markup Framework}) présenté dans la
        figure~\ref{fig:tmf}. En outre des éléments
        factuels, une base terminologique \textsc{Tbx} est composée de concepts
        (élément \texttt{termEntry}), représentés par un ou plusieurs termes
        (élément \texttt{tig}) groupés par langue (élément \texttt{langSet}).
        La figure~\ref{fig:tbx_example} montre un extrait de vocabulaire
        contrôlé au format \textsc{Tbx}.
        \begin{figure}
          \centering
          \begin{tikzpicture}
            \node [draw, rectangle] (root) {Terminologie};
            \node [draw, rectangle, below=of root] (concept) {Concept(s)};
            \node [draw, rectangle, left=of concept] (factual) {Informations factuelles};
            \node [draw, rectangle, right=of concept] (other) {Autres informations};
            \node [draw, rectangle, below=of concept] (langset) {Section(s) de langue};
            \node [draw, rectangle, below=of langset] (tig) {Section(s) de terme};
            \node [draw, rectangle, below=of tig] (term) {Terme};

            \draw [->] (root) -- (concept);
            \draw [->] (root) -- (factual);
            \draw [->] (root) -- (other);
            \draw [->] (concept) -- (langset);
            \draw [->] (langset) -- (tig);
            \draw [->] (tig) -- (term);
          \end{tikzpicture}
          \caption{Arbre hiérarchique du standard \textsc{Tmf}
                   \label{fig:tmf}}
        \end{figure}
        \begin{figure}[h!]
          \lstset{language=XML}
          \lstset{morekeywords={martif, type, martifHeader, text, body,
          termEntry, langSet, tig, term, xml:id}}
          \lstset{frame=single}
          \lstinputlisting{input/data/linguistics_tbx_example.tbx}
          \caption{Exemple de terminologie au format \textsc{Tbx}
                   \label{fig:tbx_example}}
        \end{figure}

      \subsubsection{Format d'échange utilisé}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-methodology-data_format-final_format}
        Le format de données utilisé pour distribuer les résultats de
        l'évaluation manuelle permet de pérenniser chaque notice, ses
        termes-clés fournis par différentes méthodes et l'évaluation de la
        pertinence et du silence de chacune de ces méthodes. Ce format garde en
        mémoire toutes les étapes successives afin de faciliter les
        exploitations futures par la communauté scientifique.

        Chaque notice est représentée au format \textsc{Tei} (cf
        figure~\ref{fig:tei_example}), une couche d'annotation
        \textsc{Tei} (\textit{stand-off}) est ajoutée pour chaque méthode
        d'indexation par termes-clés et une sous-couche d'annotation
        \textsc{Tei} décrivant les résultats de l'évaluation manuelle est
        ajoutée à chacune d'elles.

        La figure~\ref{fig:tei_tbx_keyphrase_example} montre un extrait
        d'indexation par termes-clés dans le format proposé. Optionnellement,
        des informations factuelles indiquant des informations telles que le nom
        et des détails sur le méthode d'indexation par termes-clés utilisée
        peuvent être ajoutées. Le résultat de l'indexation par termes-clés se
        trouve dans l'élément \texttt{annotations}. L'ensemble des termes-clés
        extraits et/ou assignés sont représentés au format \textsc{Tbx}. Chaque
        terme-clé est un \texttt{termEntry}.
        \begin{figure}[h!]
          \lstset{language=XML}
          \lstset{morekeywords={stdf, xml:id, soHeader, annotations, termEntry, langSet, tig, term}}
          \lstset{frame=single}
          \lstinputlisting{input/data/keyphrase_example.xml}
          \caption{Exemple d'indexation par termes-clés dans le format d'échange
                   \label{fig:tei_tbx_keyphrase_example}}
        \end{figure}

        La figure~\ref{fig:tei_tbx_evaluation_example} montre un extrait
        d'évaluation manuelle dans le format proposé. Celle-ci se trouve dans un
        élément \texttt{stdf} (\textit{stand-off}), imbriqué dans l'élément
        \texttt{stdf} de l'indexation par termes-clés qu'elle évalue. Elle
        répartie en deux groupes d'annotations (élément
        \texttt{annotationGrp}), le premier pour évaluer la pertinence et le
        second pour évaluer le silence. Chaque terme-clé est évalué
        individuellement (élément \texttt{span}), son score est indiqué par
        l'élément \texttt{num}, un commentaire peut être ajouté dans l'élément
        \texttt{note} et des liens vers des termes-clés extraits/assignés ou de
        référence peuvent être indiqué avec l'élément \texttt{link}.
        \begin{figure}[h!]
          \lstset{language=XML}
          \lstset{morekeywords={stdf, soHeader, annotations, annotationGrp, target, type, span, from, num, link, note}}
          \lstset{frame=single}
          \lstinputlisting{input/data/evaluation_example.xml}
          \caption{Exemple d'évaluation automatique dans le format d'échange
                   \label{fig:tei_tbx_evaluation_example}}
        \end{figure}

  %-----------------------------------------------------------------------------

  \section{Analyse des évaluations manuelles}
  \label{sec:main-automatic_evaluation_of_keyphrase_annotation-results}
    \TODO{intro $\rightarrow$ évaluation en ? phrases}
    
    \subsection{Évaluation manuelle de TopicRank}
    \label{subsec:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank}
      \TODO{phase 1~: évaluation de la pertinence + évaluation du silence}
      \TODO{ne pas oublier de rappeler les données et les méthodes}
    
      \subsubsection{Pertinence}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-pertinence}
      \begin{table}[h!]
          \centering
          \begin{tabular}{l|c|c|c|c}
            \toprule
            \multirow{2}{*}{\textbf{Méthode}} & \multirow{2}{*}{\textbf{0}} & \multicolumn{2}{c|}{\textbf{1}} & \multirow{2}{*}{\textbf{2}}\\
            \cline{3-4}
            & & \multicolumn{1}{p{.175\linewidth}|}{\centering{}redondant} & \multicolumn{1}{p{.175\linewidth}|}{\centering{}non redondant} &\\
            \hline
            \textsc{Tf-Idf} & \textbf{53,8~\%} & 6,8~\% & 4,2~\% & 35,3~\%\\
            TopicRank & 56,3~\% & \textbf{0,9~\%} & \textbf{5,7~\%} & \textbf{37,1~\%}\\
            \bottomrule
          \end{tabular}
          \caption{Taux de termes-clés avec un score de 0, de 1 ou de 2 pour
                   l'évaluation de la pertinence de \textsc{Tf-Idf} et de
                   TopicRank
                   \label{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-pertinence_score_ratio}}
        \end{table}

        \begin{table}[h!]
          \centering
          \begin{tabular}{l|ccc}
            \toprule
            \textbf{Méthode} & \textbf{P} & \textbf{R} & \textbf{F}\\
            \hline
            \textsc{Tf-Idf} & 39,5 & 29,7 & 33,5\\
            TopicRank & \textbf{42,8} & \textbf{32,2} & \textbf{36,2}\\
            \bottomrule
          \end{tabular}
          \caption[
            Performances de \textsc{Tf-Idf} et de TopicRank en termes de
            précision, de rappel et de f-mesure
          ]{
            Performances de \textsc{Tf-Idf} et de TopicRank en termes de
            précision (P), de rappel (R) et de f-mesure (F)
            \label{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-prf}}
        \end{table}
    
      \subsubsection{Silence}
      \label{subsubsec:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-silence}
        \begin{table}[h!]
          \centering
          \begin{tabular}{l|c|c|c}
            \toprule
            \textbf{Méthode} & \textbf{0} & \textbf{1} & \textbf{2}\\
            \hline
            \textsc{Tf-Idf} & 31,4~\% & 48,5~\% & 20,1~\%\\
            TopicRank & \textbf{35,0~\%} & \textbf{48,3~\%} & \textbf{16,8~\%}\\
            \bottomrule
          \end{tabular}
          \caption{Taux de termes-clés de référence avec un score de 0, de 1 ou
                   de 2 pour l'évaluation du silence de \textsc{Tf-Idf} et de
                   TopicRank
                   \label{tab:main-automatic_evaluation_of_keyphrase_annotation-results-topicrank-silence_score_ratio}}
        \end{table}

    \subsection{Évaluation manuelle de TopicCoRank}
    \label{subsec:main-automatic_evaluation_of_keyphrase_annotation-results-topiccorank}
      \TODO{phase 2~: évaluation de la pertinence + évaluation du silence}
      \TODO{ne pas oublier de rappeler les données et les méthodes}

    \TODO{\dots}

  %-----------------------------------------------------------------------------

  \section{Conclusion}
  \label{sec:main-automatic_evaluation_of_keyphrase_annotation-Conclusion}

