\section{Introduction}
  Les termes-clés sont des mots ou des expressions (multi-mots) représentant les
  aspects principaux qui sont abordés dans un document. De ce fait, ils sont
  utilisés dans de nombreux domaines du Traitement Automatique des Langues
  (TAL). \citet{turney1999learningalgorithms} émet l'hypothèse qu'ils peuvent
  faciliter la lecture d'un utilisateur en lui permettant de surfer d'un point
  clé à un autre lorsqu'ils sont mis en évidence dans un texte. D'autres
  chercheurs utilisent leurs vertus synthétiques dans des méthodes de
  construction automatique de résumés \citep{wan2007iterativereinforcement,
  litvak2008graphbased, boudin2013multisentencecompression}, mais ils s'avèrent
  surtout de plus en plus utiles avec l'essor de l'Internet et la disponibilité
  de nombreux documents numériques qu'il faut pouvoir indexer de manière
  pertinente pour faciliter leur recherche par des utilisateurs
  \citep{medelyan2008smalltrainingset}. Dans ce contexte de recherche
  d'information, les termes-clés peuvent aussi être directement bénéfiques aux
  utilisateurs en servant de suggestions à une requête qu'ils essaient de
  formuler \citep{jones1999phrasier}.

  Bien que les termes-clés soient utiles pour de multiples tâches, très peu de
  documents en sont pourvus, du fait du coût important de production de ceux-ci,
  en termes de temps et de ressources humaines. Pour y remédier de nombreux
  chercheurs s'intéressent à l'extraction automatique de ceux-ci et certaines
  campagnes d'évaluations, telles que DEFT \citep{paroubek2012deft} et SemEval
  \citep{kim2010semeval}, proposent des tâches d'extraction automatique de
  termes-clés dans le but de confronter les différents systèmes existants. Pour
  ce faire, les données et la méthode d'évaluation sont les mêmes pour tous les
  systèmes.
  
  Il existe aussi une autre tâche nommée assignation automatique de termes-clés.
  Cette tâche est très proche de l'extraction automatique de termes-clés, mais
  elle est plus contrôlée. Elle consiste aussi à donner un ensemble de
  termes-clés pour un document, mais certains de ces termes peuvent ne pas être
  présents dans celui-ci. Ceci est dû au fait que les méthodes d'assignation de
  termes-clés utilisent des ressources supplémentaires telles que des
  référentiels terminologiques. Ceux-ci contiennent des termes spécifiques au(x)
  domaine(s) traité(s) et l'assignation de ces termes peut être déclenchée par
  la présence de certains autres dans le document analysé.

  Dans cet article, seules les méthodes d'extraction automatique de termes-clés
  sont présentées. Celles-ci appartiennent à deux catégories distinctes : les
  méthodes supervisées et les méthodes non-supervisées. Dans le cas supervisé,
  l'extraction des termes-clés est effectuée grâce à un apprentissage préalable
  servant à calibrer la méthode avec un corpus dont les documents sont annotés
  en termes-clés. Les méthodes non-supervisées ne requièrent pas de phase
  d'apprentissage. Elles exploitent des représentations efficaces des documents
  ainsi que des propriétés définies à partir de traits statistiques pour
  extraire les termes-clés parmi des termes candidats.

  Dans la section \ref{sec:methods} de cet article, nous présentons les méthodes
  existantes d'extraction automatique de termes-clés, en commençant par les
  méthodes non-supervisées, puis les méthodes supervisées. Dans la section
  \ref{sec:conclusion} nous terminons par un bilan de l'état de l'art et nous
  discutons des perspectives de travaux futurs.

\section{Les méthodes d'extraction automatique de termes-clés}
\label{sec:methods}
  L'extraction de termes-clés est une tâche qui consiste à analyser un document
  et à en extraire les aspects importants. Alors que les méthodes de résumé
  automatique utilisent des phrases pour construire une vision synthétique du
  document, l'extraction de termes-clés se focalise sur les unités textuelles
  qui composent ces phrases. Un ensemble de termes-clés peut donc être perçu
  comme un résumé dont les points clés sont exprimés sans liaisons entre eux.
  Les unités textuelles sur lesquelles travaillent les systèmes d'extraction
  automatique de termes-clés sont appelées termes candidats. Ces derniers sont
  des mots ou des multi-mots (phrasèmes) pouvant être promus au statut de
  terme-clé.

  L'extraction de termes candidats est une étape préliminaire de l'extraction de
  termes-clés, que ce soit pour les méthodes non-supervisées ou supervisées.
  Cette étape est importante, car si certains termes-clés du document analysé ne
  sont pas présents dans l'ensemble des termes candidats, alors ceux-ci ne
  pourront pas être extrait. \citet{hulth2003keywordextraction} étudie trois
  méthodes d'extraction des termes candidats. L'une consiste à extraire les
  chunks nominaux\footnote{Un chunk est une unité minimale de sens constituée
  d'un ou de plusieurs mots. Un chunk nominal est un chunk dont la tête est un
  nom ou un pronom. Par exemple, dans \og Nous avons une bonne politique
  qualitative. \fg, \og Nous \fg\ et \og une bonne politique qualitative \fg\ 
  sont des chunks nominaux.}, tandis que les deux autres extraient tous les
  n-grammes et les filtrent, soit pour retirer les termes contenant des mots
  outils dans le premier cas, soit pour ne retenir que les termes respectant
  certains patrons syntaxiques dans le second cas (usage des parties du
  discours). Dans ses expériences \citet{hulth2003keywordextraction} montre que
  l'extraction de termes-clés à partir de n-grammes filtrés avec les mots outils
  donne les meilleurs résultats parmi les trois méthodes qu'elle propose.

  Les travaux de \citet{hulth2003keywordextraction} sont évalués avec un corpus
  dont les documents sont des résumés d'articles scientifiques. Cependant, dans
  d'autres domaines tels que la bio-médecine, la nature des termes à extraire
  n'est pas la même. En effet, ce sont les acronymes et les entités nommées
  (noms de protéines par exemple) qu'il est nécessaire d'extraire en tant que
  termes-clés \cite{nobata2008kleio}. Pour cela, l'extraction de termes
  candidats est spécifique au domaine d'application. Les méthodes d'extraction
  de termes-clés présentées dans cet article traitent des documents supposés
  sans spécificités particulières, les méthodes d'extraction de termes candidats
  sont donc les mêmes que celles expérimentées par
  \citet{hulth2003keywordextraction}, mais il est envisageable de les adapter à
  des domaines présentant des spécificités particulières.

  Utilisés avec les méthodes non-supervisées, les termes candidats sont ordonnés
  selon un score d'importance obtenu soit à partir d'eux-mêmes, soit à partir de
  l'importance des mots qui les composent. Si une méthode s'appuie uniquement
  sur les mots, alors le score d'un terme candidat est généralement calculé en
  faisant la somme des mots qui le composent. Cependant, ceci n'est pas toujours
  juste, c'est donc un inconvénient important des méthodes travaillant sur les
  mots pour extraire les termes-clés. En effet, la sommation peut privilégier
  des termes qui contiennent beaucoup de mots non-importants vis-à-vis de termes
  contenant très peu de mots, mais importants.
  
  Utilisés dans les méthodes supervisées, les termes candidats sont classés en
  tant que termes-clés ou non termes-clés grâce à des méthodes de
  classification.

  \subsection{Méthodes non-supervisées}
  \label{sec:unsupervised_methods}
    Les méthodes non-supervisées d'extraction de termes-clés ont la
    particularité de s'abstraire du domaine et de la langue des documents à
    analyser\footnote{L'abstraction de la langue est vraie pour ce qui est de la
    méthodologie, cependant les pré-traitements tels que la segmentation en
    phrases, en mots et l'étiquetage en parties du discours sont eux spécifiques
    à la langue.}. Cette abstraction est due au fait que les termes candidats
    sont analysés avec des règles simples déduites à partir de traits
    statistiques issus seulement du texte analysé, ou bien d'un corpus de
    référence non annoté.

    De nombreuses approches sont proposées. Certaines se fondent uniquement sur
    des statistiques alors que d'autres les combinent avec des représentations
    plus complexes des documents. Ces représentations peuvent aller de groupes
    de mots sémantiquement similaires à des graphes dont les n\oe{}uds sont des
    unités textuelles (mots, expressions, phrases, etc.) liées par des
    relations de recommandation\footnote{Pour une étude comparative de certaines
    des méthodes par regroupement \citep{liu2009keycluster} et à base de graphe
    \citep{mihalcea2004textrank, wan2008expandrank}, voir l'article de
    \citet{hassan2010conundrums}.}.

    \subsubsection{Approches statistiques}
      Plusieurs approches cherchent à définir ce qu'est un terme-clé en
      s'appuyant sur certains traits statistiques et en étudiant leur rapport
      avec la notion d'importance d'un terme candidat. Plus un terme candidat
      est jugé important vis-à-vis du document analysé, plus celui-ci est
      pertinent en tant que terme-clé.

      TF-IDF (cf. équation \ref{math:tfidf}) de \citet{jones1972tfidf} et Likey
      (cf. équation \ref{math:likey}) de \citet{paukkeri2010likey} sont deux
      méthodes qui comparent le comportement d'un terme candidat dans le
      document analysé avec son comportement dans une collection de documents
      (corpus de référence). L'objectif est de trouver les termes candidats dont
      le comportement dans le document varie positivement comparé à leur
      comportement global dans la collection. Dans les deux méthodes ceci
      s'exprime par le fait qu'un terme a une forte importance vis-à-vis du
      document analysé s'il y est très présent, alors qu'il ne l'est pas dans le
      reste de la collection.
      \begin{align}
        \text{TF-IDF}(\text{terme}) &= TF(\text{terme}) \times \log\left(\frac{N}{DF(\text{terme})}\right) \label{math:tfidf}\\
        \notag\\
        Likey(\text{terme}) &= \frac{\text{rang}_{\text{document}}(\text{terme})}{\text{rang}_{\text{corpus}}(\text{terme})} \label{math:likey}
      \end{align}
      Dans TF-IDF, $TF$ représente le nombre d'occurrences d'un terme dans le
      document analysé et $DF$ représente le nombre de documents dans lequel il
      est présent, $N$ étant le nombre total de documents. Plus le score TF-IDF
      d'un terme candidat est élevé, plus celui-ci est important dans le
      document analysé. Dans Likey, le rang d'un terme candidat dans le document
      et dans le corpus est obtenu à partir de son nombre d'occurrences,
      respectivement dans le document et dans le corpus de référence. Plus le
      rapport entre ces deux rangs est faible, plus le terme candidat évalué est
      important dans le document analysé.

      Okapi (ou BM25) \citep{robertson1999okapi} est une mesure alternative à
      TF-IDF. En Recherche d'Information (RI), celle-ci est plus utilisée que le
      TF-IDF. Bien que l'extraction automatique de termes-clés soit une
      discipline à la frontière entre le TAL et la RI, la méthode de pondération
      Okapi n'a, à notre connaissance, pas été appliquée pour l'extraction de
      termes-clés. Dans l'article de \citet{claveau2012vectorisation}, Okapi est
      décrit comme un TF-IDF prenant mieux en compte la longueur des documents.
      Cette dernière est utilisée pour normaliser le $TF$ (qui devient
      $TF_{BM25}$) :
      \begin{align}
        \text{Okapi}(\text{terme}) &= TF_{BM25}(\text{terme}) \times \log\left(\frac{N - DF(\text{terme}) + 0,5}{DF(\text{terme}) + 0,5}\right) \label{math:okapi}\\
        \notag\\
        TF_{BM25} &= \frac{TF(\text{terme}) \times (k_1 + 1)}{TF(\text{terme}) + k_1 \times \left(1 - b + b \times \frac{DL}{DL_{\text{moyenne}}}\right)} \label{math:tf_bm25}
      \end{align}\\
      Dans la formule (\ref{math:tf_bm25}), $k_1$ et $b$ sont des constantes
      fixées à $2$ et $0,75$ respectivement. $DL$ représente la longueur du
      document analysé et $DL_{moyenne}$ la longueur moyenne des documents de la
      collection utilisée.

      \citet{barker2000nounphrasehead} estiment que les grands phrasèmes sont
      plus informatifs et qu'ils doivent être privilégiés. Pour cela, leur
      approche est très simple : plus un groupe nominal est long et fréquent
      dans le document analysé, plus il est jugé pertinent en tant que terme-clé
      de ce document. Cependant, pour éviter la répétition dans le texte, les
      auteurs des documents utilisent les même expression sous des formes
      alternatives (plus courtes, par exemple). La fréquence d'une expression ne
      reflète donc pas forcément sa fréquence réelle d'utilisation, car celle-ci
      est répartie dans les différentes alternatives. De ce fait,
      \citet{barker2000nounphrasehead} repèrent dans les groupes nominaux la
      tête nominale et utilisent en plus la fréquence de celle-ci.

      \citet{tomokiyo2003languagemodel} tentent de vérifier deux propriétés, en
      utilisant des modèles de langue uni-grammes et n-grammes et en calculant
      leur divergence (Kullback-Leibler). Les deux propriétés qu'ils tentent de
      vérifier sont les suivantes :
      \begin{enumerate}
        \item[-]{La grammaticalité : un terme-clé doit être bien formé
                 syntaxiquement.}
        \item[-]{L'informativité : un terme-clé doit capturer au moins une des
                 idées essentielles exprimées dans le document analysé.}
      \end{enumerate}
      Pour un terme candidat donné, plus sa probabilité en passant du modèle
      uni-gramme généré à partir du document vers le modèle n-gramme généré
      à partir du même document augmente, plus il respecte la propriété de
      grammaticalité. De même, plus sa probabilité en passant du modèle
      n-gramme généré à partir d'un corpus de référence vers le modèle
      n-gramme généré à partir du document analysé augmente, plus le terme
      candidat est informatif.

      La méthode que propose \citet{ding2011binaryintegerprogramming} utilise
      TF-IDF comme indicateur de l'importance d'un terme-clé. Dans un ensemble,
      cette importance doit être maximisée pour chaque terme-clé, mais les
      auteurs estiment que ceci n'est pas suffisant. Comme
      \citet{tomokiyo2003languagemodel}, ils définissent deux propriétés qui
      doivent être respectées :
      \begin{enumerate}
        \item[-]{La couverture : un ensemble de termes-clés doit couvrir
                 l'intégralité des sujets abordés dans le document représenté.}
        \item[-]{La cohérence : les termes-clés doivent être cohérents entre
                 eux.}
      \end{enumerate}
      La propriété de couverture est évaluée avec le modèle \textit{Latent
      Dirichlet Allocation} (LDA) qui donne la probabilité d'un terme candidat
      sachant un sujet. La cohérence est évaluée pour chaque paire de
      termes-clés de l'ensemble avec la mesure d'information mutuelle. Ces deux
      propriétés sont définies comme contraintes que les auteurs utilisent avec
      une méthode de programmation par les entiers (technique d'optimisation),
      la maximisation de la pertinence de chaque terme-clé étant l'objectif à
      atteindre.

      Les traits statistiques utilisés dans les méthodes précédentes sont
      uniquement utilisés pour déterminer un score de pertinence des termes
      candidats en tant que termes-clés. Une donnée statistique non citée
      précédemment, mais pourtant récurrente dans les méthodes d'extraction de
      termes-clés, est la fréquence de co-occurrences entre deux phrasèmes
      (termes). Deux phrasèmes co-occurrent s'ils apparaissent ensemble dans le
      même contexte. La co-occurrence peut être calculée de manière stricte (les
      phrasèmes doivent être côte-à-côte) ou bien dans une fenêtre de mots.
      Compter le nombre de co-occurrences entre deux termes permet d'estimer
      s'ils sont sémantiquement liés ou non. Ce lien sémantique à lui seul ne
      peut pas servir à extraire des termes-clés, mais il permet de mieux
      organiser les termes d'un document pour affiner l'extraction
      \citep{matsuo2004wordcooccurrence, liu2009keycluster,
      mihalcea2004textrank}.

    \subsubsection{Approches par regroupement}
      L'objectif des approches par regroupement est de définir des groupes dont
      les unités textuelles partagent une ou plusieurs caractéristiques
      communes. Ainsi, lorsque des termes-clés sont extraits à partir de chaque
      groupe, cela permet de mieux couvrir le document analysé selon les
      caractéristiques utilisées.

      Dans la méthode de \citet{matsuo2004wordcooccurrence}, ce sont les termes
      (phrasèmes) qui sont regroupés. Parmi ceux-ci, seuls les plus fréquents
      sont concernés par le regroupement. Celui-ci s'effectue en fonction du
      lien sémantique\footnote{Deux phrasèmes qui co-occurrent fréquemment
      ensemble sont jugés sémantiquement liés.} entre les termes. Après le
      regroupement, la méthode consiste à comparer les termes candidats du
      document analysé avec les groupes de termes fréquents, en faisant
      l'hypothèse qu'un terme candidat qui co-occurre plus que selon toute
      probabilité avec les termes fréquents d'un ou plusieurs groupes est plus
      vraisemblablement un terme-clé.

      Dans l'algorithme KeyCluster,  \citet{liu2009keycluster} utilisent aussi
      un regroupement sémantique, mais dans leur cas ils considèrent les mots du
      document analysé et ils excluent les mots outils. Dans chaque groupe
      sémantique, le mot qui est le plus proche du centroïde est sélectionné
      comme mot de référence. L'ensemble des mots de référence est ensuite
      utilisé pour filtrer les termes candidats en ne considérant comme
      termes-clés que ceux qui contiennent au moins un mot de référence (tous
      les mots de référence devant être utilisés dans au moins un terme-clé).

    \subsubsection{Approches à base de graphe}
      Les approches à base de graphe consistent à représenter le contenu d'un
      document sous la forme d'un graphe. La méthodologie appliquée est issue de
      PageRank \citep{brin1998pagerank}, un algorithme d'ordonnancement de pages
      Web (n\oe{}uds du graphe) grâce aux liens de recommandation qui existent
      entre elles (arcs du graphe). TextRank \citep{mihalcea2004textrank} et
      SingleRank \citep{wan2008expandrank} sont les deux adaptations de base de
      PageRank pour l'extraction automatique de termes-clés\footnote{TextRank a
      aussi été utilisé pour faire du résumé automatique.}. Dans celles-ci, les
      pages Web sont remplacées par des unités textuelles dont la granularité
      est le mot et un arc est créé entre deux n\oe{}uds si les mots qu'ils
      représentent co-occurrent dans une fenêtre de mots donnée.
    
      Le graphe est noté $G = (N, A)$, où $N$ est l'ensemble des n\oe{}uds du
      graphe et où $A$ est l'ensemble de ses arcs entrants et sortant :
      $A_{entrant} \cup A_{sortant}$\footnote{Dans le cas de TextRank et de
      SingleRank\ $A_{entrant} = A_{sortant}$, car le graphe n'est pas
      orienté.}. Pour chaque n\oe{}ud du graphe, un score est calculé par un
      processus itératif destiné à simuler la notion de recommandation d'une
      unité textuelle par d'autres\footnote{Plus le score d'une unité textuelle
      est élevé, plus celle-ci est importante dans le document analysé.}
      (cf. équation \ref{math:textrank}). Ce score à chaque n\oe{}ud $n_i$
      permet d'ordonner les mots par degré d'importance dans le document
      analysé. La liste ordonnée des mots peut ensuite être utilisée pour
      extraire les termes-clés.
      \begin{align}
        S(n_i) &= (1 - \lambda) + \lambda \times \sum_{n_j \in A_{\text{entrant}}(n_i)} \frac{p_{j, i} \times S(n_j)}{\mathlarger{\sum}_{n_k \in A_{\text{sortant}}(n_j)} p_{j, k}} \label{math:textrank}
      \end{align}
      $\lambda$ est un facteur d'atténuation qui peut être considéré ici comme
      la probabilité pour que le n\oe{}ud $n_i$ soit atteint par recommandation.
      $p_{j, i}$ représente le poids de l'arc allant du n\oe{}ud $n_j$ vers le
      n\oe{}ud $n_i$, soit le nombre de co-occurrences entre les deux mots $i$
      et $j$\footnote{TextRank utilise un graphe non-pondéré. Dans ce cas,
      $p_{j, i}$ vaut toujours $1$.}.

      Dans leurs travaux, \citet{wan2008expandrank} s'intéressent à l'ajout
      d'informations dans le graphe grâce à des documents similaires (voisins)
      et aux relations de co-occurrences qu'ils possèdent (ExpandRank).
      L'objectif est de faire mieux ressortir les mots importants du graphe en
      ajoutant de nouveaux liens de recommandation ou bien en renforçant ceux
      qui existent déjà. L'usage de documents similaires peut cependant ajouter
      ou renforcer des liens qui ne devraient pas l'être. Pour éviter cela, les
      auteurs réduisent l'impact des documents voisins en utilisant leur degré
      de similarité avec le document analysé. Une alternative à ExpandRank,
      CollabRank, également proposée par \citet{wan2008collabrank}, fonctionne
      de la même manière, mais certains choix des auteurs rendent impossible
      l'usage du degré de similarité pour réduire l'impact des documents
      voisins. Les résultats moins concluants de CollabRank tendent à confirmer
      l'importance de l'usage du degré de similarité.

      Dans l'optique d'améliorer encore TextRank/SingleRank,
      \citet{liu2010topicalpagerank} proposent une méthode qui cherche cette
      fois-ci à augmenter la couverture de l'ensemble des termes-clés extraits
      dans le document analysé (TopicalPageRank). Pour ce faire, ils tentent
      d'affiner le rang d'importance des mots dans le document en tenant compte
      de leur rang dans chaque sujet abordé. Le rang d'un mot pour un sujet est
      obtenu en intégrant à son score PageRank la probabilité qu'il appartienne
      au sujet (cf. équation \ref{math:topicalpagerank}). Le rang global d'un
      terme candidat est ensuite obtenu en fusionnant ses rangs pour chaque
      sujet.
      \begin{align}
        S_{\text{sujet}}(N_i) &= (1 - \lambda) \times p(\text{sujet} | i) + \lambda \times \sum_{N_j \in A_{\text{entrant}}(N_i)} \frac{p_{j, i} \times S(N_j)}{\mathlarger{\sum}_{N_k \in A_{\text{sortant}}(N_j)} p_{j, k}} \label{math:topicalpagerank}
      \end{align}

      Les approches à bases de graphe présentées ci-dessus effectuent toutes un
      ordonnancement des mots du document analysé selon leur importance dans
      celui-ci. Pour extraire les termes-clés il est donc nécessaire d'effectuer
      du travail supplémentaire à partir de la liste ordonnée de mots. Dans la
      méthode TextRank, les $k$ mots les plus importants sont sélectionnés et
      retournés (après que ceux apparaissant en collocation dans le document
      aient été concaténés). La technique utilisée dans les autres méthodes
      consiste à ordonner les termes candidats en fonction de la somme du score
      des mots qui les composent. Cependant, puisque l'un des avantages du
      graphe est que les n\oe{}uds peuvent avoir une granularité contrôlée,
      \citet{liang2009querylog} décident d'utiliser des mots et des multi-mots
      au lieu de simples mots et de tirer profit de traits supplémentaires, la
      taille du terme ou encore sa première position dans le document analysé.

  \subsection{Méthodes supervisées}
  \label{sec:supervised_methods}
    Les méthodes supervisées sont des méthodes capables d'apprendre à réaliser
    une tâche particulière, soit ici l'extraction de termes-clés.
    L'apprentissage se fait grâce à un corpus dont les documents sont annotés en
    termes-clés. L'annotation permet d'extraire les exemples et les
    contres-exemples dont les traits statistiques et/ou linguistiques servent à
    apprendre une classification binaire. La classification binaire consiste à
    indiquer si un terme candidat est un terme-clé ou non.

    De nombreux algorithmes d'apprentissage sont utilisés dans divers domaines.
    Ils peuvent potentiellement s'adapter à n'importe quelle tâche, dont celle
    de l'extraction automatique de termes-clés. Les algorithmes utilisés pour
    celle-ci construisent des modèles probabilistes, des arbres de décision, des
    Séparateurs à Large Marge (SVM) ou encore des réseaux de
    neurones\footnote{\citet{sarkar2012machinelearningcomparison} proposent une
    étude comparative de l'usage des arbres de décision, de la classification
    naïve bayésienne et des réseaux de neurones pour l'extraction automatique de
    termes-clés.}.

    KEA \citep{witten1999kea} est une méthode qui utilise une classification
    naïve bayésienne pour attribuer un score de vraisemblance à chaque terme
    candidat, le but étant d'indiquer s'ils sont des termes-clés ou
    non\footnote{Il est important de noter que le score de vraisemblance pour
    chaque terme candidat permet aussi de les ordonner entre eux.}.
    \citet{witten1999kea} utilisent trois distributions conditionnelles apprises
    à partir du corpus d'apprentissage. La première correspond à la probabilité
    pour que chaque terme candidat soit étiqueté \textit{oui} (terme-clé) ou
    \textit{non} (non terme-clé). Les deux autres correspondent à deux
    différents traits qui sont le poids TF-IDF du terme candidat et sa première
    position dans le document :
    \begin{align}
      P(\text{terme}) &= \frac{P_{\text{oui}}(\text{terme})}{P_{\text{oui}}(\text{terme}) + P_{\text{non}}(\text{terme})} \label{math:kea}\\
      \notag\\
      P_{\text{oui}}(\text{terme}) &= P(\text{terme} | \text{oui}) \times \prod_{\text{trait} \in \{\text{TF-IDF}, \text{position}\}} P_{\text{trait}}\left(\text{trait}(\text{terme}) | \text{oui}\right) \notag\\
      \notag\\
      P_{\text{non}}(\text{terme}) &= P(\text{terme} | \text{non}) \times \prod_{\text{trait} \in \{\text{TF-IDF}, \text{position}\}} P_{\text{trait}}\left(\text{trait}(\text{terme}) | \text{non}\right) \notag
    \end{align}
    L'un des avantages de la classification naïve bayésienne est que chaque
    distribution est supposée indépendante. L'ajout de nouveaux traits dans la
    méthode KEA est donc très aisé.
    
    Parmi les variantes de KEA proposées, \citet{frank1999keafrequency} ajoutent
    un troisième trait : le nombre de fois que le terme candidat est un
    terme-clé dans le corpus d'apprentissage. L'ajout de ce trait permet
    d'améliorer les performances de la version originale de KEA, mais uniquement
    lorsque la quantité de données d'apprentissage est très importante. Une
    autre amélioration de KEA, proposée par \citet{turney2003keacoherence},
    tente d'augmenter la cohérence entre les termes candidats les mieux classés.
    Pour ce faire, une première étape de classification est effectuée avec la
    méthode originale. Cette première étape permet d'obtenir un premier
    classement des termes candidats selon leur score de vraisemblance. Ensuite,
    de nouveaux traits sont ajoutés et une nouvelle étape de classification est
    lancée. Les nouveaux traits ont pour but d'augmenter le score de
    vraisemblance des termes candidats ayant un fort lien sémantique avec
    certains des termes les mieux classés après la première étape. Enfin,
    \citet{nguyen2007keadocumentstructure} proposent l'ajout des informations
    concernant la structure des documents. En effet, certaines sections telles
    que l'introduction et la conclusion dans les articles scientifiques sont
    plus susceptibles de contenir des termes-clés qu'une section présentant des
    résultats expérimentaux, par exemple. Dans leur version modifiée de KEA, ils
    proposent aussi l'usage de traits linguistiques tels que les parties du
    discours qui ont prouvées jouer un rôle non-négligeable pour l'extraction de
    termes-clés \citep{hulth2003keywordextraction}.
    
    En même temps que KEA \citep{witten1999kea},
    \citet{turney1999learningalgorithms} met au point l'algorithme génétique
    GenEx. GenEx est constitué de deux composants. Le premier composant, le
    géniteur, sert à apprendre des paramètres lors de la phase d'apprentissage.
    Ces paramètres sont utilisés par le second composant, l'extracteur, pour
    donner un score d'importance à chaque terme candidat. Plus les paramètres
    sont optimaux, meilleure est la classification des termes. Pour ce faire,
    les paramètres sont représentés sous la forme de bits qui constituent une
    population d'individus que le géniteur fait évoluer jusqu'à obtenir un état
    stable correspondant aux paramètres optimaux.

    Dans son article présentant GenEx, \citet{turney1999learningalgorithms}
    discute une autre méthode pour l'extraction de termes-clés. Cette méthode
    utilise de nombreux traits qui servent à entraîner $50$ arbres de décision
    C4.5 (technique de \textit{Random Forest}). Dans un arbre de décision,
    chaque branche représente un test sur l'un des traits d'un terme candidat.
    Les tests permettent un routage du terme candidat vers la feuille de l'arbre
    qui détermine sa classe. Grâce à la technique de \textit{Random Forest},
    soit l'usage de plusieurs arbres entraînés sur un échantillon différent du
    corpus d'apprentissage, l'extraction automatique de termes-clés est réduite
    à un vote de chaque arbre pour chaque terme candidat. Cela permet un
    classement des termes candidats en fonction de leur nombre de votes
    positifs. Les termes-clés extraits correspondent aux termes candidats les
    mieux classés.

    La même année que les travaux de \citet{hulth2003keywordextraction} sur
    le bien fondé d'utiliser des traits linguistiques pour l'extraction
    automatique de termes-clés, \citet{sujian2003maximumentropy} proposent une
    méthode utilisant un modèle d'entropie maximale (cf. équation
    \ref{math:maximum_entropy}) dont l'un des traits repose sur les parties du
    discours des mots qui composent les termes candidats. Un modèle de maximum
    d'entropie consiste à trouver parmi plusieurs distributions, une pour chaque
    trait, laquelle a la plus forte entropie. La distribution ayant la plus
    forte entropie est par définition celle qui contient le moins
    d'informations, ce qui la rend de ce fait moins arbitraire pour l'extraction
    des termes-clés.
    \begin{align}
      \text{Score}(\text{terme}) &= \frac{P(\text{oui} | \text{terme})}{P(\text{non} | \text{terme})} \label{math:maximum_entropy}\\
      \notag\\
      P(\text{classe} | \text{terme}) &= \frac{\exp\left(\mathlarger{\sum}_{\text{trait}} \alpha_{\text{trait}} \times \text{trait}(\text{terme}, \text{classe})\right)}{\mathlarger{\sum}_{c \in \{\text{oui}, \text{non}\}} \exp\left(\mathlarger{\sum}_{\text{trait}} \alpha_{\text{trait}} \times \text{trait}(\text{terme}, c)\right)} \notag
    \end{align}
    Le paramètre $\alpha_{\text{trait}}$ définit l'importance du trait auquel il
    est associé.

    Les Séparateurs à Large Marge sont aussi des classifieurs utilisés par les
    méthodes d'extraction automatique de termes-clés. Ils exploitent divers
    traits afin de projeter des exemples et des contres-exemples sur un plan,
    puis ils cherchent l'hyperplan qui les sépare. Cet hyperplan sert ensuite
    dans l'analyse de nouvelles données. Dans le contexte de l'extraction de
    termes-clés, les exemples sont les termes-clés et les contres-exemples sont
    les termes candidats qui ne sont pas des termes-clés. Ce mode de
    fonctionnement des SVM est utilisé par \citet{zhang2006svm}, mais un autre
    type de SVM est plus largement utilisé dans les méthodes supervisées
    d'extraction de termes-clés. Il s'agit de SVM qui utilisent de multiples
    marges représentant des rangs. Ces classifieurs permettent donc
    d'ordonnancer les termes-clés lors de leur extraction
    \citep{herbrich1999svm, joachims2006linearsvm, jiang2009rankingsvm}.
    La méthode KeyWE de \citet{eichler2010keywe} utilise ce type de SVM avec
    le trait TF-IDF ainsi qu'un trait booléen ayant la valeur vraie si le terme
    candidat apparaît dans un titre d'un article Wikipedia (un terme candidat
    apparaissant dans le titre d'un article de Wikipedia a une plus forte
    probabilité d'être un terme-clé). L'ordonnancement des termes candidats par
    le SVM permet ensuite de contrôler le nombre de termes-clés à extraire
    (choix des $k$ termes candidats les mieux classés).

    Tout comme \citet{turney1999learningalgorithms},
    \citet{ercan2007lexicalchains} utilisent eux aussi une forêt d'arbres C4.5
    dans leur méthode d'extraction de termes-clés. Ils utilisent des traits
    classiques et leur contribution se situe au niveau de l'utilisation d'un
    trait calculé à partir de chaînes lexicales. Une chaîne lexicale lie les
    mots d'un document selon certaines relations telles que la synonymie,
    l'hyponymie ou la méronymie. Ces relations permettent de calculer un score
    qui sert de trait. Cette approche est intéressante, mais du fait de
    limitations des chaînes lexicales actuellement disponibles elle présente
    l'inconvénient de ne retourner que des mots (aucun multi-mot). Cependant,
    l'usage d'une forêt d'arbre C4.5 permet un classement des mots à partir de
    leur nombre de votes positifs. Il est donc envisageable de déduire les
    termes-clés à partir de la liste ordonnée et pondérée des mots clés (voir
    les méthodes non-supervisées à bases de graphe -- section
    \ref{sec:unsupervised_methods}).

    Une autre méthode pour l'extraction automatique de termes-clés consiste à
    utiliser un perceptron multi-couches \citep{sarkar2010neuralnetwork}. Un
    perceptron multi-couches est un réseau de neurones constitué d'au moins
    trois couches, chaque couche étant composée de neurones. Dans les deux
    couches extrêmes les neurones représentent respectivement les entrées et les
    sorties. Les couches centrales sont des couches cachées qui permettent
    d'acheminer les valeurs des entrées vers les sorties, où de nouvelles
    valeurs sont obtenues grâce à la pondération des transitions d'un neurone
    d'une couche vers un neurone de la couche suivante. Les entrées
    correspondent aux traits d'un terme candidat (ici TF-IDF, la position, la
    taille, etc.) et les sorties représentent les classes qu'il peut prendre
    (terme-clé ou non terme-clé). La valeur obtenue pour chaque sortie (classe)
    permet d'obtenir une probabilité pour que le terme candidat analysé soit un
    terme-clé ou non. Dans leur méthode, \citet{sarkar2010neuralnetwork}
    utilisent cette probabilité pour ordonner les termes candidats afin de mieux
    contrôler le nombre de termes-clés à extraire.

    Dans leurs travaux, \citet{liu2011vocabularygap} proposent une méthode
    d'extraction de termes-clés basée sur un modèle génératif. Leur méthode est
    très différente de celle de \citet{witten1999kea} puisqu'ils décident
    d'utiliser une approche de traduction automatique. L'usage original de cette
    approche est justifié par le fait qu'un ensemble de termes-clés doit décrire
    de manière synthétique le document. Leur hypothèse est donc qu'un ensemble
    de termes-clés est une traduction d'un document dans un autre langage.
    Le modèle est appris à partir de paires de traductions dont l'un
    des termes est issu des titres ou des résumés des documents du corpus
    d'apprentissage et dont l'autre terme est issu des corps de ces mêmes
    documents. Les titres et les résumés sont utilisés comme langage synthétique
    et les corps des documents comme le langage naturel de ceux-ci.

\section{Conclusion et perspectives}
\label{sec:conclusion}
  L'extraction automatique de termes-clés est une tâche importante qui permet la
  valorisation d'un document (représentation synthétique, mise en évidence des
  points clés dans le document, etc.) et qui facilite l'accès aux documents
  pertinents pour une requête utilisateur (indexation pour la recherche
  d'information).
  
  Les méthodes existantes pour la tâche d'extraction automatique de termes-clés
  sont soit supervisées, soit non-supervisées. Les méthodes non-supervisées sont
  des méthodes émergentes ayant la particularité de s'abstraire de la
  spécificité des données traitées. Cette abstraction s'explique par des
  approches basées sur des constatations à propos de ce qu'est un terme-clé au
  sens général : importance sémantique, degré d'information, structure
  syntaxique, etc. Contrairement aux méthodes non-supervisées, les méthodes
  supervisées n'utilisent pas de propriétés définies à partir des traits
  statistiques et linguistiques, mais elles utilisent des modèles de décision
  appris à partir de ces traits, calculés sur les termes-clés d'un corpus
  d'apprentissage. L'usage d'un corpus d'apprentissage implique que les modèles
  appris soient spécifiques au domaine disciplinaire et à la langue de celui-ci.
  Cette spécificité peut s'avérer avantageuse lorsque le domaine et la langue
  que représente le corpus sont les mêmes pour les documents qui sont ensuite
  analysés, mais si tel n'est pas le cas les résultats de l'extraction peuvent
  en pâtir.

  De futurs travaux peuvent se focaliser sur une hybridation des méthodes
  non-supervisées et supervisées. Dans un premier temps, il peut être
  intéressant de tenter d'améliorer les méthodes à base de graphe existantes.
  En effet, le graphe possède plusieurs points de variabilité sur lesquels il
  est possible d'agir pour affiner l'extraction : la granularité des n\oe{}uds,
  le type de relations permettant la création des arcs ou encore le facteur
  d'atténuation $\lambda$ utilisé dans le calcul du score des n\oe{}uds. La
  granularité peut être étendue à des groupes de phrasèmes similaires (des
  variantes dont le sens est sensiblement le même). Cette nouvelle granularité
  peut impliquer la définition d'une nouvelle relation pour la création des arcs
  entre les n\oe{}uds. Enfin, des traits peuvent être appris, pondérés grâce à
  de l'apprentissage préalable, puis utilisés avec le facteur $(1 - \lambda)$
  dans le calcul du score pour chaque n\oe{}ud (voir la modification du score
  dans TopicalPageRank \citep{liu2010topicalpagerank}). Il est possible que ce
  dernier point demande de modifier la formule du score PageRank afin d'utiliser
  le score de recommandation et de nouveaux traits de manière cohérente (sans
  que la valeur d'un trait ne puisse annihiler le score de recommandation).

