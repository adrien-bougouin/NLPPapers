<!--
  == For your convenience, this form can be processed by EasyChair
  == automatically. You can fill out this form offline and then upload it
  == to EasyChair. Several review forms can be uploaded simultaneously.
  == You can modify your reviews as many times as you want.
  == 
  == When filling out the review form please mind the following rules:
  == 
  == (1) Blocks such as this are comments. EasyChair will ignore them.
  ==     Do not write any text into these blocks as it will be ignored.
  ==     You can add comments to the review form or remove them.
  == (2) Write only into the tags where instructed. Do not modify any
  ==     tags and attributes, or the review will become unusable and will
  ==     be rejected by EasyChair.
  -->
<review id="1837251::942477"
        submission="100"
        title="Measuring success in unsupervised rank-based keyword assignment to documents"
        authors="(anonymous)"
        pc_member="Florian Boudin">
<score id="225870" name="Adéquation">
<!--
  == L'article a-t-il sa place à TALN 2014 ?.
  ==
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 5 Certainement
  == 4 Probablement
  == 3 Pas sûr
  == 2 Probablement pas
  == 1 Certainement pas
  -->
5
</score>
<score id="225871" name="Clarté">
<!--
  == Pour un lecteur raisonnablement avisé, ce qui a été fait est-il
  == clair ? Comprend-on pourquoi le travail a été effectué ? L'article
  == est-il bien écrit et bien structuré ?.
  ==
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 5 Très clair.
  == 4 Compréhensible par la plupart des lecteurs.
  == 3 Partiellement compréhensible, nécessite un effort.
  == 2 Des aspects importants restent confus même en faisant un effort.
  == 1 La plus grande partie de l'article est incompréhensible.
  -->
5
</score>
<score id="225872" name="Originalité">
<!--
  == À quel point l'approche est-elle originale ? L'article est-il
  == novateur en termes de sujet, méthodologie ou contenu ? À quel point
  == la recherche décrite est innovante et enthousiasmante ? Notez qu'un
  == article peut être original même si les résultats ne sont pas
  == convaincants..
  ==
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 5 Surprenant : de nouveaux problèmes, techniques, une nouvelle méthodologie ou une nouvelle compréhension d'un problème connu -- personne n'a rien essayé de tel dans le passé.
  == 4 Créatif : un problème intrigant ou une technique novatrice qui sont différents de ce qui s'est fait par le passé.
  == 3 Respectable : une contribution originale qui représente une extension notable par rapport à l'existant.
  == 2 Limité : évidences, apport mineur sur des techniques connues.
  == 1 Une grande quantité de ce qui est proposé a déjà été fait avant ou en mieux.
  -->
2
</score>
<score id="225885" name="Justesse">
<!--
  == L'approche technique est-elle juste et bien choisie ? Peut-on faire
  == confiance aux affirmations des auteurs ? Ces propos sont-il soutenus
  == par des experiences adéquates, les résultats sont-ils correctement
  == interprétés ?.
  ==
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 5 Excellent : l'approche est bien choisie et soutenue par une argumentation et des expériences convaincantes.
  == 4 Solide : propos généralement bien étayés même si certains aspects de l'approche ou de l'évaluation ne sont pas tout à fait convaincants.
  == 3 Raisonnable : l'approche n'est pas fondamentalement mauvaise et au moins une des propositions est probablement correcte.
  == 2 Troublant : le travail aurait dû être fait différemment.
  == 1 Défauts inacceptables.
  -->
1
</score>
<score id="225886" name="Comparaison">
<!--
  == Les auteurs positionnent-ils le travail de manière adéquate par
  == rapport aux travaux sur des approches ou problèmes similaires ? Les
  == références bibliographiques sont-elles adéquates ? Si l'article
  == présente des recherches empiriques, les résultats sont-ils
  == présentés en regard des meilleurs résultats passés ?.
  ==
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 5 Comparaison précise et complète avec les travaux similaires. Excellent travail étant donné les contraintes de taille.
  == 4 Références bibliographiques solides mais quelques travaux manquent.
  == 3 La bibliographie et les comparaisons sont acceptables mais il est difficile de situer le travail.
  == 2 Une connaissance partielle de l'état de l'art ou une comparaison des résultats empiriques inadéquate.
  == 1 Très peu de relation à l'état de l'art ou manque d'une comparaison nécessaire des résultats empiriques.
  -->
1
</score>
<score id="225887" name="Impact">
<!--
  == Quel impact l'article proposé aura-t-il sur les travaux futurs ? Les
  == idées proposées seront-elles une inspiration pour les chercheurs de
  == la communauté ? Les travaux donnent-ils une vision novatrice et
  == éclairante sur un aspect mystérieux d'un problème ?.
  ==
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 5 Changera le domaine de recherche en affectant les choix des chercheurs et la manière dont ils travaillent.
  == 4 Certaines idées vont être très utiles à un bon nombre de chercheurs.
  == 3 Intéressant mais pas trop influent. Les travaux seront cités mais principalement à titre comparatif ou en tant que source mineure d'inspiration.
  == 2 Impact marginal. Le travail ne sera probablement pas cité.
  == 1 Aucun impact dans le domaine.
  -->
3
</score>
<score id="158432" name="Recommandation">
<!--
  == Prenez en compte vos notes précédentes pour la note finale de
  == recommandation pour cet article. Gardez en tête qu'il n'existe pas
  == d'article parfait et que nous ciblons une conférence pleine de
  == travaux intéressants, divers et opportuns. Penchez plutôt pour un
  == avis tranché, ce qui aidera considérablement le comité de
  == programme. Souvenez-vous aussi que les auteurs peuvent prendre en
  == compte vos commentaires avant de soumettre la version finale de leur
  == article. 

  == L'article doit-il être accepté ou non ?.
  ==
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 6 L'un des meilleurs articles de la conférence ; je pense qu'il faut vraiment l'accepter.
  == 5 Solide article que j'aimerais bien voir accepté.
  == 4 Article de qualité correcte qui vaut la peine d'être présenté.
  == 3 Ambivalent : article de qualité raisonnable mais il n'est pas vraiment à la hauteur des standards.
  == 2 Plutôt contre : certains aspects font que je suis contre la publication de cet article.
  == 1 Je suis prêt à me battre pour que cet article soit rejeté.
  -->
2
</score>
<score id="158433" name="Confiance du relecteur">
<!--
  == Select your choice from the options below and write its number below,
  == before the </score> tag.
  ==
  == 3 Mon évaluation est correcte. J'ai lu l'article avec beaucoup d'attention et je connais très bien le domaine.
  == 2 Relativement sûr mais j'ai peut-être raté quelque chose. Je connais assez bien le domaine, j'ai pu oublier quelques détails, par exemple dans les formules, les expériences ou les travaux reliés.
  == 1 Pas vraiment mon domaine ou le papier est trop difficile à comprendre. Mon évaluation est relativement approximative.
  -->
3
</score>
<!-- ======== Commentaires ======== -->
<text id="158435" name="Commentaires à destination des auteurs">
<!--
  == Faites d'abord un résumé en quelques phrases des travaux proposés
  == en utilisant vos propres mots pour montrer aux auteurs que vous avez
  == bien compris l'article. Puis donnez ici vos commentaires détaillés
  == pour justifier les notes sur chaque critère. En particulier, si vous
  == avez donné la note la plus basse sur un critère, veillez à bien
  == argumenter votre choix. Ce champ est requis..
  -->
This paper focuses on automatic evaluation of keyword extraction systems, especially systems ranking keyword candidates. The position of this paper is that commonly used precision, recall and f-score measures are not well suited to evaluate outputs of so-called rank-based systems. Rank-based systems provide a ranked list of EVERY keyword candidates when precision, recall and f-score require a finite set of extracted keywords. Hence, researchers cut off the ranked list at the n best ranked keyword candidates. To avoid such decision as "What is the appropriate value of n?", the paper presents the NDCG metric, an existing metric for the evaluation of ranking systems.

Major comments
--------------------

Evaluating the ranking instead of the n best ranked candidates is important, especially for a fair comparison of existing rank-based systems (as shown in Table 1). However, it is not true that this issue has not been considered yet. Indeed, previous work already uses similar metrics to NDCG: MAP, MRR and Bpref [1, 2, 3]. Therefore, the "survey of the evaluation approaches" is too weak. Also, Section 3 should present the R-precision [4], which is the precision evaluated when, for each document, n is set to the number of gold keywords assigned to the document.

There is no proper evaluation of the proposed NDCG. NDCG should be discussed regarding MAP, MRR and Bpref; It should be compared to them in a real situation (not based on only one example); Finally its correlation with human evaluation should be shown. Also, f-scores reported in Table 1 are not correct. For Example, the f-score of System 2 for n = 8 should be 0.80:
  * correctly returned keywords = 6
  * returned keywords = 8
  * all correct keywords = 7

  * P  = 6 / 8 = 0.75
  * R  = 6 / 7 = 0.86
  * F1 = 2 * ((0.75 * 0.86) / (0.75 + 0.86)) = 0.80

In the current state, the paper cannot be accepted. A complete survey of evaluation metrics (both set-based and rank-based) for keyword extraction systems must be done and, in addition to the example provided by Table 1, experiments must be performed to show the correlation of NDCG (and other metrics) with human evaluation.

Minor comments
--------------------

- "If we set Y = {0, 1}, then we see that a classification-based system can be viewed as a simple type of ranked-based keyword assignment system." (page 1)
  Some classification-based systems also provide a "degree of relevancy" (within [0, 1]) [5].

- The fact that NDCG already exists should be stated in the
  introduction, not only in Section 5.

- "Liu et al. Liu et al. (2010)"? (page 2)

- "We HOPE to show"? (page 2)

- "than than" (page 4)

- "we have been able explain" (page 6)
  -> we have been able TO explain

[1] Jiang, Xin and Hu, Yunhua and Li, Hang (2009). A ranking approach to keyphrase extraction, in Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval.
[2] Hofmann, Katja and Tsagkias, Manos and Meij, Edgar and De Rijke, Maarten (2009). The impact of document structure on keyphrase extraction, in Proceedings of the 18th ACM conference on Information and knowledge management.
[3] Liu, Zhiyuan and Huang, Wenyi and Zheng, Yabin and Sun, Maosong (2010). Automatic keyphrase extraction via topic decomposition, in Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.
[4] Zesch, Torsten and Gurevych, Iryna (2009). Approximate matching for evaluating keyphrase extraction, in Proceedings of the 7th International Conference on Recent Advances in Natural Language Processing.
[5] Witten, Ian H and Paynter, Gordon W and Frank, Eibe and Gutwin, Carl and Nevill-Manning, Craig G (1999). KEA: Practical automatic keyphrase extraction, in Proceedings of the fourth ACM conference on Digital libraries.
</text>
<text id="158436" name="Remarques confidentielles pour le comité de lecture">
<!--
  == Vous pouvez entrer des remarques à destination des membres du comité
  == de lecture uniquement. Ces remarques ne seront pas vues par les
  == auteurs. Ce champ est optionnel..
  -->
Je ne suis pas favorable à cet article. Celui-ci ne se compare pas aux mesures adéquates. De plus, au-delà de pointer les faiblesses des évaluations réalisées dans les travaux précédents, les auteurs se montrent parfois insultant (l'évaluation de Litvak & Last est "irréaliste" et "il est clair" qu'une partie du travail de Mihalcea & Tarau n'est pas correcte). Enfin, les auteurs cite l'article (Liu et al., 2010) dans lequel les mesures MRR et Bpref sont utilisées. Je me demande donc pour quelle raison les auteurs ne parlent pas de ces mesures.
</text>
<reviewer>
  <first_name> Adrien </first_name>
  <last_name>  Bougouin </last_name>
  <email> adrien.bougouin@univ-nantes.fr </email>
</reviewer>
</review>

